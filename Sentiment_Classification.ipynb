{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "deae2746",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import heapq\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "from typing import List\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "\n",
        "# Add project root to Python path\n",
        "repo_root = Path(\".\").resolve()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "# Import helper utilities from organized modules\n",
        "import importlib\n",
        "import utils.finbert\n",
        "import utils.analysis\n",
        "import utils.ablation\n",
        "import utils.run_dirs\n",
        "import sparse_autoencoder.finbert_sae\n",
        "\n",
        "# Force reload to get latest code\n",
        "importlib.reload(utils.finbert)\n",
        "importlib.reload(utils.analysis)\n",
        "importlib.reload(utils.ablation)\n",
        "importlib.reload(utils.run_dirs)\n",
        "importlib.reload(sparse_autoencoder.finbert_sae)\n",
        "\n",
        "from utils.finbert import compute_metrics\n",
        "from utils.analysis import (\n",
        "    FeatureStatsAggregator,\n",
        "    FeatureTopTokenTracker,\n",
        "    HeadlineFeatureAggregator\n",
        ")\n",
        "from utils.ablation import create_intervention_hook\n",
        "from utils.run_dirs import make_analysis_run_dir\n",
        "from sparse_autoencoder.finbert_sae import SparseAutoencoder, load_sae\n",
        "\n",
        "# --------- CUDA sanity check ----------\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Define device for SAE loading\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31a1b31e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load dataset, comes with train and validation fold \n",
        "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def remove_leading_tickers(text):\n",
        "    return re.sub(\n",
        "        r'^\\s*(?:\\$[A-Z]{1,6}\\s*)+(?:[-:]\\s*)?',\n",
        "        '',\n",
        "        text\n",
        "    )\n",
        "\n",
        "# Clean dataset\n",
        "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
        "ds = ds.map(lambda x: {\"text\": remove_leading_tickers(x[\"text\"])})\n",
        "\n",
        "# Load dataset\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8034bf6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants/Hyperparameters for training model and SAE\n",
        "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Configuration for Inference\n",
        "MAX_SAMPLES = 100  # Limit for testing\n",
        "TOP_FEATURES = 100  # Top features to track per metric\n",
        "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
        "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
        "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f233625a",
      "metadata": {},
      "source": [
        "Fine Tune Hyperparameters of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f40e9d6",
      "metadata": {},
      "source": [
        "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~4k to 32k interpretable sparse features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c951276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes SAEs based on BERT.\n",
        "# Configuration\n",
        "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Create SAE save directory\n",
        "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load dataset\n",
        "# train_ds = ds[\"train\"]\n",
        "\n",
        "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
        "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
        "\n",
        "# Collect training activations\n",
        "all_activations = []\n",
        "captured_activations = []\n",
        "\n",
        "def capture_hook(module, input, output):\n",
        "    if isinstance(output, tuple):\n",
        "        hidden_states = output[0]\n",
        "    else:\n",
        "        hidden_states = output\n",
        "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
        "\n",
        "# Register hook\n",
        "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
        "\n",
        "# Collect activations from all training data\n",
        "print(\"Extracting activations from training set...\")\n",
        "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(train_ds)):\n",
        "        text = sample[\"text\"]\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        captured_activations.clear()\n",
        "        _ = model(**inputs)\n",
        "        \n",
        "        if captured_activations:\n",
        "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
        "            activation = captured_activations[0].squeeze(0)\n",
        "            \n",
        "            # Get attention mask and token IDs (keep on GPU)\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            \n",
        "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
        "\n",
        "            # Print the number of valid tokens\n",
        "            # kept = valid_mask.sum().item()\n",
        "            # total = attention_mask.sum().item()\n",
        "            # print(f\"Kept {kept}/{total} tokens\")\n",
        "\n",
        "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
        "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
        "\n",
        "            # print(\"TOKENS:\", tokens)\n",
        "            # print(\"DROPPED:\", dropped_tokens)\n",
        "            # print(\"KEPT:\", kept_tokens)\n",
        "            \n",
        "            # Only keep activations for real content tokens (still on GPU)\n",
        "            activation = activation[valid_mask]\n",
        "            \n",
        "            # Only add if there are real tokens\n",
        "            if activation.shape[0] > 0:\n",
        "                # Move to CPU only when storing for later processing\n",
        "                all_activations.append(activation.cpu())\n",
        "\n",
        "hook_handle.remove()\n",
        "\n",
        "# Flatten all activations into a single tensor [total_tokens, 768]\n",
        "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
        "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
        "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
        "\n",
        "# Train SAEs for each latent dimension\n",
        "for LATENT_DIM in LATENT_DIMS:\n",
        "    print(f\"\\\\n{'='*80}\")\n",
        "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create SAE\n",
        "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
        "    sae.to(device)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # Create DataLoader\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    dataset = TensorDataset(all_activations_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
        "    sae.train()\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        total_l1_loss = 0\n",
        "        \n",
        "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            reconstruction, latent = sae(batch_x)\n",
        "            \n",
        "            # Reconstruction loss (MSE)\n",
        "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
        "            \n",
        "            # L1 sparsity loss\n",
        "            l1_loss = latent.abs().mean()\n",
        "            \n",
        "            # Combined loss\n",
        "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Renormalize decoder weights (standard SAE practice)\n",
        "            with torch.no_grad():\n",
        "                sae.decoder.weight.data = nn.functional.normalize(\n",
        "                    sae.decoder.weight.data, dim=0\n",
        "                )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_l1_loss += l1_loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_recon = total_recon_loss / len(dataloader)\n",
        "        avg_l1 = total_l1_loss / len(dataloader)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
        "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
        "    \n",
        "    # Save the trained SAE\n",
        "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
        "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
        "    torch.save({\n",
        "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
        "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
        "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
        "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
        "        'config': {\n",
        "            'input_dim': 768,\n",
        "            'latent_dim': LATENT_DIM,\n",
        "            'layer': LAYER_TO_EXTRACT,\n",
        "            'model': save_dir,\n",
        "        }\n",
        "    }, SAE_SAVE_PATH)\n",
        "    \n",
        "    # Test sparsity\n",
        "    sae.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_acts = all_activations_tensor[:1000].to(device)\n",
        "        sample_latent = sae.encode(sample_acts)\n",
        "        sparsity = (sample_latent > 0).float().mean()\n",
        "        print(f\"\\\\nâœ“ SAE trained successfully!\")\n",
        "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
        "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"All SAEs trained successfully!\")\n",
        "print(f\"Available SAE models:\")\n",
        "for dim in LATENT_DIMS:\n",
        "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
        "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d70e94b",
      "metadata": {},
      "source": [
        "Finetune FinBERT Model\n",
        "\n",
        "The FinBERT model is trained on the training fold of our dataset to improve its prediction accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0697b2e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes the FINBERT model.\n",
        "\n",
        "# 2) Load model/tokenizer\n",
        "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# 3) Tokenize\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
        "val_tok = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
        "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
        "\n",
        "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4) Metrics\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# 5) Training config\n",
        "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_twitter_ft\",\n",
        "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
        "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
        "    report_to=\"none\",              # avoids needing wandb, etc.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
        "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f30b519",
      "metadata": {},
      "source": [
        "Inference with Interpretability\n",
        "\n",
        "We use our FinBERT + SAE on test data. We extract a Layer Activations with Sentiment Predictions (SAE-style Analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a64315a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ABLATION EXPERIMENT\n",
            "============================================================\n",
            "Ablation Mode: union_top_k\n",
            "K value: 10\n",
            "âœ“ Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
            "  Layer: 8\n",
            "  Input dim: 768\n",
            "  Latent dim: 32768\n",
            "Ablation mode: union_top_k\n",
            "Layer: 8\n",
            "SAE Size: 32k (32768 features)\n",
            "Max Samples: 100\n",
            "\n",
            "ðŸ”¬ Running baseline inference (no ablation)...\n",
            "  Baseline: 20/100 samples\n",
            "  Baseline: 40/100 samples\n",
            "  Baseline: 60/100 samples\n",
            "  Baseline: 80/100 samples\n",
            "  Baseline: 100/100 samples\n",
            "âœ“ Baseline accuracy: 87.00%\n",
            "\n",
            "\n",
            "Mode 3 (Union Top-K): Collected 91 unique features from union of top-10 across 100 samples\n",
            "Features to ablate: [423, 550, 602, 687, 2976, 3715, 3993, 4083, 4247, 4456, 5111, 6026, 6555, 6793, 6977, 7673, 7776, 7828, 7927, 8051, 8232, 8262, 8784, 9368, 9395, 9718, 9847, 10033, 10604, 11814, 12053, 12193, 12514, 13142, 13185, 13644, 14807, 15370, 15540, 15818, 15959, 15991, 16205, 16393, 16907, 17220, 17323, 17433, 17639, 18188, 18291, 18317, 18425, 18464, 19876, 20245, 20268, 20637, 21110, 21508, 21969, 22130, 22354, 22992, 23041, 23500, 24303, 24583, 24655, 25148, 25518, 25797, 26139, 26260, 26886, 27274, 27518, 27757, 27766, 28607, 28638, 28660, 28908, 29073, 29555, 29952, 30235, 31394, 31923, 32411, 32601]\n",
            "\n",
            "ðŸ”¬ Running ablation inference (features zeroed)...\n",
            "âœ“ Ablated accuracy: 60.00%\n",
            "\n",
            "============================================================\n",
            "FEATURE ABLATION RESULTS\n",
            "============================================================\n",
            "Ablation Mode: union_top_k\n",
            "Ablated Features: [423, 550, 602, 687, 2976, 3715, 3993, 4083, 4247, 4456, 5111, 6026, 6555, 6793, 6977, 7673, 7776, 7828, 7927, 8051, 8232, 8262, 8784, 9368, 9395, 9718, 9847, 10033, 10604, 11814, 12053, 12193, 12514, 13142, 13185, 13644, 14807, 15370, 15540, 15818, 15959, 15991, 16205, 16393, 16907, 17220, 17323, 17433, 17639, 18188, 18291, 18317, 18425, 18464, 19876, 20245, 20268, 20637, 21110, 21508, 21969, 22130, 22354, 22992, 23041, 23500, 24303, 24583, 24655, 25148, 25518, 25797, 26139, 26260, 26886, 27274, 27518, 27757, 27766, 28607, 28638, 28660, 28908, 29073, 29555, 29952, 30235, 31394, 31923, 32411, 32601]\n",
            "Baseline Accuracy: 87.00%\n",
            "Ablated Accuracy: 60.00%\n",
            "Accuracy Change: -27.00%\n",
            "\n",
            "Flipped Predictions: 33/100 samples\n",
            "Flip Rate: 33.00%\n",
            "\n",
            "============================================================\n",
            "FLIPPED PREDICTIONS (showing first 10):\n",
            "============================================================\n",
            "\n",
            "--- Sample #0 ---\n",
            "Text: Ally Financial pulls outlook\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.982) â†’ Ablated: Neutral (conf: 0.851)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21110: 7.3072 [ABLATED]\n",
            "  Feature 24583: 4.8220 [ABLATED]\n",
            "  Feature 21508: 4.6871 [ABLATED]\n",
            "  Feature 32601: 4.5500 [ABLATED]\n",
            "  Feature 15959: 4.3837 [ABLATED]\n",
            "  Feature 27518: 4.1182 [ABLATED]\n",
            "  Feature 29555: 3.8110 [ABLATED]\n",
            "  Feature 3993: 3.6784 [ABLATED]\n",
            "  Feature 13142: 3.6465 [ABLATED]\n",
            "  Feature 22354: 3.4679 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #2 ---\n",
            "Text: Moody's turns negative on Party City\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.981) â†’ Ablated: Neutral (conf: 0.925)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.3068 [ABLATED]\n",
            "  Feature 21508: 7.2474 [ABLATED]\n",
            "  Feature 5111: 5.7303 [ABLATED]\n",
            "  Feature 21110: 5.6846 [ABLATED]\n",
            "  Feature 27518: 5.1542 [ABLATED]\n",
            "  Feature 25797: 4.9270 [ABLATED]\n",
            "  Feature 24583: 4.6555 [ABLATED]\n",
            "  Feature 16393: 4.5483 [ABLATED]\n",
            "  Feature 13142: 4.4483 [ABLATED]\n",
            "  Feature 4083: 4.1524 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #4 ---\n",
            "Text: Compass Point cuts to Sell\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.983) â†’ Ablated: Neutral (conf: 0.678)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.2449 [ABLATED]\n",
            "  Feature 21110: 6.1200 [ABLATED]\n",
            "  Feature 16393: 5.5337 [ABLATED]\n",
            "  Feature 25797: 5.1744 [ABLATED]\n",
            "  Feature 5111: 4.9148 [ABLATED]\n",
            "  Feature 29952: 4.5362 [ABLATED]\n",
            "  Feature 8262: 4.5314 [ABLATED]\n",
            "  Feature 14807: 4.5311 [ABLATED]\n",
            "  Feature 10604: 4.4184 [ABLATED]\n",
            "  Feature 687: 4.4105 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #5 ---\n",
            "Text: Barclays cools on Molson Coors\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.648) â†’ Ablated: Neutral (conf: 0.901)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 8.8932 [ABLATED]\n",
            "  Feature 32601: 6.1672 [ABLATED]\n",
            "  Feature 15991: 4.6884 [ABLATED]\n",
            "  Feature 21508: 4.4652 [ABLATED]\n",
            "  Feature 29952: 4.4536 [ABLATED]\n",
            "  Feature 5111: 4.2631 [ABLATED]\n",
            "  Feature 28660: 4.2182 [ABLATED]\n",
            "  Feature 7927: 4.2108 [ABLATED]\n",
            "  Feature 687: 4.1628 [ABLATED]\n",
            "  Feature 27757: 4.0974 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #6 ---\n",
            "Text: Barclays cuts to Equal Weight\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.972) â†’ Ablated: Neutral (conf: 0.790)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21508: 5.0885 [ABLATED]\n",
            "  Feature 24583: 4.5491 [ABLATED]\n",
            "  Feature 27518: 3.8906 [ABLATED]\n",
            "  Feature 8784: 3.8306 [ABLATED]\n",
            "  Feature 4456: 3.7008 [ABLATED]\n",
            "  Feature 31923: 3.6618 [ABLATED]\n",
            "  Feature 26139: 3.4318 [ABLATED]\n",
            "  Feature 21110: 3.4233 [ABLATED]\n",
            "  Feature 5111: 3.4078 [ABLATED]\n",
            "  Feature 8232: 3.3543 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #10 ---\n",
            "Text: Children's Place downgraded to neutral from outperform at Wedbush, price target slashed to $60 from $130\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.999) â†’ Ablated: Neutral (conf: 0.530)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.2199 [ABLATED]\n",
            "  Feature 21508: 8.0324 [ABLATED]\n",
            "  Feature 21110: 5.6904 [ABLATED]\n",
            "  Feature 4247: 5.5460 [ABLATED]\n",
            "  Feature 28908: 5.4686 [ABLATED]\n",
            "  Feature 25518: 5.2739 [ABLATED]\n",
            "  Feature 25797: 5.1319 [ABLATED]\n",
            "  Feature 17433: 5.1150 [ABLATED]\n",
            "  Feature 16205: 5.0357 [ABLATED]\n",
            "  Feature 9718: 4.9125 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #11 ---\n",
            "Text: Clovis Oncology downgraded to in line from outperform at Evercore ISI\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.999) â†’ Ablated: Neutral (conf: 0.650)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21508: 8.0159 [ABLATED]\n",
            "  Feature 24583: 7.2731 [ABLATED]\n",
            "  Feature 4456: 6.9411 [ABLATED]\n",
            "  Feature 18317: 5.8428 [ABLATED]\n",
            "  Feature 21110: 5.7342 [ABLATED]\n",
            "  Feature 28908: 5.7124 [ABLATED]\n",
            "  Feature 13142: 5.6617 [ABLATED]\n",
            "  Feature 25797: 5.6591 [ABLATED]\n",
            "  Feature 15991: 4.7023 [ABLATED]\n",
            "  Feature 21969: 4.4700 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #12 ---\n",
            "Text: Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.883) â†’ Ablated: Neutral (conf: 0.670)\n",
            "Top 10 SAE Features:\n",
            "  Feature 25518: 8.1618 [ABLATED]\n",
            "  Feature 4456: 6.9640 [ABLATED]\n",
            "  Feature 21110: 6.9277 [ABLATED]\n",
            "  Feature 21508: 6.4393 [ABLATED]\n",
            "  Feature 24583: 6.3370 [ABLATED]\n",
            "  Feature 19876: 5.9274 [ABLATED]\n",
            "  Feature 32411: 5.1512 [ABLATED]\n",
            "  Feature 32601: 4.9442 [ABLATED]\n",
            "  Feature 31394: 4.5632 [ABLATED]\n",
            "  Feature 15959: 4.5496 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #13 ---\n",
            "Text: Goldman pulls Progressive from Goldman's conviction list; shares -2.7%\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.994) â†’ Ablated: Neutral (conf: 0.852)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 7.5666 [ABLATED]\n",
            "  Feature 25797: 5.8555 [ABLATED]\n",
            "  Feature 32601: 5.7686 [ABLATED]\n",
            "  Feature 21508: 5.6294 [ABLATED]\n",
            "  Feature 4247: 5.6123 [ABLATED]\n",
            "  Feature 21110: 4.9541 [ABLATED]\n",
            "  Feature 16205: 4.5684 [ABLATED]\n",
            "  Feature 15959: 4.4294 [ABLATED]\n",
            "  Feature 28908: 4.3922 [ABLATED]\n",
            "  Feature 8784: 4.3459 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #15 ---\n",
            "Text: Intelsat cut to Market Perform at Raymond James\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.987) â†’ Ablated: Neutral (conf: 0.787)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 8.8027 [ABLATED]\n",
            "  Feature 24583: 5.4964 [ABLATED]\n",
            "  Feature 5111: 5.2002 [ABLATED]\n",
            "  Feature 18188: 4.9918 [ABLATED]\n",
            "  Feature 21508: 4.9749 [ABLATED]\n",
            "  Feature 21110: 4.6236 [ABLATED]\n",
            "  Feature 22354: 4.4753 [ABLATED]\n",
            "  Feature 21969: 4.2713 [ABLATED]\n",
            "  Feature 28660: 4.2391 [ABLATED]\n",
            "  Feature 687: 4.1818 [ABLATED]\n",
            "\n",
            "\n",
            "ðŸ’¾ Saving ablated results for visualization...\n",
            "ðŸ’¾ Saving ablated results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-01-30T01-17-47_run-072\n",
            "\n",
            "âœ… Ablation experiment complete!\n",
            "   ðŸ“ Ablated results saved to: 2026-01-30T01-17-47_run-072\n",
            "   ðŸŽ¯ Ablated Accuracy: 60.00%\n",
            "   ðŸ”¢ Total tokens: 1542\n",
            "   âœ¨ SAE features: 32768\n",
            "\n",
            "ðŸŒ Start the viewer to see ablated results:\n",
            "   python viz_analysis/feature_probe_server.py\n",
            "   cd sae-viewer && npm start\n"
          ]
        }
      ],
      "source": [
        "# Feature Ablation Experiment\n",
        "# This cell performs ablation by zeroing out specified SAE features and comparing predictions\n",
        "\n",
        "# Configuration\n",
        "# ========== ABLATION CONFIGURATION ==========\n",
        "ABLATION_CONFIG = {\n",
        "    #\"mode\": \"per_sample_top_k\",  # Options: \"manual\" | \"per_sample_top_k\" | \"union_top_k\"\n",
        "    \"mode\": \"union_top_k\",\n",
        "    \"k\": 10  # Only for per_sample_top_k and union_top_k modes\n",
        "}\n",
        "\n",
        "# Mode 1: Manual features (only used if mode == \"manual\")\n",
        "MANUAL_FEATURES = [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "if ABLATION_CONFIG['mode'] != 'manual':\n",
        "    print(f\"K value: {ABLATION_CONFIG['k']}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# Define device and move model to it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu only, please install CUDA-compatible Torch\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the SAE using the helper function\n",
        "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
        "\n",
        "# Extract dimensions from the loaded config\n",
        "SAE_INPUT_DIM = sae_config['input_dim']\n",
        "SAE_LATENT_DIM = sae_config['latent_dim']\n",
        "\n",
        "print(f\"Ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "print(f\"Layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"SAE Size: {SAE_SIZE} ({SAE_LATENT_DIM} features)\")\n",
        "print(f\"Max Samples: {MAX_SAMPLES}\\n\")\n",
        "\n",
        "# Storage for results\n",
        "baseline_predictions = []\n",
        "ablated_predictions = []\n",
        "sample_data = []\n",
        "\n",
        "# Initialize trackers for SAE features (same as inference cell)\n",
        "feature_stats_ablated = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
        "top_token_tracker_ablated = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
        "headline_aggregator_ablated = HeadlineFeatureAggregator(top_k=10)\n",
        "all_prompt_metadata_ablated = []\n",
        "\n",
        "# Storage for capturing SAE features during ablation (for tracking)\n",
        "current_sample_data = {\"sae_features\": None, \"token_ids\": None, \"prompt_tokens\": None, \"text\": None, \"idx\": None}\n",
        "\n",
        "# Run baseline inference (no ablation)\n",
        "print(\"ðŸ”¬ Running baseline inference (no ablation)...\")\n",
        "baseline_results = []\n",
        "baseline_features_map = {}  # Store baseline SAE features for comparison\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(test_ds):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # Forward pass (normal, no intervention)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        \n",
        "        pred_id = logits.argmax(dim=-1).item()\n",
        "        pred_label = model.config.id2label[pred_id]\n",
        "        confidence = probs[0, pred_id].item()\n",
        "        \n",
        "        baseline_results.append({\n",
        "            \"sample_idx\": idx,\n",
        "            \"text\": text,\n",
        "            \"true_label\": model.config.id2label[true_label],\n",
        "            \"predicted_label\": pred_label,\n",
        "            \"predicted_id\": pred_id,\n",
        "            \"confidence\": confidence,\n",
        "            \"logits\": logits.cpu().numpy(),\n",
        "            \"probs\": probs.cpu().numpy()\n",
        "        })\n",
        "        \n",
        "        # Extract baseline SAE features for ablation comparison\n",
        "        # Capture activations from target layer\n",
        "        captured_acts = []\n",
        "        def capture_hook(module, input, output):\n",
        "            if isinstance(output, tuple):\n",
        "                captured_acts.append(output[0].detach())\n",
        "            else:\n",
        "                captured_acts.append(output.detach())\n",
        "        \n",
        "        target_layer_baseline = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "        temp_hook = target_layer_baseline.register_forward_hook(capture_hook)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "        temp_hook.remove()\n",
        "        \n",
        "        if captured_acts:\n",
        "            bert_activation = captured_acts[0].squeeze(0)\n",
        "            \n",
        "            # Filter special tokens\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            valid_mask = attention_mask & not_special\n",
        "            bert_activation = bert_activation[valid_mask]\n",
        "            \n",
        "            if bert_activation.shape[0] > 0:\n",
        "                # Get SAE features\n",
        "                sae_features = sae.encode(bert_activation)\n",
        "                sae_features_cpu = sae_features.detach().cpu().numpy()\n",
        "                \n",
        "                # Get max activation per feature across all tokens\n",
        "                max_activations_per_feature = sae_features_cpu.max(axis=0)\n",
        "                \n",
        "                # Get top 10 features\n",
        "                top_10_indices = np.argsort(max_activations_per_feature)[-10:][::-1]\n",
        "                top_features = [\n",
        "                    {\n",
        "                        \"feature_id\": int(fid), \n",
        "                        \"activation\": float(max_activations_per_feature[fid])\n",
        "                    }\n",
        "                    for fid in top_10_indices\n",
        "                ]\n",
        "                total_activation = sum(feat[\"activation\"] for feat in top_features)\n",
        "                \n",
        "                baseline_features_map[idx] = {\n",
        "                    \"top_features\": top_features,\n",
        "                    \"total_activation\": total_activation\n",
        "                }\n",
        "            else:\n",
        "                baseline_features_map[idx] = {\n",
        "                    \"top_features\": [],\n",
        "                    \"total_activation\": 0.0\n",
        "                }\n",
        "        else:\n",
        "            baseline_features_map[idx] = {\n",
        "                \"top_features\": [],\n",
        "                \"total_activation\": 0.0\n",
        "            }\n",
        "        \n",
        "        if (idx + 1) % 20 == 0:\n",
        "            print(f\"  Baseline: {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
        "\n",
        "baseline_accuracy = sum(1 for r in baseline_results if r[\"predicted_id\"] == test_ds[r[\"sample_idx\"]][\"label\"]) / len(baseline_results)\n",
        "print(f\"âœ“ Baseline accuracy: {baseline_accuracy:.2%}\\n\")\n",
        "\n",
        "# Determine features to ablate based on mode\n",
        "if ABLATION_CONFIG[\"mode\"] == \"manual\":\n",
        "    FEATURES_TO_ABLATE = MANUAL_FEATURES\n",
        "    print(f\"\\nMode 1 (Manual): Ablating {len(FEATURES_TO_ABLATE)} manually specified features\")\n",
        "elif ABLATION_CONFIG[\"mode\"] == \"union_top_k\":\n",
        "    feature_set = set()\n",
        "    for idx in baseline_features_map:\n",
        "        top_k_ids = [f['feature_id'] for f in baseline_features_map[idx]['top_features'][:ABLATION_CONFIG[\"k\"]]]\n",
        "        feature_set.update(top_k_ids)\n",
        "    FEATURES_TO_ABLATE = sorted(list(feature_set))\n",
        "    print(f\"\\nMode 3 (Union Top-K): Collected {len(FEATURES_TO_ABLATE)} unique features from union of top-{ABLATION_CONFIG['k']} across {len(baseline_results)} samples\")\n",
        "elif ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "    FEATURES_TO_ABLATE = None\n",
        "    print(f\"\\nMode 2 (Per-Sample Top-K): Will ablate top-{ABLATION_CONFIG['k']} features individually for each sample\")\n",
        "else:\n",
        "    raise ValueError(f\"Unknown ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "\n",
        "# Verify features are within valid range\n",
        "if ABLATION_CONFIG[\"mode\"] != \"per_sample_top_k\":\n",
        "    if any(fid < 0 or fid >= SAE_LATENT_DIM for fid in FEATURES_TO_ABLATE):\n",
        "        invalid = [fid for fid in FEATURES_TO_ABLATE if fid < 0 or fid >= SAE_LATENT_DIM]\n",
        "        raise ValueError(f\"Invalid feature IDs (must be 0-{SAE_LATENT_DIM-1}): {invalid}\")\n",
        "\n",
        "print(f\"Features to ablate: {FEATURES_TO_ABLATE if FEATURES_TO_ABLATE else 'Per-sample dynamic'}\\n\")\n",
        "\n",
        "# Register intervention hook (Mode 1 & 3: global hook)\n",
        "if ABLATION_CONFIG[\"mode\"] != \"per_sample_top_k\":\n",
        "    target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "    intervention_hook = create_intervention_hook(sae, FEATURES_TO_ABLATE, device, current_sample_data)\n",
        "    hook_handle = target_layer.register_forward_hook(intervention_hook)\n",
        "\n",
        "# Run ablation inference\n",
        "print(\"ðŸ”¬ Running ablation inference (features zeroed)...\")\n",
        "ablated_results = []\n",
        "\n",
        "baseline_lookup = {r[\"sample_idx\"]: r for r in baseline_results}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(test_ds):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
        "        \n",
        "        # Get string tokens for display (properly cleaned)\n",
        "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        prompt_tokens = []\n",
        "        for tok in raw_tokens:\n",
        "            if tok.startswith(\"##\"):\n",
        "                prompt_tokens.append(tok[2:])  # Remove ##\n",
        "            else:\n",
        "                prompt_tokens.append(tok)\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Mode 2: Register per-sample hook\n",
        "        if ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "            features_to_ablate_sample = [\n",
        "                f['feature_id']\n",
        "                for f in baseline_features_map[idx]['top_features'][:ABLATION_CONFIG[\"k\"]]\n",
        "            ]\n",
        "            target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "            intervention_hook = create_intervention_hook(\n",
        "                sae, features_to_ablate_sample, device, current_sample_data\n",
        "            )\n",
        "            hook_handle = target_layer.register_forward_hook(intervention_hook)\n",
        "\n",
        "        # Clear sample data\n",
        "        current_sample_data[\"sae_features\"] = None\n",
        "        current_sample_data[\"token_ids\"] = token_ids\n",
        "        current_sample_data[\"prompt_tokens\"] = prompt_tokens\n",
        "        current_sample_data[\"text\"] = text\n",
        "        current_sample_data[\"idx\"] = idx\n",
        "\n",
        "        # Forward pass with intervention (features ablated)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Mode 2: Remove per-sample hook\n",
        "        if ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "            hook_handle.remove()\n",
        "\n",
        "        pred_id = logits.argmax(dim=-1).item()\n",
        "        pred_label = model.config.id2label[pred_id]\n",
        "        confidence = probs[0, pred_id].item()\n",
        "\n",
        "        # Track which features were ablated for this sample\n",
        "        if ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "            features_ablated_for_this_sample = [\n",
        "                f[\"feature_id\"]\n",
        "                for f in baseline_features_map[idx][\"top_features\"][:ABLATION_CONFIG[\"k\"]]\n",
        "            ]\n",
        "        else:\n",
        "            features_ablated_for_this_sample = FEATURES_TO_ABLATE\n",
        "\n",
        "        ablated_results.append({\n",
        "                    \"sample_idx\": idx,\n",
        "                    \"text\": text,\n",
        "                    \"true_label\": model.config.id2label[true_label],\n",
        "                    \"predicted_label\": pred_label,\n",
        "                    \"predicted_id\": pred_id,\n",
        "                    \"confidence\": confidence,\n",
        "                    \"logits\": logits.detach().cpu().numpy(),\n",
        "                    \"probs\": probs.detach().cpu().numpy(),\n",
        "                    \"ablated_features\": features_ablated_for_this_sample\n",
        "                })\n",
        "\n",
        "        # Track SAE features (after ablation) if we captured them\n",
        "        if current_sample_data[\"sae_features\"] is not None:\n",
        "            sae_features_cpu = current_sample_data[\"sae_features\"].cpu().numpy()\n",
        "            \n",
        "            # Filter special tokens (same as inference cell)\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool().cpu().numpy()\n",
        "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0).cpu().numpy()\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = np.array([tid not in special_ids for tid in token_ids_tensor])\n",
        "            valid_mask = attention_mask & not_special\n",
        "            \n",
        "            # Filter features and tokens\n",
        "            sae_features_filtered = sae_features_cpu[valid_mask]\n",
        "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask) if valid]\n",
        "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask) if valid]\n",
        "            \n",
        "            if sae_features_filtered.shape[0] > 0:\n",
        "                seq_len = sae_features_filtered.shape[0]\n",
        "                \n",
        "                # Update feature statistics\n",
        "                feature_stats_ablated.update(sae_features_filtered)\n",
        "                \n",
        "                # Track top tokens per feature\n",
        "                top_token_tracker_ablated.update(\n",
        "                    sae_features_filtered,\n",
        "                    filtered_token_ids,\n",
        "                    prompt_idx=idx,\n",
        "                    prompt_text=text,\n",
        "                    prompt_tokens=filtered_prompt_tokens,\n",
        "                    predicted_label=pred_label,\n",
        "                    true_label=model.config.id2label[true_label]\n",
        "                )\n",
        "                \n",
        "                # Aggregate top features at headline level\n",
        "                baseline_data = baseline_lookup[idx]\n",
        "                if ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "                    features_for_tracking = [\n",
        "                        f['feature_id']\n",
        "                        for f in baseline_features_map[idx]['top_features'][:ABLATION_CONFIG[\"k\"]]\n",
        "                    ]\n",
        "                else:\n",
        "                    features_for_tracking = FEATURES_TO_ABLATE\n",
        "\n",
        "                headline_aggregator_ablated.add_headline_with_ablation_metrics(\n",
        "                    prompt_idx=idx,\n",
        "                    prompt_text=text,\n",
        "                    token_activations=sae_features_filtered,\n",
        "                    token_ids=filtered_token_ids,\n",
        "                    token_strings=filtered_prompt_tokens,\n",
        "                    predicted_label=pred_label,\n",
        "                    true_label=model.config.id2label[true_label],\n",
        "                    confidence=confidence,\n",
        "                    baseline_features=baseline_features_map[idx],\n",
        "                    features_to_ablate=features_for_tracking,\n",
        "                    baseline_prediction=baseline_data[\"predicted_label\"],\n",
        "                    baseline_confidence=baseline_data[\"confidence\"]\n",
        "                )\n",
        "                \n",
        "                # Save prompt metadata\n",
        "                all_prompt_metadata_ablated.append({\n",
        "                    \"row_id\": idx,\n",
        "                    \"seq_len\": seq_len,\n",
        "                    \"prompt\": text,\n",
        "                    \"predicted_label\": pred_label,\n",
        "                    \"true_label\": model.config.id2label[true_label],\n",
        "                    \"correct\": pred_id == true_label\n",
        "                })\n",
        "\n",
        "if (idx + 1) % 20 == 0:\n",
        "    print(f\"  Ablated: {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
        "\n",
        "# Remove hook (Mode 1 & 3 only, Mode 2 removes per-sample)\n",
        "if ABLATION_CONFIG[\"mode\"] != \"per_sample_top_k\":\n",
        "    hook_handle.remove()\n",
        "\n",
        "ablated_accuracy = sum(1 for r in ablated_results if r[\"predicted_id\"] == test_ds[r[\"sample_idx\"]][\"label\"]) / len(ablated_results)\n",
        "print(f\"âœ“ Ablated accuracy: {ablated_accuracy:.2%}\\n\")\n",
        "\n",
        "# Compare results and find flipped predictions\n",
        "flipped_samples = []\n",
        "for baseline, ablated in zip(baseline_results, ablated_results):\n",
        "    if baseline[\"predicted_id\"] != ablated[\"predicted_id\"]:\n",
        "        # Get top SAE features for this sample (from baseline run)\n",
        "        # We need to capture activations for this sample\n",
        "        # For now, we'll compute them on-the-fly\n",
        "        \n",
        "        # Tokenize and get activations\n",
        "        inputs = tokenizer(baseline[\"text\"], return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # Capture activations\n",
        "        captured_acts = []\n",
        "        def capture_hook(module, input, output):\n",
        "            if isinstance(output, tuple):\n",
        "                captured_acts.append(output[0].detach())\n",
        "            else:\n",
        "                captured_acts.append(output.detach())\n",
        "        \n",
        "        temp_hook = target_layer.register_forward_hook(capture_hook)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "        temp_hook.remove()\n",
        "        \n",
        "        if captured_acts:\n",
        "            bert_activation = captured_acts[0].squeeze(0)\n",
        "            \n",
        "            # Filter special tokens\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            valid_mask = attention_mask & not_special\n",
        "            bert_activation = bert_activation[valid_mask]\n",
        "            \n",
        "            if bert_activation.shape[0] > 0:\n",
        "                # Get SAE features\n",
        "                sae_features = sae.encode(bert_activation)\n",
        "                sae_features_cpu = sae_features.detach().cpu().numpy()\n",
        "                \n",
        "                # Get max activation per feature across all tokens\n",
        "                max_activations_per_feature = sae_features_cpu.max(axis=0)\n",
        "                \n",
        "                # Get top 10 features\n",
        "                top_10_indices = np.argsort(max_activations_per_feature)[-10:][::-1]\n",
        "                ablated_features_for_sample = ablated.get(\"ablated_features\") or []\n",
        "                top_features = [\n",
        "                    {\"feature_id\": int(fid), \"activation\": float(max_activations_per_feature[fid]), \n",
        "                     \"ablated\": fid in ablated_features_for_sample}\n",
        "                    for fid in top_10_indices\n",
        "                ]\n",
        "            else:\n",
        "                top_features = []\n",
        "        else:\n",
        "            top_features = []\n",
        "        \n",
        "        flipped_samples.append({\n",
        "            \"sample_idx\": baseline[\"sample_idx\"],\n",
        "            \"text\": baseline[\"text\"],\n",
        "            \"true_label\": baseline[\"true_label\"],\n",
        "            \"baseline_pred\": baseline[\"predicted_label\"],\n",
        "            \"baseline_conf\": baseline[\"confidence\"],\n",
        "            \"ablated_pred\": ablated[\"predicted_label\"],\n",
        "            \"ablated_conf\": ablated[\"confidence\"],\n",
        "            \"top_features\": top_features\n",
        "        })\n",
        "\n",
        "# Print results\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "if FEATURES_TO_ABLATE is not None:\n",
        "    print(f\"Ablated Features: {FEATURES_TO_ABLATE}\")\n",
        "else:\n",
        "    print(f\"Ablated Features: Per-sample top-{ABLATION_CONFIG['k']}\")\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.2%}\")\n",
        "print(f\"Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"Accuracy Change: {(ablated_accuracy - baseline_accuracy):.2%}\")\n",
        "print(f\"\\nFlipped Predictions: {len(flipped_samples)}/{len(baseline_results)} samples\")\n",
        "print(f\"Flip Rate: {len(flipped_samples)/len(baseline_results):.2%}\\n\")\n",
        "\n",
        "if flipped_samples:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"FLIPPED PREDICTIONS (showing first {min(10, len(flipped_samples))}):\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, flip in enumerate(flipped_samples[:10], 1):\n",
        "        print(f\"\\n--- Sample #{flip['sample_idx']} ---\")\n",
        "        print(f\"Text: {flip['text'][:120]}{'...' if len(flip['text']) > 120 else ''}\")\n",
        "        print(f\"True Label: {flip['true_label']}\")\n",
        "        print(f\"Original: {flip['baseline_pred']} (conf: {flip['baseline_conf']:.3f}) â†’ \"\n",
        "              f\"Ablated: {flip['ablated_pred']} (conf: {flip['ablated_conf']:.3f})\")\n",
        "        \n",
        "        if flip['top_features']:\n",
        "            print(\"Top 10 SAE Features:\")\n",
        "            for feat in flip['top_features']:\n",
        "                ablated_marker = \" [ABLATED]\" if feat['ablated'] else \"\"\n",
        "                print(f\"  Feature {feat['feature_id']}: {feat['activation']:.4f}{ablated_marker}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No predictions were flipped by ablating these features.\")\n",
        "\n",
        "# Save ablated results in the same format as inference cell\n",
        "print(\"\\nðŸ’¾ Saving ablated results for visualization...\")\n",
        "\n",
        "# Create run directory for ablated results (use directly, don't create separate folder)\n",
        "ablated_run_dir = make_analysis_run_dir(str(repo_root))\n",
        "print(f\"ðŸ’¾ Saving ablated results to: {ablated_run_dir}\")\n",
        "\n",
        "# Compute final statistics for ablated run\n",
        "stats_ablated = feature_stats_ablated.get_stats()\n",
        "\n",
        "# Get top features for each metric\n",
        "top_features_by_metric_ablated = {}\n",
        "for metric_name, values in stats_ablated.items():\n",
        "    if metric_name == \"mean_act_squared\":\n",
        "        continue\n",
        "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
        "    top_features_by_metric_ablated[metric_name] = [\n",
        "        {\n",
        "            \"feature_id\": int(idx),\n",
        "            \"value\": float(values[idx]),\n",
        "            \"metrics\": {\n",
        "                \"mean_activation\": float(stats_ablated[\"mean_activation\"][idx]),\n",
        "                \"max_activation\": float(stats_ablated[\"max_activation\"][idx]),\n",
        "                \"fraction_active\": float(stats_ablated[\"fraction_active\"][idx])\n",
        "            }\n",
        "        }\n",
        "        for idx in top_indices\n",
        "    ]\n",
        "\n",
        "# 1. Save prompts metadata\n",
        "prompts_file = ablated_run_dir / \"prompts.jsonl\"\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in all_prompt_metadata_ablated:\n",
        "        json.dump(meta, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# 2. Save feature statistics\n",
        "feature_stats_file = ablated_run_dir / \"feature_stats.json\"\n",
        "feature_stats_data = {\n",
        "    \"num_features\": SAE_LATENT_DIM,\n",
        "    \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "    \"top_feature_count\": TOP_FEATURES,\n",
        "    \"accuracy\": ablated_accuracy,\n",
        "    \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "    \"mean_act_squared\": stats_ablated[\"mean_act_squared\"].tolist(),\n",
        "    \"metrics\": {\n",
        "        metric_name: {\n",
        "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
        "            \"top_features\": top_features_by_metric_ablated[metric_name]\n",
        "        }\n",
        "        for metric_name in stats_ablated.keys() if metric_name != \"mean_act_squared\"\n",
        "    }\n",
        "}\n",
        "with open(feature_stats_file, \"w\") as f:\n",
        "    json.dump(feature_stats_data, f, indent=2)\n",
        "\n",
        "# 3. Save top tokens per feature\n",
        "feature_tokens_file = ablated_run_dir / \"feature_tokens.json\"\n",
        "feature_tokens_data = {\n",
        "    \"features\": top_token_tracker_ablated.export()\n",
        "}\n",
        "with open(feature_tokens_file, \"w\") as f:\n",
        "    json.dump(feature_tokens_data, f, indent=2)\n",
        "\n",
        "# 4. Save headline-level features\n",
        "headline_features_file = ablated_run_dir / \"headline_features.json\"\n",
        "with open(headline_features_file, \"w\") as f:\n",
        "    json.dump(headline_aggregator_ablated.export(), f, indent=2)\n",
        "\n",
        "# 5. Save metadata\n",
        "metadata_file = ablated_run_dir / \"metadata.json\"\n",
        "with open(metadata_file, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"model\": save_dir,\n",
        "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
        "        \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "        \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "        \"accuracy\": ablated_accuracy,\n",
        "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
        "        \"split\": \"validation\",\n",
        "        \"hidden_dim\": SAE_INPUT_DIM,\n",
        "        \"latent_dim\": SAE_LATENT_DIM,\n",
        "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
        "        \"top_features_per_metric\": TOP_FEATURES,\n",
        "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
        "        \"ablation_mode\": ABLATION_CONFIG[\"mode\"],\n",
        "        \"ablated_features\": FEATURES_TO_ABLATE if FEATURES_TO_ABLATE else \"per_sample_dynamic\",\n",
        "        \"ablation_k\": ABLATION_CONFIG.get(\"k\"),\n",
        "        \"note\": f\"SAE sparse features with predictions (mode: {ABLATION_CONFIG['mode']})\"\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Ablation experiment complete!\")\n",
        "print(f\"   ðŸ“ Ablated results saved to: {ablated_run_dir.name}\")\n",
        "print(f\"   ðŸŽ¯ Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"   ðŸ”¢ Total tokens: {feature_stats_ablated.total_tokens}\")\n",
        "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
        "print(f\"\\nðŸŒ Start the viewer to see ablated results:\")\n",
        "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
        "print(f\"   cd sae-viewer && npm start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eefe6eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference (non refactored)\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import heapq\n",
        "from typing import List, Tuple\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# Start of Inference\n",
        "# Add project root to path to import utilities\n",
        "repo_root = Path(\".\").resolve()\n",
        "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
        "\n",
        "from utils.run_dirs import make_analysis_run_dir\n",
        "\n",
        "# Configuration\n",
        "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
        "MAX_SAMPLES = 100  # Limit for testing\n",
        "TOP_FEATURES = 100  # Top features to track per metric\n",
        "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
        "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
        "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the SAE using the helper function\n",
        "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
        "\n",
        "# Extract dimensions from the loaded config\n",
        "SAE_INPUT_DIM = sae_config['input_dim']\n",
        "SAE_LATENT_DIM = sae_config['latent_dim']\n",
        "\n",
        "print(f\"âœ“ SAE loaded: {SAE_INPUT_DIM} dims â†’ {SAE_LATENT_DIM} sparse features\")\n",
        "\n",
        "# Create run directory using the same utility as main.py\n",
        "# This ensures the server can find it automatically in analysis_data/\n",
        "run_dir = make_analysis_run_dir(str(repo_root))\n",
        "print(f\"\\nðŸ’¾ Saving results to: {run_dir}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "sae.to(device)\n",
        "model.eval()\n",
        "sae.eval()\n",
        "\n",
        "# Load dataset\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
        "\n",
        "# Feature statistics tracker (per-token aggregation)\n",
        "class FeatureStatsAggregator:\n",
        "    def __init__(self, feature_dim: int):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.total_tokens = 0\n",
        "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
        "    \n",
        "    def update(self, token_activations: np.ndarray):\n",
        "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
        "        self.total_tokens += token_activations.shape[0]\n",
        "        self.sum_activations += token_activations.sum(axis=0)\n",
        "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
        "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
        "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
        "    \n",
        "    def get_stats(self):\n",
        "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
        "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
        "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
        "        return {\n",
        "            \"mean_activation\": mean_act,\n",
        "            \"max_activation\": self.max_activations,\n",
        "            \"fraction_active\": frac_active,\n",
        "            \"mean_act_squared\": mean_act_squared\n",
        "        }\n",
        "\n",
        "# Top token tracker per feature\n",
        "class FeatureTopTokenTracker:\n",
        "    def __init__(self, feature_dim: int, top_k: int):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.top_k = top_k\n",
        "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
        "        self.heaps = [[] for _ in range(feature_dim)]\n",
        "    \n",
        "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
        "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
        "               predicted_label: str = None, true_label: str = None):\n",
        "        \"\"\"Update with tokens from one prompt\"\"\"\n",
        "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
        "            # For each token, find top features\n",
        "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
        "            \n",
        "            for feat_id in top_features:\n",
        "                activation = float(act_vec[feat_id])\n",
        "                if activation <= 0:\n",
        "                    continue\n",
        "                \n",
        "                heap = self.heaps[feat_id]\n",
        "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
        "                \n",
        "                metadata = {\n",
        "                    \"activation\": activation,\n",
        "                    \"token_str\": token_str,\n",
        "                    \"token_id\": int(token_id),\n",
        "                    \"token_position\": int(token_pos),\n",
        "                    \"prompt_index\": int(prompt_idx),\n",
        "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
        "                    \"prompt_snippet\": prompt_text[:160],\n",
        "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
        "                    \"prompt_tokens\": prompt_tokens,\n",
        "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
        "                    \"true_label\": true_label,\n",
        "                }\n",
        "                \n",
        "                if len(heap) < self.top_k:\n",
        "                    heapq.heappush(heap, (activation, metadata))\n",
        "                elif activation > heap[0][0]:\n",
        "                    heapq.heapreplace(heap, (activation, metadata))\n",
        "    \n",
        "    def export(self):\n",
        "        \"\"\"Export top tokens for each feature\"\"\"\n",
        "        result = {}\n",
        "        for feat_id in range(self.feature_dim):\n",
        "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
        "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
        "        return result\n",
        "\n",
        "# Aggregate top features per headline (sample-level view)\n",
        "class HeadlineFeatureAggregator:\n",
        "    def __init__(self, top_k: int = 10):\n",
        "        self.top_k = top_k\n",
        "        self.headlines = []  # List of headline metadata with top features\n",
        "    \n",
        "    def add_headline(self, prompt_idx: int, prompt_text: str,\n",
        "                     token_activations: np.ndarray,\n",
        "                     token_ids: List[int],\n",
        "                     token_strings: List[str],\n",
        "                     predicted_label: str, true_label: str):\n",
        "        \"\"\"Aggregate features across all tokens in a headline\"\"\"\n",
        "        if token_activations.size == 0:\n",
        "            return\n",
        "        # Max activation per feature and which token triggered it\n",
        "        max_token_idx_per_feature = token_activations.argmax(axis=0)  # [feature_dim]\n",
        "        max_activation_per_feature = token_activations.max(axis=0)     # [feature_dim]\n",
        "        \n",
        "        # Get top K features by their max activation in this headline\n",
        "        top_feature_ids = np.argsort(max_activation_per_feature)[-self.top_k:][::-1]\n",
        "        \n",
        "        features = [\n",
        "            {\n",
        "                \"feature_id\": int(fid),\n",
        "                \"max_activation\": float(max_activation_per_feature[fid]),\n",
        "                \"token_position\": int(max_token_idx_per_feature[fid]),\n",
        "                \"token_id\": int(token_ids[max_token_idx_per_feature[fid]]),\n",
        "                \"token_str\": token_strings[max_token_idx_per_feature[fid]],\n",
        "            }\n",
        "            for fid in top_feature_ids if max_activation_per_feature[fid] > 0\n",
        "        ]\n",
        "        \n",
        "        self.headlines.append({\n",
        "            \"row_id\": int(prompt_idx),\n",
        "            \"prompt\": prompt_text,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"true_label\": true_label,\n",
        "            \"correct\": predicted_label == true_label,\n",
        "            \"num_tokens\": int(token_activations.shape[0]),\n",
        "            \"features\": features\n",
        "        })\n",
        "    \n",
        "    def export(self):\n",
        "        return self.headlines\n",
        "\n",
        "# Initialize trackers for SAE features\n",
        "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
        "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
        "headline_aggregator = HeadlineFeatureAggregator(top_k=10)\n",
        "\n",
        "# Storage for per-sample metadata\n",
        "all_prompt_metadata = []\n",
        "all_prediction_metadata = []\n",
        "\n",
        "# Hook to capture activations\n",
        "captured_activations = []\n",
        "\n",
        "def capture_hook(module, input, output):\n",
        "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
        "    if isinstance(output, tuple):\n",
        "        hidden_states = output[0]\n",
        "    else:\n",
        "        hidden_states = output\n",
        "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
        "\n",
        "# Register hook on target layer\n",
        "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
        "\n",
        "print(f\"\\nðŸ”¬ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
        "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
        "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
        "\n",
        "# Process samples\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(test_ds):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize with truncation\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
        "        \n",
        "        # Get string tokens for display (properly cleaned)\n",
        "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
        "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        prompt_tokens = []\n",
        "        for tok in raw_tokens:\n",
        "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
        "            if tok.startswith(\"##\"):\n",
        "                prompt_tokens.append(tok[2:])  # Remove ##\n",
        "            else:\n",
        "                prompt_tokens.append(tok)\n",
        "        \n",
        "        # Forward pass\n",
        "        inputs = inputs.to(device)\n",
        "        captured_activations.clear()\n",
        "        outputs = model(**inputs)\n",
        "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
        "        pred_label = model.config.id2label[pred_id]\n",
        "        \n",
        "        # Get captured activation and pass through SAE\n",
        "        if captured_activations:\n",
        "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
        "            bert_activation = captured_activations[0].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (same as training) - do on GPU\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            \n",
        "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
        "            \n",
        "            # Filter activations on GPU\n",
        "            bert_activation = bert_activation[valid_mask]\n",
        "            \n",
        "            # Skip if no valid tokens\n",
        "            if bert_activation.shape[0] == 0:\n",
        "                continue\n",
        "            \n",
        "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
        "            sae_features = sae.encode(bert_activation)\n",
        "            \n",
        "            # Only now move to CPU for numpy conversion and token filtering\n",
        "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
        "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
        "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
        "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
        "            \n",
        "            seq_len = sae_features_cpu.shape[0]\n",
        "            \n",
        "            # Update feature statistics with SAE features\n",
        "            feature_stats.update(sae_features_cpu)\n",
        "            \n",
        "            # Track top tokens per feature\n",
        "            top_token_tracker.update(\n",
        "                sae_features_cpu, \n",
        "                filtered_token_ids, \n",
        "                prompt_idx=idx,\n",
        "                prompt_text=text,\n",
        "                prompt_tokens=filtered_prompt_tokens,\n",
        "                predicted_label=pred_label,  # Pass prediction info\n",
        "                true_label=model.config.id2label[true_label]\n",
        "            )\n",
        "            \n",
        "            # Aggregate top features at headline level\n",
        "            headline_aggregator.add_headline(\n",
        "                prompt_idx=idx,\n",
        "                prompt_text=text,\n",
        "                token_activations=sae_features_cpu,\n",
        "                token_ids=filtered_token_ids,\n",
        "                token_strings=filtered_prompt_tokens,\n",
        "                predicted_label=pred_label,\n",
        "                true_label=model.config.id2label[true_label],\n",
        "                confidence=confidence\n",
        "            )\n",
        "            \n",
        "            # Save prompt metadata\n",
        "            all_prompt_metadata.append({\n",
        "                \"row_id\": idx,\n",
        "                \"seq_len\": seq_len,\n",
        "                \"prompt\": text,\n",
        "                \"predicted_label\": pred_label,\n",
        "                \"true_label\": model.config.id2label[true_label],\n",
        "                \"correct\": pred_id == true_label\n",
        "            })\n",
        "        \n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
        "\n",
        "# Remove hook\n",
        "hook_handle.remove()\n",
        "\n",
        "# Compute final statistics\n",
        "print(\"\\nðŸ“Š Computing feature statistics...\")\n",
        "stats = feature_stats.get_stats()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
        "print(f\"ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Get top features for each metric\n",
        "top_features_by_metric = {}\n",
        "for metric_name, values in stats.items():\n",
        "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
        "    top_features_by_metric[metric_name] = [\n",
        "        {\n",
        "            \"feature_id\": int(idx),\n",
        "            \"value\": float(values[idx]),\n",
        "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
        "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
        "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
        "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
        "            }\n",
        "        }\n",
        "        for idx in top_indices\n",
        "    ]\n",
        "\n",
        "# Save results\n",
        "print(\"\\nðŸ’¾ Saving results...\")\n",
        "\n",
        "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
        "prompts_file = run_dir / \"prompts.jsonl\"\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in all_prompt_metadata:\n",
        "        json.dump(meta, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
        "feature_stats_file = run_dir / \"feature_stats.json\"\n",
        "feature_stats_data = {\n",
        "    \"num_features\": SAE_LATENT_DIM,\n",
        "    \"total_tokens\": feature_stats.total_tokens,\n",
        "    \"top_feature_count\": TOP_FEATURES,\n",
        "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
        "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
        "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
        "    \"metrics\": {\n",
        "        metric_name: {\n",
        "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
        "            \"top_features\": top_features_by_metric[metric_name]\n",
        "        }\n",
        "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
        "    }\n",
        "}\n",
        "with open(feature_stats_file, \"w\") as f:\n",
        "    json.dump(feature_stats_data, f, indent=2)\n",
        "\n",
        "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
        "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
        "feature_tokens_data = {\n",
        "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
        "}\n",
        "with open(feature_tokens_file, \"w\") as f:\n",
        "    json.dump(feature_tokens_data, f, indent=2)\n",
        "\n",
        "# 4. Save headline-level features\n",
        "headline_features_file = run_dir / \"headline_features.json\"\n",
        "with open(headline_features_file, \"w\") as f:\n",
        "    json.dump(headline_aggregator.export(), f, indent=2)\n",
        "\n",
        "# 5. Save metadata\n",
        "metadata_file = run_dir / \"metadata.json\"\n",
        "with open(metadata_file, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"model\": save_dir,\n",
        "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
        "        \"num_samples\": len(all_prompt_metadata),\n",
        "        \"total_tokens\": feature_stats.total_tokens,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
        "        \"split\": \"validation\",\n",
        "        \"hidden_dim\": SAE_INPUT_DIM,\n",
        "        \"latent_dim\": SAE_LATENT_DIM,\n",
        "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
        "        \"top_features_per_metric\": TOP_FEATURES,\n",
        "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
        "        \"note\": \"SAE sparse features with predictions\"\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… COMPLETE!\")\n",
        "print(f\"   ðŸ“ Results saved to: {run_dir.name}\")\n",
        "print(f\"   ðŸŽ¯ Accuracy: {accuracy:.2%}\")\n",
        "print(f\"   ðŸ”¢ Total tokens: {feature_stats.total_tokens}\")\n",
        "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
        "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
        "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
        "    metrics = feat['metrics']\n",
        "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
        "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
        "          f\"max={metrics['max_activation']:.4f}, \"\n",
        "          f\"frac={metrics['fraction_active']:.2%}\")\n",
        "\n",
        "print(f\"\\nðŸŒ Start the viewer to see results:\")\n",
        "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
        "print(f\"   cd sae-viewer && npm start\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def14c72",
      "metadata": {},
      "source": [
        "Testing Inference based on Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de85884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick analysis on simple headlines\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "\n",
        "example_sentences = [\n",
        "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
        "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
        "    \"The company reported results largely in line with analyst expectations.\",\n",
        "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
        "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
        "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
        "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
        "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
        "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
        "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
        "]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# optional: move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def predict_sentiment(text: str):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs)\n",
        "    pred_id = out.logits.argmax(dim=-1).item()\n",
        "    return model.config.id2label[pred_id]\n",
        "\n",
        "for text in example_sentences:\n",
        "    label = predict_sentiment(text)\n",
        "    print(f\"{label.upper():8} | {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f0b474",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Visualisation for Dataset\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
        "\n",
        "test_ds[\"text\"][0:200]\n",
        "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "#ds2[\"validation\"][\"text\"][34]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce67a413",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference WITHOUT SAEs - Plain Model Accuracy on Test Data\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL INFERENCE WITHOUT SAEs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Use validation set for evaluation\n",
        "test_ds = ds[\"validation\"]\n",
        "# MAX_SAMPLES = len(test_ds)  # Process all samples, or set a limit if needed\n",
        "MAX_SAMPLES = 100\n",
        "MAX_SEQ_LENGTH = 64\n",
        "\n",
        "print(f\"\\nðŸ”¬ Running inference on {MAX_SAMPLES} test samples...\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Model: {save_dir}\\n\")\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Process samples\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(test_ds, desc=\"Processing\")):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize with truncation\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
        "        \n",
        "        # Check if prediction is correct\n",
        "        if pred_id == true_label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"âœ… INFERENCE COMPLETE (WITHOUT SAEs)\")\n",
        "print(f\"{'=' * 60}\")\n",
        "print(f\"   ðŸ“Š Total Samples: {total_predictions}\")\n",
        "print(f\"   âœ“ Correct Predictions: {correct_predictions}\")\n",
        "print(f\"   âœ— Incorrect Predictions: {total_predictions - correct_predictions}\")\n",
        "print(f\"   ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
        "print(f\"{'=' * 60}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
