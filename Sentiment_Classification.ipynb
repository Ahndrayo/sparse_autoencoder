{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "deae2746",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method signature verified: 13 parameters\n",
            "Torch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import heapq\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "from typing import List\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "\n",
        "# Add project root to Python path\n",
        "repo_root = Path(\".\").resolve()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "# Import helper utilities from organized modules\n",
        "import importlib\n",
        "\n",
        "# Remove cached modules to force a fresh import\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod.startswith(\"utils.\") or mod.startswith(\"sparse_autoencoder.\"):\n",
        "        del sys.modules[mod]\n",
        "\n",
        "import utils.finbert\n",
        "import utils.analysis\n",
        "import utils.ablation\n",
        "import utils.run_dirs\n",
        "import sparse_autoencoder.finbert_sae\n",
        "import utils.data_cleaning\n",
        "\n",
        "# Force reload to get latest code\n",
        "# importlib.reload(utils.finbert)\n",
        "# importlib.reload(utils.analysis)\n",
        "# importlib.reload(utils.ablation)\n",
        "# importlib.reload(utils.run_dirs)\n",
        "# importlib.reload(sparse_autoencoder.finbert_sae)\n",
        "\n",
        "from utils.finbert import compute_metrics\n",
        "from utils.analysis import (\n",
        "    FeatureStatsAggregator,\n",
        "    FeatureTopTokenTracker,\n",
        "    HeadlineFeatureAggregator\n",
        ")\n",
        "from utils.ablation import (\n",
        "    create_intervention_hook,\n",
        "    validate_feature_ids,\n",
        "    normalize_decoder_weights,\n",
        "    expand_features_with_similarity,\n",
        "    run_baseline_inference,\n",
        "    run_ablation_inference,\n",
        "    find_flipped_predictions,\n",
        ")\n",
        "from utils.run_dirs import make_analysis_run_dir\n",
        "from sparse_autoencoder.finbert_sae import SparseAutoencoder, load_sae\n",
        "from utils.data_cleaning import clean_text, remove_leading_tickers\n",
        "\n",
        "# Verify correct class is loaded\n",
        "import inspect\n",
        "sig = inspect.signature(HeadlineFeatureAggregator.add_headline_with_ablation_metrics)\n",
        "print(f\"Method signature verified: {len(sig.parameters)} parameters\")\n",
        "\n",
        "# --------- CUDA sanity check ----------\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Define device for SAE loading\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31a1b31e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load dataset, comes with train and validation fold \n",
        "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "\n",
        "\n",
        "# Clean dataset\n",
        "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
        "ds = ds.map(lambda x: {\"text\": remove_leading_tickers(x[\"text\"])})\n",
        "\n",
        "# Load dataset\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8034bf6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants/Hyperparameters for training model and SAE\n",
        "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Configuration for Inference\n",
        "MAX_SAMPLES = 100  # Limit for testing\n",
        "TOP_FEATURES = 100  # Top features to track per metric\n",
        "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
        "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
        "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f233625a",
      "metadata": {},
      "source": [
        "Fine Tune Hyperparameters of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f40e9d6",
      "metadata": {},
      "source": [
        "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~4k to 32k interpretable sparse features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c951276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes SAEs based on BERT.\n",
        "# Configuration\n",
        "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Create SAE save directory\n",
        "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load dataset\n",
        "# train_ds = ds[\"train\"]\n",
        "\n",
        "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
        "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
        "\n",
        "# Collect training activations\n",
        "all_activations = []\n",
        "captured_activations = []\n",
        "\n",
        "def capture_hook(module, input, output):\n",
        "    if isinstance(output, tuple):\n",
        "        hidden_states = output[0]\n",
        "    else:\n",
        "        hidden_states = output\n",
        "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
        "\n",
        "# Register hook\n",
        "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
        "\n",
        "# Collect activations from all training data\n",
        "print(\"Extracting activations from training set...\")\n",
        "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(train_ds)):\n",
        "        text = sample[\"text\"]\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        captured_activations.clear()\n",
        "        _ = model(**inputs)\n",
        "        \n",
        "        if captured_activations:\n",
        "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
        "            activation = captured_activations[0].squeeze(0)\n",
        "            \n",
        "            # Get attention mask and token IDs (keep on GPU)\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            \n",
        "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
        "\n",
        "            # Print the number of valid tokens\n",
        "            # kept = valid_mask.sum().item()\n",
        "            # total = attention_mask.sum().item()\n",
        "            # print(f\"Kept {kept}/{total} tokens\")\n",
        "\n",
        "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
        "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
        "\n",
        "            # print(\"TOKENS:\", tokens)\n",
        "            # print(\"DROPPED:\", dropped_tokens)\n",
        "            # print(\"KEPT:\", kept_tokens)\n",
        "            \n",
        "            # Only keep activations for real content tokens (still on GPU)\n",
        "            activation = activation[valid_mask]\n",
        "            \n",
        "            # Only add if there are real tokens\n",
        "            if activation.shape[0] > 0:\n",
        "                # Move to CPU only when storing for later processing\n",
        "                all_activations.append(activation.cpu())\n",
        "\n",
        "hook_handle.remove()\n",
        "\n",
        "# Flatten all activations into a single tensor [total_tokens, 768]\n",
        "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
        "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
        "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
        "\n",
        "# Train SAEs for each latent dimension\n",
        "for LATENT_DIM in LATENT_DIMS:\n",
        "    print(f\"\\\\n{'='*80}\")\n",
        "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create SAE\n",
        "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
        "    sae.to(device)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # Create DataLoader\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    dataset = TensorDataset(all_activations_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
        "    sae.train()\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        total_l1_loss = 0\n",
        "        \n",
        "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            reconstruction, latent = sae(batch_x)\n",
        "            \n",
        "            # Reconstruction loss (MSE)\n",
        "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
        "            \n",
        "            # L1 sparsity loss\n",
        "            l1_loss = latent.abs().mean()\n",
        "            \n",
        "            # Combined loss\n",
        "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Renormalize decoder weights (standard SAE practice)\n",
        "            with torch.no_grad():\n",
        "                sae.decoder.weight.data = nn.functional.normalize(\n",
        "                    sae.decoder.weight.data, dim=0\n",
        "                )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_l1_loss += l1_loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_recon = total_recon_loss / len(dataloader)\n",
        "        avg_l1 = total_l1_loss / len(dataloader)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
        "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
        "    \n",
        "    # Save the trained SAE\n",
        "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
        "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
        "    torch.save({\n",
        "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
        "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
        "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
        "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
        "        'config': {\n",
        "            'input_dim': 768,\n",
        "            'latent_dim': LATENT_DIM,\n",
        "            'layer': LAYER_TO_EXTRACT,\n",
        "            'model': save_dir,\n",
        "        }\n",
        "    }, SAE_SAVE_PATH)\n",
        "    \n",
        "    # Test sparsity\n",
        "    sae.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_acts = all_activations_tensor[:1000].to(device)\n",
        "        sample_latent = sae.encode(sample_acts)\n",
        "        sparsity = (sample_latent > 0).float().mean()\n",
        "        print(f\"\\\\nâœ“ SAE trained successfully!\")\n",
        "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
        "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"All SAEs trained successfully!\")\n",
        "print(f\"Available SAE models:\")\n",
        "for dim in LATENT_DIMS:\n",
        "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
        "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d70e94b",
      "metadata": {},
      "source": [
        "Finetune FinBERT Model\n",
        "\n",
        "The FinBERT model is trained on the training fold of our dataset to improve its prediction accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0697b2e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes the FINBERT model.\n",
        "\n",
        "# 2) Load model/tokenizer\n",
        "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# 3) Tokenize\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
        "val_tok = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
        "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
        "\n",
        "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4) Metrics\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# 5) Training config\n",
        "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_twitter_ft\",\n",
        "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
        "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
        "    report_to=\"none\",              # avoids needing wandb, etc.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
        "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f30b519",
      "metadata": {},
      "source": [
        "Inference with Interpretability\n",
        "\n",
        "We use our FinBERT + SAE on test data. We extract a Layer Activations with Sentiment Predictions (SAE-style Analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a64315a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ABLATION EXPERIMENT\n",
            "============================================================\n",
            "Ablation Mode: union_top_k\n",
            "K value: 10\n",
            "Similarity Expansion: True\n",
            "Similarity Top-M: 100\n",
            "âœ“ Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
            "  Layer: 8\n",
            "  Input dim: 768\n",
            "  Latent dim: 32768\n",
            "Ablation mode: union_top_k\n",
            "Layer: 8\n",
            "SAE Size: 32k (32768 features)\n",
            "Max Samples: 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Feature Ablation Experiment\n",
        "# This cell performs ablation by zeroing out specified SAE features and comparing predictions\n",
        "\n",
        "# Configuration\n",
        "# ========== ABLATION CONFIGURATION ==========\n",
        "ABLATION_CONFIG = {\n",
        "    #\"mode\": \"per_sample_top_k\",  # Options: \"manual\" | \"per_sample_top_k\" | \"union_top_k\"\n",
        "    \"mode\": \"union_top_k\",\n",
        "    #\"mode\": \"manual\",\n",
        "    #\"mode\": \"per_sample_top_k\",\n",
        "    \"k\": 10,  # Only for per_sample_top_k and union_top_k modes\n",
        "    \"skip_sae_reconstruction\": False,  # If True, skip SAE hook entirely (true baseline)\n",
        "    # Similarity expansion (new ablation mode that can wrap all modes)\n",
        "    \"similarity_expansion\": True,\n",
        "    \"similarity_top_m\": 100  # Total features per seed feature (includes original)\n",
        "}\n",
        "\n",
        "# Mode 1: Manual features (only used if mode == \"manual\")\n",
        "#MANUAL_FEATURES = [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
        "MANUAL_FEATURES = [21110, 24583, 21508, 32601, 15959, 27518, 29555, 3993, 13142, 22354] # row 0 true top 10\n",
        "#MANUAL_FEATURES = []\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "if ABLATION_CONFIG['mode'] != 'manual':\n",
        "    print(f\"K value: {ABLATION_CONFIG['k']}\")\n",
        "print(f\"Similarity Expansion: {ABLATION_CONFIG['similarity_expansion']}\")\n",
        "if ABLATION_CONFIG['similarity_expansion']:\n",
        "    print(f\"Similarity Top-M: {ABLATION_CONFIG['similarity_top_m']}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# Define device and move model to it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu only, please install CUDA-compatible Torch\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the SAE using the helper function\n",
        "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
        "\n",
        "# Extract dimensions from the loaded config\n",
        "SAE_INPUT_DIM = sae_config['input_dim']\n",
        "SAE_LATENT_DIM = sae_config['latent_dim']\n",
        "\n",
        "print(f\"Ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "print(f\"Layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"SAE Size: {SAE_SIZE} ({SAE_LATENT_DIM} features)\")\n",
        "print(f\"Max Samples: {MAX_SAMPLES}\\n\")\n",
        "\n",
        "# Storage for results\n",
        "baseline_predictions = []\n",
        "ablated_predictions = []\n",
        "sample_data = []\n",
        "\n",
        "# Initialize trackers for SAE features (same as inference cell)\n",
        "feature_stats_ablated = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
        "top_token_tracker_ablated = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
        "headline_aggregator_ablated = HeadlineFeatureAggregator(top_k=10)\n",
        "all_prompt_metadata_ablated = []\n",
        "\n",
        "# Storage for capturing SAE features during ablation (for tracking)\n",
        "current_sample_data = {\"sae_features\": None, \"token_ids\": None, \"prompt_tokens\": None, \"text\": None, \"idx\": None}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cac4bc40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”¬ Running baseline inference (no ablation)...\n",
            "  Baseline: 20/100 samples\n",
            "  Baseline: 40/100 samples\n",
            "  Baseline: 60/100 samples\n",
            "  Baseline: 80/100 samples\n",
            "  Baseline: 100/100 samples\n",
            "âœ“ Baseline accuracy: 87.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Baseline Inference\n",
        "print(\"ðŸ”¬ Running baseline inference (no ablation)...\")\n",
        "baseline_results, baseline_features_map, baseline_accuracy = run_baseline_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_ds=test_ds,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_samples=MAX_SAMPLES,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "print(f\"âœ“ Baseline accuracy: {baseline_accuracy:.2%}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "760bf70f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity expansion: enabled (top_m=100)\n",
            "\n",
            "Mode 3 (Union Top-K): Collected 7901 unique features from union of top-10 across 100 samples\n",
            "  Similarity expansion: 91 â†’ 7901 (top_m=100)\n"
          ]
        }
      ],
      "source": [
        "# Feature Selection & Expansion\n",
        "skip_hooks = ABLATION_CONFIG.get(\"skip_sae_reconstruction\", False)\n",
        "\n",
        "similarity_enabled = ABLATION_CONFIG.get(\"similarity_expansion\", False)\n",
        "similarity_top_m = int(ABLATION_CONFIG.get(\"similarity_top_m\", 10))\n",
        "if similarity_top_m < 1:\n",
        "    raise ValueError(\"similarity_top_m must be >= 1\")\n",
        "\n",
        "normalized_decoder = None\n",
        "similarity_cache = {}\n",
        "if similarity_enabled and not skip_hooks:\n",
        "    normalized_decoder = normalize_decoder_weights(sae, device)\n",
        "    print(f\"Similarity expansion: enabled (top_m={similarity_top_m})\")\n",
        "elif similarity_enabled and skip_hooks:\n",
        "    print(\"Similarity expansion requested, but skip_sae_reconstruction=True; expansion disabled.\")\n",
        "    similarity_enabled = False\n",
        "\n",
        "if skip_hooks:\n",
        "    print(\"\\nâ„¹ï¸ Skipping SAE reconstruction (true baseline mode)\")\n",
        "    print(\"   Predictions will match baseline exactly\\n\")\n",
        "    FEATURES_TO_ABLATE = None\n",
        "    ORIGINAL_SEED_FEATURES = None\n",
        "else:\n",
        "    ORIGINAL_SEED_FEATURES = None\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\":\n",
        "        FEATURES_TO_ABLATE = MANUAL_FEATURES\n",
        "        ORIGINAL_SEED_FEATURES = list(MANUAL_FEATURES)\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"manual features\")\n",
        "        original_feature_count = len(FEATURES_TO_ABLATE)\n",
        "        if similarity_enabled:\n",
        "            FEATURES_TO_ABLATE = expand_features_with_similarity(\n",
        "                FEATURES_TO_ABLATE, normalized_decoder, similarity_top_m, similarity_cache\n",
        "            )\n",
        "        print(f\"\\nMode 1 (Manual): Ablating {len(FEATURES_TO_ABLATE)} manually specified features\")\n",
        "        if similarity_enabled:\n",
        "            print(f\"  Similarity expansion: {original_feature_count} â†’ {len(FEATURES_TO_ABLATE)} (top_m={similarity_top_m})\")\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"union_top_k\":\n",
        "        feature_set = set()\n",
        "        for idx in baseline_features_map:\n",
        "            top_k_ids = [f[\"feature_id\"] for f in baseline_features_map[idx][\"top_features\"][:ABLATION_CONFIG[\"k\"]]]\n",
        "            feature_set.update(top_k_ids)\n",
        "        FEATURES_TO_ABLATE = sorted(list(feature_set))\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"union_top_k features\")\n",
        "        original_feature_count = len(FEATURES_TO_ABLATE)\n",
        "        if similarity_enabled:\n",
        "            FEATURES_TO_ABLATE = expand_features_with_similarity(\n",
        "                FEATURES_TO_ABLATE, normalized_decoder, similarity_top_m, similarity_cache\n",
        "            )\n",
        "        print(f\"\\nMode 3 (Union Top-K): Collected {len(FEATURES_TO_ABLATE)} unique features from union of top-{ABLATION_CONFIG['k']} across {len(baseline_results)} samples\")\n",
        "        if similarity_enabled:\n",
        "            print(f\"  Similarity expansion: {original_feature_count} â†’ {len(FEATURES_TO_ABLATE)} (top_m={similarity_top_m})\")\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "        FEATURES_TO_ABLATE = None\n",
        "        print(f\"\\nMode 2 (Per-Sample Top-K): Will ablate top-{ABLATION_CONFIG['k']} features individually for each sample\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] != \"per_sample_top_k\":\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"global ablation features\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\":\n",
        "        print(f\"Features to ablate: {FEATURES_TO_ABLATE if FEATURES_TO_ABLATE is not None else 'Per-sample dynamic'}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a1d3f4f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”¬ Running ablation inference (features zeroed)...\n",
            "  Ablated: 20/100 samples\n",
            "  Ablated: 40/100 samples\n",
            "  Ablated: 60/100 samples\n",
            "  Ablated: 80/100 samples\n",
            "  Ablated: 100/100 samples\n",
            "âœ“ Ablated accuracy: 57.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ablation Inference\n",
        "print(\"ðŸ”¬ Running ablation inference (features zeroed)...\")\n",
        "(\n",
        "    ablated_results,\n",
        "    all_prompt_metadata_ablated,\n",
        "    similarity_stats,\n",
        "    all_ablated_features_set,\n",
        "    ablated_accuracy,\n",
        ") = run_ablation_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_ds=test_ds,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_samples=MAX_SAMPLES,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ablation_config=ABLATION_CONFIG,\n",
        "    features_to_ablate=FEATURES_TO_ABLATE,\n",
        "    baseline_results=baseline_results,\n",
        "    baseline_features_map=baseline_features_map,\n",
        "    feature_stats_ablated=feature_stats_ablated,\n",
        "    top_token_tracker_ablated=top_token_tracker_ablated,\n",
        "    headline_aggregator_ablated=headline_aggregator_ablated,\n",
        "    current_sample_data=current_sample_data,\n",
        "    similarity_enabled=similarity_enabled,\n",
        "    normalized_decoder=normalized_decoder,\n",
        "    similarity_top_m=similarity_top_m,\n",
        "    similarity_cache=similarity_cache,\n",
        ")\n",
        "print(f\"âœ“ Ablated accuracy: {ablated_accuracy:.2%}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "57981356",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ABLATION RESULTS\n",
            "============================================================\n",
            "Ablation Mode: union_top_k\n",
            "Total Features Ablated: 7901\n",
            "  (After similarity expansion)\n",
            "Baseline Accuracy: 87.00%\n",
            "Ablated Accuracy: 57.00%\n",
            "Accuracy Change: -30.00%\n",
            "\n",
            "Flipped Predictions: 36/100 samples\n",
            "Flip Rate: 36.00%\n",
            "\n",
            "============================================================\n",
            "FLIPPED PREDICTIONS (showing first 10):\n",
            "============================================================\n",
            "\n",
            "--- Sample #0 ---\n",
            "Text: Ally Financial pulls outlook\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.982) â†’ Ablated: Neutral (conf: 0.855)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21110: 7.3072 [ABLATED]\n",
            "  Feature 24583: 4.8220 [ABLATED]\n",
            "  Feature 21508: 4.6871 [ABLATED]\n",
            "  Feature 32601: 4.5500 [ABLATED]\n",
            "  Feature 15959: 4.3837 [ABLATED]\n",
            "  Feature 27518: 4.1182 [ABLATED]\n",
            "  Feature 29555: 3.8110 [ABLATED]\n",
            "  Feature 3993: 3.6784 [ABLATED]\n",
            "  Feature 13142: 3.6465 [ABLATED]\n",
            "  Feature 22354: 3.4679 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #2 ---\n",
            "Text: Moody's turns negative on Party City\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.981) â†’ Ablated: Neutral (conf: 0.927)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.3068 [ABLATED]\n",
            "  Feature 21508: 7.2474 [ABLATED]\n",
            "  Feature 5111: 5.7303 [ABLATED]\n",
            "  Feature 21110: 5.6846 [ABLATED]\n",
            "  Feature 27518: 5.1542 [ABLATED]\n",
            "  Feature 25797: 4.9270 [ABLATED]\n",
            "  Feature 24583: 4.6555 [ABLATED]\n",
            "  Feature 16393: 4.5483 [ABLATED]\n",
            "  Feature 13142: 4.4483 [ABLATED]\n",
            "  Feature 4083: 4.1524 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #3 ---\n",
            "Text: Deutsche Bank cuts to Hold\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.996) â†’ Ablated: Neutral (conf: 0.534)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.7064 [ABLATED]\n",
            "  Feature 21110: 6.2142 [ABLATED]\n",
            "  Feature 21508: 5.8726 [ABLATED]\n",
            "  Feature 24583: 4.9774 [ABLATED]\n",
            "  Feature 25797: 4.6049 [ABLATED]\n",
            "  Feature 5111: 4.4135 [ABLATED]\n",
            "  Feature 27518: 4.4000 [ABLATED]\n",
            "  Feature 29073: 4.2756 [ABLATED]\n",
            "  Feature 32411: 4.1279 [ABLATED]\n",
            "  Feature 16393: 3.8279 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #4 ---\n",
            "Text: Compass Point cuts to Sell\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.983) â†’ Ablated: Neutral (conf: 0.652)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.2449 [ABLATED]\n",
            "  Feature 21110: 6.1200 [ABLATED]\n",
            "  Feature 16393: 5.5337 [ABLATED]\n",
            "  Feature 25797: 5.1744 [ABLATED]\n",
            "  Feature 5111: 4.9148 [ABLATED]\n",
            "  Feature 29952: 4.5362 [ABLATED]\n",
            "  Feature 8262: 4.5314 [ABLATED]\n",
            "  Feature 14807: 4.5311 [ABLATED]\n",
            "  Feature 10604: 4.4184 [ABLATED]\n",
            "  Feature 687: 4.4105 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #5 ---\n",
            "Text: Barclays cools on Molson Coors\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.648) â†’ Ablated: Neutral (conf: 0.904)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 8.8932 [ABLATED]\n",
            "  Feature 32601: 6.1672 [ABLATED]\n",
            "  Feature 15991: 4.6884 [ABLATED]\n",
            "  Feature 21508: 4.4652 [ABLATED]\n",
            "  Feature 29952: 4.4536 [ABLATED]\n",
            "  Feature 5111: 4.2631 [ABLATED]\n",
            "  Feature 28660: 4.2182 [ABLATED]\n",
            "  Feature 7927: 4.2108 [ABLATED]\n",
            "  Feature 687: 4.1628 [ABLATED]\n",
            "  Feature 27757: 4.0974 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #6 ---\n",
            "Text: Barclays cuts to Equal Weight\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.972) â†’ Ablated: Neutral (conf: 0.786)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21508: 5.0885 [ABLATED]\n",
            "  Feature 24583: 4.5491 [ABLATED]\n",
            "  Feature 27518: 3.8906 [ABLATED]\n",
            "  Feature 8784: 3.8306 [ABLATED]\n",
            "  Feature 4456: 3.7008 [ABLATED]\n",
            "  Feature 31923: 3.6618 [ABLATED]\n",
            "  Feature 26139: 3.4318 [ABLATED]\n",
            "  Feature 21110: 3.4233 [ABLATED]\n",
            "  Feature 5111: 3.4078 [ABLATED]\n",
            "  Feature 8232: 3.3543 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #10 ---\n",
            "Text: Children's Place downgraded to neutral from outperform at Wedbush, price target slashed to $60 from $130\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.999) â†’ Ablated: Neutral (conf: 0.648)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 10.2199 [ABLATED]\n",
            "  Feature 21508: 8.0324 [ABLATED]\n",
            "  Feature 21110: 5.6904 [ABLATED]\n",
            "  Feature 4247: 5.5460 [ABLATED]\n",
            "  Feature 28908: 5.4686 [ABLATED]\n",
            "  Feature 25518: 5.2739 [ABLATED]\n",
            "  Feature 25797: 5.1319 [ABLATED]\n",
            "  Feature 17433: 5.1150 [ABLATED]\n",
            "  Feature 16205: 5.0357 [ABLATED]\n",
            "  Feature 9718: 4.9125 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #11 ---\n",
            "Text: Clovis Oncology downgraded to in line from outperform at Evercore ISI\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.999) â†’ Ablated: Neutral (conf: 0.702)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21508: 8.0159 [ABLATED]\n",
            "  Feature 24583: 7.2731 [ABLATED]\n",
            "  Feature 4456: 6.9411 [ABLATED]\n",
            "  Feature 18317: 5.8428 [ABLATED]\n",
            "  Feature 21110: 5.7342 [ABLATED]\n",
            "  Feature 28908: 5.7124 [ABLATED]\n",
            "  Feature 13142: 5.6617 [ABLATED]\n",
            "  Feature 25797: 5.6591 [ABLATED]\n",
            "  Feature 15991: 4.7023 [ABLATED]\n",
            "  Feature 21969: 4.4700 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #12 ---\n",
            "Text: Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.883) â†’ Ablated: Neutral (conf: 0.672)\n",
            "Top 10 SAE Features:\n",
            "  Feature 25518: 8.1618 [ABLATED]\n",
            "  Feature 4456: 6.9640 [ABLATED]\n",
            "  Feature 21110: 6.9277 [ABLATED]\n",
            "  Feature 21508: 6.4393 [ABLATED]\n",
            "  Feature 24583: 6.3370 [ABLATED]\n",
            "  Feature 19876: 5.9274 [ABLATED]\n",
            "  Feature 32411: 5.1512 [ABLATED]\n",
            "  Feature 32601: 4.9442 [ABLATED]\n",
            "  Feature 31394: 4.5632 [ABLATED]\n",
            "  Feature 15959: 4.5496 [ABLATED]\n",
            "\n",
            "\n",
            "--- Sample #13 ---\n",
            "Text: Goldman pulls Progressive from Goldman's conviction list; shares -2.7%\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.994) â†’ Ablated: Neutral (conf: 0.850)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 7.5666 [ABLATED]\n",
            "  Feature 25797: 5.8555 [ABLATED]\n",
            "  Feature 32601: 5.7686 [ABLATED]\n",
            "  Feature 21508: 5.6294 [ABLATED]\n",
            "  Feature 4247: 5.6123 [ABLATED]\n",
            "  Feature 21110: 4.9541 [ABLATED]\n",
            "  Feature 16205: 4.5684 [ABLATED]\n",
            "  Feature 15959: 4.4294 [ABLATED]\n",
            "  Feature 28908: 4.3922 [ABLATED]\n",
            "  Feature 8784: 4.3459 [ABLATED]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Results Analysis\n",
        "flipped_samples = find_flipped_predictions(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    baseline_results=baseline_results,\n",
        "    ablated_results=ablated_results,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if skip_hooks is False:\n",
        "    print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\" and ORIGINAL_SEED_FEATURES is not None:\n",
        "        print(f\"Seed Features: {ORIGINAL_SEED_FEATURES}\")\n",
        "        print(f\"Total Features Ablated: {len(FEATURES_TO_ABLATE)}\")\n",
        "        if similarity_enabled and len(FEATURES_TO_ABLATE) != len(ORIGINAL_SEED_FEATURES):\n",
        "            print(f\"  (Expanded from {len(ORIGINAL_SEED_FEATURES)} seeds)\")\n",
        "\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"union_top_k\" and FEATURES_TO_ABLATE is not None:\n",
        "        print(f\"Total Features Ablated: {len(FEATURES_TO_ABLATE)}\")\n",
        "        if similarity_enabled:\n",
        "            print(\"  (After similarity expansion)\")\n",
        "\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "        print(f\"Per-Sample: top-{ABLATION_CONFIG['k']} features\")\n",
        "        if len(all_ablated_features_set) > 0:\n",
        "            print(f\"Unique Features Ablated: {len(all_ablated_features_set)} (across {len(ablated_results)} samples)\")\n",
        "            if similarity_stats[\"expanded_counts\"]:\n",
        "                avg_per_sample = float(np.mean(similarity_stats[\"expanded_counts\"]))\n",
        "                print(f\"Per-Sample Average: {avg_per_sample:.1f} features\")\n",
        "            else:\n",
        "                print(f\"Per-Sample Average: {ABLATION_CONFIG['k']} features\")\n",
        "\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.2%}\")\n",
        "print(f\"Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"Accuracy Change: {(ablated_accuracy - baseline_accuracy):.2%}\")\n",
        "print(f\"\\nFlipped Predictions: {len(flipped_samples)}/{len(baseline_results)} samples\")\n",
        "print(f\"Flip Rate: {len(flipped_samples)/len(baseline_results):.2%}\\n\")\n",
        "\n",
        "if flipped_samples:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"FLIPPED PREDICTIONS (showing first {min(10, len(flipped_samples))}):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, flip in enumerate(flipped_samples[:10], 1):\n",
        "        print(f\"\\n--- Sample #{flip['sample_idx']} ---\")\n",
        "        print(f\"Text: {flip['text'][:120]}{'...' if len(flip['text']) > 120 else ''}\")\n",
        "        print(f\"True Label: {flip['true_label']}\")\n",
        "        print(f\"Original: {flip['baseline_pred']} (conf: {flip['baseline_conf']:.3f}) â†’ \"\n",
        "              f\"Ablated: {flip['ablated_pred']} (conf: {flip['ablated_conf']:.3f})\")\n",
        "\n",
        "        if flip['top_features']:\n",
        "            print(\"Top 10 SAE Features:\")\n",
        "            for feat in flip['top_features']:\n",
        "                ablated_marker = \" [ABLATED]\" if feat['ablated'] else \"\"\n",
        "                print(f\"  Feature {feat['feature_id']}: {feat['activation']:.4f}{ablated_marker}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No predictions were flipped by ablating these features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e4df4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results\n",
        "print(\"\\nðŸ’¾ Saving ablated results for visualization...\")\n",
        "\n",
        "ablated_run_dir = make_analysis_run_dir(str(repo_root))\n",
        "print(f\"ðŸ’¾ Saving ablated results to: {ablated_run_dir}\")\n",
        "\n",
        "stats_ablated = feature_stats_ablated.get_stats()\n",
        "\n",
        "top_features_by_metric_ablated = {}\n",
        "for metric_name, values in stats_ablated.items():\n",
        "    if metric_name == \"mean_act_squared\":\n",
        "        continue\n",
        "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
        "    top_features_by_metric_ablated[metric_name] = [\n",
        "        {\n",
        "            \"feature_id\": int(idx),\n",
        "            \"value\": float(values[idx]),\n",
        "            \"metrics\": {\n",
        "                \"mean_activation\": float(stats_ablated[\"mean_activation\"][idx]),\n",
        "                \"max_activation\": float(stats_ablated[\"max_activation\"][idx]),\n",
        "                \"fraction_active\": float(stats_ablated[\"fraction_active\"][idx]),\n",
        "            },\n",
        "        }\n",
        "        for idx in top_indices\n",
        "    ]\n",
        "\n",
        "prompts_file = ablated_run_dir / \"prompts.jsonl\"\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in all_prompt_metadata_ablated:\n",
        "        json.dump(meta, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "feature_stats_file = ablated_run_dir / \"feature_stats.json\"\n",
        "feature_stats_data = {\n",
        "    \"num_features\": SAE_LATENT_DIM,\n",
        "    \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "    \"top_feature_count\": TOP_FEATURES,\n",
        "    \"accuracy\": ablated_accuracy,\n",
        "    \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "    \"mean_act_squared\": stats_ablated[\"mean_act_squared\"].tolist(),\n",
        "    \"metrics\": {\n",
        "        metric_name: {\n",
        "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
        "            \"top_features\": top_features_by_metric_ablated[metric_name],\n",
        "        }\n",
        "        for metric_name in stats_ablated.keys() if metric_name != \"mean_act_squared\"\n",
        "    },\n",
        "}\n",
        "with open(feature_stats_file, \"w\") as f:\n",
        "    json.dump(feature_stats_data, f, indent=2)\n",
        "\n",
        "feature_tokens_file = ablated_run_dir / \"feature_tokens.json\"\n",
        "feature_tokens_data = {\"features\": top_token_tracker_ablated.export()}\n",
        "with open(feature_tokens_file, \"w\") as f:\n",
        "    json.dump(feature_tokens_data, f, indent=2)\n",
        "\n",
        "headline_features_file = ablated_run_dir / \"headline_features.json\"\n",
        "with open(headline_features_file, \"w\") as f:\n",
        "    json.dump(headline_aggregator_ablated.export(), f, indent=2)\n",
        "\n",
        "metadata_file = ablated_run_dir / \"metadata.json\"\n",
        "with open(metadata_file, \"w\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"model\": save_dir,\n",
        "            \"layer_extracted\": LAYER_TO_EXTRACT,\n",
        "            \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "            \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "            \"accuracy\": ablated_accuracy,\n",
        "            \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
        "            \"split\": \"validation\",\n",
        "            \"hidden_dim\": SAE_INPUT_DIM,\n",
        "            \"latent_dim\": SAE_LATENT_DIM,\n",
        "            \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
        "            \"top_features_per_metric\": TOP_FEATURES,\n",
        "            \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
        "            \"ablation_mode\": ABLATION_CONFIG[\"mode\"],\n",
        "            \"ablated_features\": FEATURES_TO_ABLATE if FEATURES_TO_ABLATE is not None else \"per_sample_dynamic\",\n",
        "            \"ablation_k\": ABLATION_CONFIG.get(\"k\"),\n",
        "            \"skip_sae_reconstruction\": ABLATION_CONFIG.get(\"skip_sae_reconstruction\", False),\n",
        "            \"similarity_expansion\": {\"enabled\": similarity_enabled, \"top_m\": similarity_top_m},\n",
        "            \"note\": f\"SAE sparse features with predictions (mode: {ABLATION_CONFIG['mode']})\",\n",
        "        },\n",
        "        f,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(\"\\nâœ… Ablation experiment complete!\")\n",
        "print(f\"   ðŸ“ Ablated results saved to: {ablated_run_dir.name}\")\n",
        "print(f\"   ðŸŽ¯ Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"   ðŸ”¢ Total tokens: {feature_stats_ablated.total_tokens}\")\n",
        "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
        "print(\"\\nðŸŒ Start the viewer to see ablated results:\")\n",
        "print(\"   python viz_analysis/feature_probe_server.py\")\n",
        "print(\"   cd sae-viewer && npm start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eefe6eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference (non refactored)\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import heapq\n",
        "from typing import List, Tuple\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# Start of Inference\n",
        "# Add project root to path to import utilities\n",
        "repo_root = Path(\".\").resolve()\n",
        "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
        "\n",
        "from utils.run_dirs import make_analysis_run_dir\n",
        "\n",
        "# Configuration\n",
        "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
        "MAX_SAMPLES = 100  # Limit for testing\n",
        "TOP_FEATURES = 100  # Top features to track per metric\n",
        "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
        "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
        "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the SAE using the helper function\n",
        "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
        "\n",
        "# Extract dimensions from the loaded config\n",
        "SAE_INPUT_DIM = sae_config['input_dim']\n",
        "SAE_LATENT_DIM = sae_config['latent_dim']\n",
        "\n",
        "print(f\"âœ“ SAE loaded: {SAE_INPUT_DIM} dims â†’ {SAE_LATENT_DIM} sparse features\")\n",
        "\n",
        "# Create run directory using the same utility as main.py\n",
        "# This ensures the server can find it automatically in analysis_data/\n",
        "run_dir = make_analysis_run_dir(str(repo_root))\n",
        "print(f\"\\nðŸ’¾ Saving results to: {run_dir}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "sae.to(device)\n",
        "model.eval()\n",
        "sae.eval()\n",
        "\n",
        "# Load dataset\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
        "\n",
        "# Feature statistics tracker (per-token aggregation)\n",
        "class FeatureStatsAggregator:\n",
        "    def __init__(self, feature_dim: int):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.total_tokens = 0\n",
        "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
        "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
        "    \n",
        "    def update(self, token_activations: np.ndarray):\n",
        "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
        "        self.total_tokens += token_activations.shape[0]\n",
        "        self.sum_activations += token_activations.sum(axis=0)\n",
        "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
        "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
        "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
        "    \n",
        "    def get_stats(self):\n",
        "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
        "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
        "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
        "        return {\n",
        "            \"mean_activation\": mean_act,\n",
        "            \"max_activation\": self.max_activations,\n",
        "            \"fraction_active\": frac_active,\n",
        "            \"mean_act_squared\": mean_act_squared\n",
        "        }\n",
        "\n",
        "# Top token tracker per feature\n",
        "class FeatureTopTokenTracker:\n",
        "    def __init__(self, feature_dim: int, top_k: int):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.top_k = top_k\n",
        "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
        "        self.heaps = [[] for _ in range(feature_dim)]\n",
        "    \n",
        "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
        "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
        "               predicted_label: str = None, true_label: str = None):\n",
        "        \"\"\"Update with tokens from one prompt\"\"\"\n",
        "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
        "            # For each token, find top features\n",
        "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
        "            \n",
        "            for feat_id in top_features:\n",
        "                activation = float(act_vec[feat_id])\n",
        "                if activation <= 0:\n",
        "                    continue\n",
        "                \n",
        "                heap = self.heaps[feat_id]\n",
        "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
        "                \n",
        "                metadata = {\n",
        "                    \"activation\": activation,\n",
        "                    \"token_str\": token_str,\n",
        "                    \"token_id\": int(token_id),\n",
        "                    \"token_position\": int(token_pos),\n",
        "                    \"prompt_index\": int(prompt_idx),\n",
        "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
        "                    \"prompt_snippet\": prompt_text[:160],\n",
        "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
        "                    \"prompt_tokens\": prompt_tokens,\n",
        "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
        "                    \"true_label\": true_label,\n",
        "                }\n",
        "                \n",
        "                if len(heap) < self.top_k:\n",
        "                    heapq.heappush(heap, (activation, metadata))\n",
        "                elif activation > heap[0][0]:\n",
        "                    heapq.heapreplace(heap, (activation, metadata))\n",
        "    \n",
        "    def export(self):\n",
        "        \"\"\"Export top tokens for each feature\"\"\"\n",
        "        result = {}\n",
        "        for feat_id in range(self.feature_dim):\n",
        "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
        "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
        "        return result\n",
        "\n",
        "# Aggregate top features per headline (sample-level view)\n",
        "class HeadlineFeatureAggregator:\n",
        "    def __init__(self, top_k: int = 10):\n",
        "        self.top_k = top_k\n",
        "        self.headlines = []  # List of headline metadata with top features\n",
        "    \n",
        "    def add_headline(self, prompt_idx: int, prompt_text: str,\n",
        "                     token_activations: np.ndarray,\n",
        "                     token_ids: List[int],\n",
        "                     token_strings: List[str],\n",
        "                     predicted_label: str, true_label: str):\n",
        "        \"\"\"Aggregate features across all tokens in a headline\"\"\"\n",
        "        if token_activations.size == 0:\n",
        "            return\n",
        "        # Max activation per feature and which token triggered it\n",
        "        max_token_idx_per_feature = token_activations.argmax(axis=0)  # [feature_dim]\n",
        "        max_activation_per_feature = token_activations.max(axis=0)     # [feature_dim]\n",
        "        \n",
        "        # Get top K features by their max activation in this headline\n",
        "        top_feature_ids = np.argsort(max_activation_per_feature)[-self.top_k:][::-1]\n",
        "        \n",
        "        features = [\n",
        "            {\n",
        "                \"feature_id\": int(fid),\n",
        "                \"max_activation\": float(max_activation_per_feature[fid]),\n",
        "                \"token_position\": int(max_token_idx_per_feature[fid]),\n",
        "                \"token_id\": int(token_ids[max_token_idx_per_feature[fid]]),\n",
        "                \"token_str\": token_strings[max_token_idx_per_feature[fid]],\n",
        "            }\n",
        "            for fid in top_feature_ids if max_activation_per_feature[fid] > 0\n",
        "        ]\n",
        "        \n",
        "        self.headlines.append({\n",
        "            \"row_id\": int(prompt_idx),\n",
        "            \"prompt\": prompt_text,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"true_label\": true_label,\n",
        "            \"correct\": predicted_label == true_label,\n",
        "            \"num_tokens\": int(token_activations.shape[0]),\n",
        "            \"features\": features\n",
        "        })\n",
        "    \n",
        "    def export(self):\n",
        "        return self.headlines\n",
        "\n",
        "# Initialize trackers for SAE features\n",
        "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
        "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
        "headline_aggregator = HeadlineFeatureAggregator(top_k=10)\n",
        "\n",
        "# Storage for per-sample metadata\n",
        "all_prompt_metadata = []\n",
        "all_prediction_metadata = []\n",
        "\n",
        "# Hook to capture activations\n",
        "captured_activations = []\n",
        "\n",
        "def capture_hook(module, input, output):\n",
        "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
        "    if isinstance(output, tuple):\n",
        "        hidden_states = output[0]\n",
        "    else:\n",
        "        hidden_states = output\n",
        "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
        "\n",
        "# Register hook on target layer\n",
        "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
        "\n",
        "print(f\"\\nðŸ”¬ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
        "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
        "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
        "\n",
        "# Process samples\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(test_ds):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize with truncation\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
        "        \n",
        "        # Get string tokens for display (properly cleaned)\n",
        "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
        "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        prompt_tokens = []\n",
        "        for tok in raw_tokens:\n",
        "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
        "            if tok.startswith(\"##\"):\n",
        "                prompt_tokens.append(tok[2:])  # Remove ##\n",
        "            else:\n",
        "                prompt_tokens.append(tok)\n",
        "        \n",
        "        # Forward pass\n",
        "        inputs = inputs.to(device)\n",
        "        captured_activations.clear()\n",
        "        outputs = model(**inputs)\n",
        "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
        "        pred_label = model.config.id2label[pred_id]\n",
        "        \n",
        "        # Get captured activation and pass through SAE\n",
        "        if captured_activations:\n",
        "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
        "            bert_activation = captured_activations[0].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (same as training) - do on GPU\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            \n",
        "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
        "            \n",
        "            # Filter activations on GPU\n",
        "            bert_activation = bert_activation[valid_mask]\n",
        "            \n",
        "            # Skip if no valid tokens\n",
        "            if bert_activation.shape[0] == 0:\n",
        "                continue\n",
        "            \n",
        "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
        "            sae_features = sae.encode(bert_activation)\n",
        "            \n",
        "            # Only now move to CPU for numpy conversion and token filtering\n",
        "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
        "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
        "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
        "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
        "            \n",
        "            seq_len = sae_features_cpu.shape[0]\n",
        "            \n",
        "            # Update feature statistics with SAE features\n",
        "            feature_stats.update(sae_features_cpu)\n",
        "            \n",
        "            # Track top tokens per feature\n",
        "            top_token_tracker.update(\n",
        "                sae_features_cpu, \n",
        "                filtered_token_ids, \n",
        "                prompt_idx=idx,\n",
        "                prompt_text=text,\n",
        "                prompt_tokens=filtered_prompt_tokens,\n",
        "                predicted_label=pred_label,  # Pass prediction info\n",
        "                true_label=model.config.id2label[true_label]\n",
        "            )\n",
        "            \n",
        "            # Aggregate top features at headline level\n",
        "            headline_aggregator.add_headline(\n",
        "                prompt_idx=idx,\n",
        "                prompt_text=text,\n",
        "                token_activations=sae_features_cpu,\n",
        "                token_ids=filtered_token_ids,\n",
        "                token_strings=filtered_prompt_tokens,\n",
        "                predicted_label=pred_label,\n",
        "                true_label=model.config.id2label[true_label],\n",
        "                confidence=confidence\n",
        "            )\n",
        "            \n",
        "            # Save prompt metadata\n",
        "            all_prompt_metadata.append({\n",
        "                \"row_id\": idx,\n",
        "                \"seq_len\": seq_len,\n",
        "                \"prompt\": text,\n",
        "                \"predicted_label\": pred_label,\n",
        "                \"true_label\": model.config.id2label[true_label],\n",
        "                \"correct\": pred_id == true_label\n",
        "            })\n",
        "        \n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
        "\n",
        "# Remove hook\n",
        "hook_handle.remove()\n",
        "\n",
        "# Compute final statistics\n",
        "print(\"\\nðŸ“Š Computing feature statistics...\")\n",
        "stats = feature_stats.get_stats()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
        "print(f\"ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Get top features for each metric\n",
        "top_features_by_metric = {}\n",
        "for metric_name, values in stats.items():\n",
        "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
        "    top_features_by_metric[metric_name] = [\n",
        "        {\n",
        "            \"feature_id\": int(idx),\n",
        "            \"value\": float(values[idx]),\n",
        "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
        "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
        "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
        "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
        "            }\n",
        "        }\n",
        "        for idx in top_indices\n",
        "    ]\n",
        "\n",
        "# Save results\n",
        "print(\"\\nðŸ’¾ Saving results...\")\n",
        "\n",
        "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
        "prompts_file = run_dir / \"prompts.jsonl\"\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in all_prompt_metadata:\n",
        "        json.dump(meta, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
        "feature_stats_file = run_dir / \"feature_stats.json\"\n",
        "feature_stats_data = {\n",
        "    \"num_features\": SAE_LATENT_DIM,\n",
        "    \"total_tokens\": feature_stats.total_tokens,\n",
        "    \"top_feature_count\": TOP_FEATURES,\n",
        "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
        "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
        "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
        "    \"metrics\": {\n",
        "        metric_name: {\n",
        "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
        "            \"top_features\": top_features_by_metric[metric_name]\n",
        "        }\n",
        "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
        "    }\n",
        "}\n",
        "with open(feature_stats_file, \"w\") as f:\n",
        "    json.dump(feature_stats_data, f, indent=2)\n",
        "\n",
        "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
        "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
        "feature_tokens_data = {\n",
        "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
        "}\n",
        "with open(feature_tokens_file, \"w\") as f:\n",
        "    json.dump(feature_tokens_data, f, indent=2)\n",
        "\n",
        "# 4. Save headline-level features\n",
        "headline_features_file = run_dir / \"headline_features.json\"\n",
        "with open(headline_features_file, \"w\") as f:\n",
        "    json.dump(headline_aggregator.export(), f, indent=2)\n",
        "\n",
        "# 5. Save metadata\n",
        "metadata_file = run_dir / \"metadata.json\"\n",
        "with open(metadata_file, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"model\": save_dir,\n",
        "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
        "        \"num_samples\": len(all_prompt_metadata),\n",
        "        \"total_tokens\": feature_stats.total_tokens,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
        "        \"split\": \"validation\",\n",
        "        \"hidden_dim\": SAE_INPUT_DIM,\n",
        "        \"latent_dim\": SAE_LATENT_DIM,\n",
        "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
        "        \"top_features_per_metric\": TOP_FEATURES,\n",
        "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
        "        \"note\": \"SAE sparse features with predictions\"\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… COMPLETE!\")\n",
        "print(f\"   ðŸ“ Results saved to: {run_dir.name}\")\n",
        "print(f\"   ðŸŽ¯ Accuracy: {accuracy:.2%}\")\n",
        "print(f\"   ðŸ”¢ Total tokens: {feature_stats.total_tokens}\")\n",
        "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
        "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
        "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
        "    metrics = feat['metrics']\n",
        "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
        "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
        "          f\"max={metrics['max_activation']:.4f}, \"\n",
        "          f\"frac={metrics['fraction_active']:.2%}\")\n",
        "\n",
        "print(f\"\\nðŸŒ Start the viewer to see results:\")\n",
        "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
        "print(f\"   cd sae-viewer && npm start\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def14c72",
      "metadata": {},
      "source": [
        "Testing Inference based on Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de85884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick analysis on simple headlines\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "\n",
        "example_sentences = [\n",
        "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
        "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
        "    \"The company reported results largely in line with analyst expectations.\",\n",
        "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
        "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
        "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
        "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
        "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
        "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
        "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
        "]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# optional: move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def predict_sentiment(text: str):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs)\n",
        "    pred_id = out.logits.argmax(dim=-1).item()\n",
        "    return model.config.id2label[pred_id]\n",
        "\n",
        "for text in example_sentences:\n",
        "    label = predict_sentiment(text)\n",
        "    print(f\"{label.upper():8} | {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f0b474",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Visualisation for Dataset\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
        "\n",
        "test_ds[\"text\"][0:200]\n",
        "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "#ds2[\"validation\"][\"text\"][34]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ce67a413",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL INFERENCE WITHOUT SAEs\n",
            "============================================================\n",
            "\n",
            "ðŸ”¬ Running inference on 100 test samples...\n",
            "   Device: cuda\n",
            "   Model: ./finbert_twitter_ft/best\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing:   4%|â–         | 100/2388 [00:01<00:27, 83.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "âœ… INFERENCE COMPLETE (WITHOUT SAEs)\n",
            "============================================================\n",
            "   ðŸ“Š Total Samples: 100\n",
            "   âœ“ Correct Predictions: 87\n",
            "   âœ— Incorrect Predictions: 13\n",
            "   ðŸŽ¯ Model Accuracy: 87.00%\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Inference WITHOUT SAEs - Plain Model Accuracy on Test Data\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL INFERENCE WITHOUT SAEs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Use validation set for evaluation\n",
        "test_ds = ds[\"validation\"]\n",
        "# MAX_SAMPLES = len(test_ds)  # Process all samples, or set a limit if needed\n",
        "MAX_SAMPLES = 100\n",
        "MAX_SEQ_LENGTH = 64\n",
        "\n",
        "print(f\"\\nðŸ”¬ Running inference on {MAX_SAMPLES} test samples...\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Model: {save_dir}\\n\")\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Process samples\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(test_ds, desc=\"Processing\")):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize with truncation\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
        "        \n",
        "        # Check if prediction is correct\n",
        "        if pred_id == true_label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"âœ… INFERENCE COMPLETE (WITHOUT SAEs)\")\n",
        "print(f\"{'=' * 60}\")\n",
        "print(f\"   ðŸ“Š Total Samples: {total_predictions}\")\n",
        "print(f\"   âœ“ Correct Predictions: {correct_predictions}\")\n",
        "print(f\"   âœ— Incorrect Predictions: {total_predictions - correct_predictions}\")\n",
        "print(f\"   ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
        "print(f\"{'=' * 60}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
