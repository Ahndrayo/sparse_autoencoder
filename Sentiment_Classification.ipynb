{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae2746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import heapq\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from typing import List\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# Add project root to Python path\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import helper utilities from organized modules\n",
    "from utils.finbert import compute_metrics\n",
    "from utils.analysis import (\n",
    "    FeatureStatsAggregator,\n",
    "    FeatureTopTokenTracker,\n",
    "    HeadlineFeatureAggregator\n",
    ")\n",
    "from utils.ablation import create_intervention_hook\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "from sparse_autoencoder.finbert_sae import SparseAutoencoder, load_sae\n",
    "\n",
    "# --------- CUDA sanity check ----------\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Define device for SAE loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset, comes with train and validation fold \n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_leading_tickers(text):\n",
    "    return re.sub(\n",
    "        r'^\\s*(?:\\$[A-Z]{1,6}\\s*)+(?:[-:]\\s*)?',\n",
    "        '',\n",
    "        text\n",
    "    )\n",
    "\n",
    "# Clean dataset\n",
    "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "ds = ds.map(lambda x: {\"text\": remove_leading_tickers(x[\"text\"])})\n",
    "\n",
    "# Load dataset\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8034bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants/Hyperparameters for training model and SAE\n",
    "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
    "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
    "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "\n",
    "# Configuration for Inference\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233625a",
   "metadata": {},
   "source": [
    "Fine Tune Hyperparameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40e9d6",
   "metadata": {},
   "source": [
    "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~4k to 32k interpretable sparse features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c951276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from 9543 training samples...\n",
      "Target layer: 8\n",
      "Will train SAEs with latent dimensions: [4096, 8192, 16384, 32768]\n",
      "Extracting activations from training set...\n",
      "Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9543/9543 [01:56<00:00, 81.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCollected 168708 token activations\n",
      "Activation shape: torch.Size([168708, 768])\n",
      "\\n================================================================================\n",
      "Training SAE with 4096 latent features (4k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0391, Recon=0.0388, L1=0.2544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m reconstruction, latent \u001b[38;5;241m=\u001b[39m \u001b[43msae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Reconstruction loss (MSE)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(reconstruction, batch_x)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36mSparseAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 31\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(latent)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstruction, latent\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mSparseAutoencoder.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode to sparse latent representation\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 23\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ReLU for sparsity\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latent\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell finetunes SAEs based on BERT.\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
    "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
    "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Create SAE save directory\n",
    "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "# train_ds = ds[\"train\"]\n",
    "\n",
    "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
    "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
    "\n",
    "# Collect training activations\n",
    "all_activations = []\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "# Collect activations from all training data\n",
    "print(\"Extracting activations from training set...\")\n",
    "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(train_ds)):\n",
    "        text = sample[\"text\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        captured_activations.clear()\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "        if captured_activations:\n",
    "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
    "            activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Get attention mask and token IDs (keep on GPU)\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "\n",
    "            # Print the number of valid tokens\n",
    "            # kept = valid_mask.sum().item()\n",
    "            # total = attention_mask.sum().item()\n",
    "            # print(f\"Kept {kept}/{total} tokens\")\n",
    "\n",
    "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
    "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
    "\n",
    "            # print(\"TOKENS:\", tokens)\n",
    "            # print(\"DROPPED:\", dropped_tokens)\n",
    "            # print(\"KEPT:\", kept_tokens)\n",
    "            \n",
    "            # Only keep activations for real content tokens (still on GPU)\n",
    "            activation = activation[valid_mask]\n",
    "            \n",
    "            # Only add if there are real tokens\n",
    "            if activation.shape[0] > 0:\n",
    "                # Move to CPU only when storing for later processing\n",
    "                all_activations.append(activation.cpu())\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "# Flatten all activations into a single tensor [total_tokens, 768]\n",
    "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
    "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
    "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
    "\n",
    "# Train SAEs for each latent dimension\n",
    "for LATENT_DIM in LATENT_DIMS:\n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create SAE\n",
    "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
    "    sae.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    dataset = TensorDataset(all_activations_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
    "    sae.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_l1_loss = 0\n",
    "        \n",
    "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstruction, latent = sae(batch_x)\n",
    "            \n",
    "            # Reconstruction loss (MSE)\n",
    "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
    "            \n",
    "            # L1 sparsity loss\n",
    "            l1_loss = latent.abs().mean()\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Renormalize decoder weights (standard SAE practice)\n",
    "            with torch.no_grad():\n",
    "                sae.decoder.weight.data = nn.functional.normalize(\n",
    "                    sae.decoder.weight.data, dim=0\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_l1_loss += l1_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_recon = total_recon_loss / len(dataloader)\n",
    "        avg_l1 = total_l1_loss / len(dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
    "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
    "    \n",
    "    # Save the trained SAE\n",
    "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
    "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
    "    torch.save({\n",
    "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
    "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
    "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
    "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
    "        'config': {\n",
    "            'input_dim': 768,\n",
    "            'latent_dim': LATENT_DIM,\n",
    "            'layer': LAYER_TO_EXTRACT,\n",
    "            'model': save_dir,\n",
    "        }\n",
    "    }, SAE_SAVE_PATH)\n",
    "    \n",
    "    # Test sparsity\n",
    "    sae.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_acts = all_activations_tensor[:1000].to(device)\n",
    "        sample_latent = sae.encode(sample_acts)\n",
    "        sparsity = (sample_latent > 0).float().mean()\n",
    "        print(f\"\\\\nâœ“ SAE trained successfully!\")\n",
    "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
    "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"All SAEs trained successfully!\")\n",
    "print(f\"Available SAE models:\")\n",
    "for dim in LATENT_DIMS:\n",
    "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
    "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70e94b",
   "metadata": {},
   "source": [
    "Finetune FinBERT Model\n",
    "\n",
    "The FinBERT model is trained on the training fold of our dataset to improve its prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0697b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_2716\\3193702175.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1049' max='1791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1049/1791 02:51 < 02:01, 6.11 it/s, Epoch 1.76/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.470456</td>\n",
       "      <td>0.822446</td>\n",
       "      <td>0.764702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 70\u001b[0m\n\u001b[0;32m     43\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     44\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finbert_twitter_ft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;66;03m# <-- use this name; some versions don't accept eval_strategy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,              \u001b[38;5;66;03m# avoids needing wandb, etc.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     61\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     62\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     68\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     73\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finbert_twitter_ft/best\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2616\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2617\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2618\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2619\u001b[0m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\transformers\\trainer.py:5654\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5654\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5656\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:577\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m    579\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:154\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    152\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:839\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 839\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v\u001b[38;5;241m.\u001b[39mto) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    841\u001b[0m     }\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    843\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell finetunes the FINBERT model.\n",
    "\n",
    "# 2) Load model/tokenizer\n",
    "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# 3) Tokenize\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4) Metrics\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# 5) Training config\n",
    "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finbert_twitter_ft\",\n",
    "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
    "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
    "    report_to=\"none\",              # avoids needing wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
    "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30b519",
   "metadata": {},
   "source": [
    "Inference with Interpretability\n",
    "\n",
    "We use our FinBERT + SAE on test data. We extract a Layer Activations with Sentiment Predictions (SAE-style Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64315a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ABLATION EXPERIMENT\n",
      "============================================================\n",
      "âœ“ Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
      "  Layer: 8\n",
      "  Input dim: 768\n",
      "  Latent dim: 32768\n",
      "Ablating features: [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
      "Layer: 8\n",
      "SAE Size: 32k (32768 features)\n",
      "Max Samples: 100\n",
      "\n",
      "ðŸ”¬ Running baseline inference (no ablation)...\n",
      "  Baseline: 20/100 samples\n",
      "  Baseline: 40/100 samples\n",
      "  Baseline: 60/100 samples\n",
      "  Baseline: 80/100 samples\n",
      "  Baseline: 100/100 samples\n",
      "âœ“ Baseline accuracy: 87.00%\n",
      "\n",
      "ðŸ”¬ Running ablation inference (features zeroed)...\n",
      "  Ablated: 20/100 samples\n",
      "  Ablated: 40/100 samples\n",
      "  Ablated: 60/100 samples\n",
      "  Ablated: 80/100 samples\n",
      "  Ablated: 100/100 samples\n",
      "âœ“ Ablated accuracy: 85.00%\n",
      "\n",
      "============================================================\n",
      "FEATURE ABLATION RESULTS\n",
      "============================================================\n",
      "Ablated Features: [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
      "Baseline Accuracy: 87.00%\n",
      "Ablated Accuracy: 85.00%\n",
      "Accuracy Change: -2.00%\n",
      "\n",
      "Flipped Predictions: 6/100 samples\n",
      "Flip Rate: 6.00%\n",
      "\n",
      "============================================================\n",
      "FLIPPED PREDICTIONS (showing first 6):\n",
      "============================================================\n",
      "\n",
      "--- Sample #2 ---\n",
      "Text: Moody's turns negative on Party City\n",
      "True Label: Bearish\n",
      "Original: Bearish (conf: 0.981) â†’ Ablated: Neutral (conf: 0.641)\n",
      "Top 10 SAE Features:\n",
      "  Feature 4456: 10.3068 [ABLATED]\n",
      "  Feature 21508: 7.2474 [ABLATED]\n",
      "  Feature 5111: 5.7303\n",
      "  Feature 21110: 5.6846 [ABLATED]\n",
      "  Feature 27518: 5.1542 [ABLATED]\n",
      "  Feature 25797: 4.9270\n",
      "  Feature 24583: 4.6555 [ABLATED]\n",
      "  Feature 16393: 4.5483\n",
      "  Feature 13142: 4.4483 [ABLATED]\n",
      "  Feature 4083: 4.1524\n",
      "\n",
      "\n",
      "--- Sample #5 ---\n",
      "Text: Barclays cools on Molson Coors\n",
      "True Label: Bearish\n",
      "Original: Bearish (conf: 0.648) â†’ Ablated: Neutral (conf: 0.666)\n",
      "Top 10 SAE Features:\n",
      "  Feature 4456: 8.8932 [ABLATED]\n",
      "  Feature 32601: 6.1672 [ABLATED]\n",
      "  Feature 15991: 4.6884\n",
      "  Feature 21508: 4.4652 [ABLATED]\n",
      "  Feature 29952: 4.4536\n",
      "  Feature 5111: 4.2631\n",
      "  Feature 28660: 4.2182\n",
      "  Feature 7927: 4.2108\n",
      "  Feature 687: 4.1628\n",
      "  Feature 27757: 4.0974\n",
      "\n",
      "\n",
      "--- Sample #12 ---\n",
      "Text: Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦\n",
      "True Label: Bearish\n",
      "Original: Bearish (conf: 0.883) â†’ Ablated: Neutral (conf: 0.386)\n",
      "Top 10 SAE Features:\n",
      "  Feature 25518: 8.1618\n",
      "  Feature 4456: 6.9640 [ABLATED]\n",
      "  Feature 21110: 6.9277 [ABLATED]\n",
      "  Feature 21508: 6.4393 [ABLATED]\n",
      "  Feature 24583: 6.3370 [ABLATED]\n",
      "  Feature 19876: 5.9274\n",
      "  Feature 32411: 5.1512\n",
      "  Feature 32601: 4.9442 [ABLATED]\n",
      "  Feature 31394: 4.5632\n",
      "  Feature 15959: 4.5496 [ABLATED]\n",
      "\n",
      "\n",
      "--- Sample #18 ---\n",
      "Text: MPLX cut at Credit Suisse on potential dilution from Marathon strategic review\n",
      "True Label: Bearish\n",
      "Original: Bearish (conf: 0.995) â†’ Ablated: Bullish (conf: 0.479)\n",
      "Top 10 SAE Features:\n",
      "  Feature 4456: 8.9896 [ABLATED]\n",
      "  Feature 13142: 5.3762 [ABLATED]\n",
      "  Feature 21508: 5.2501 [ABLATED]\n",
      "  Feature 25797: 5.2128\n",
      "  Feature 550: 5.1966\n",
      "  Feature 5111: 4.9626\n",
      "  Feature 21969: 4.9523 [ABLATED]\n",
      "  Feature 21110: 4.9096 [ABLATED]\n",
      "  Feature 9395: 4.4925\n",
      "  Feature 15540: 4.4527\n",
      "\n",
      "\n",
      "--- Sample #77 ---\n",
      "Text: Snap Analyst Projects 37% Revenue Growth In 2020\n",
      "True Label: Bullish\n",
      "Original: Bearish (conf: 0.677) â†’ Ablated: Bullish (conf: 0.652)\n",
      "Top 10 SAE Features:\n",
      "  Feature 21110: 7.8783 [ABLATED]\n",
      "  Feature 4456: 7.1909 [ABLATED]\n",
      "  Feature 25797: 5.5498\n",
      "  Feature 18317: 5.2954\n",
      "  Feature 25518: 5.1933\n",
      "  Feature 32601: 5.1514 [ABLATED]\n",
      "  Feature 13142: 5.0637 [ABLATED]\n",
      "  Feature 24583: 5.0035 [ABLATED]\n",
      "  Feature 6026: 4.9927\n",
      "  Feature 26260: 4.9678\n",
      "\n",
      "\n",
      "--- Sample #94 ---\n",
      "Text: Analysts Expect Breakeven For China Online Education Group (NYSE:COE)\n",
      "True Label: Neutral\n",
      "Original: Bullish (conf: 0.509) â†’ Ablated: Neutral (conf: 0.528)\n",
      "Top 10 SAE Features:\n",
      "  Feature 21110: 7.3188 [ABLATED]\n",
      "  Feature 4456: 7.0675 [ABLATED]\n",
      "  Feature 24583: 6.7122 [ABLATED]\n",
      "  Feature 25797: 6.3374\n",
      "  Feature 9395: 6.0569\n",
      "  Feature 5111: 5.6185\n",
      "  Feature 13142: 5.3574 [ABLATED]\n",
      "  Feature 18317: 5.3225\n",
      "  Feature 20637: 4.7845\n",
      "  Feature 6026: 4.6764\n",
      "\n",
      "\n",
      "ðŸ’¾ Saving ablated results for visualization...\n",
      "ðŸ’¾ Saving ablated results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-01-29T00-42-11_run-058\n",
      "\n",
      "âœ… Ablation experiment complete!\n",
      "   ðŸ“ Ablated results saved to: 2026-01-29T00-42-11_run-058\n",
      "   ðŸŽ¯ Ablated Accuracy: 85.00%\n",
      "   ðŸ”¢ Total tokens: 1542\n",
      "   âœ¨ SAE features: 32768\n",
      "\n",
      "ðŸŒ Start the viewer to see ablated results:\n",
      "   python viz_analysis/feature_probe_server.py\n",
      "   cd sae-viewer && npm start\n"
     ]
    }
   ],
   "source": [
    "# Feature Ablation Experiment\n",
    "# This cell performs ablation by zeroing out specified SAE features and comparing predictions\n",
    "\n",
    "# Configuration\n",
    "# FEATURES_TO_ABLATE = [21110, 24583, 4456]  # <-- Specify feature IDs to ablate (can be single or multiple)\n",
    "FEATURES_TO_ABLATE = [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ABLATION EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "# Define device and move model to it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu only, please install CUDA-compatible Torch\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the SAE using the helper function\n",
    "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
    "\n",
    "# Extract dimensions from the loaded config\n",
    "SAE_INPUT_DIM = sae_config['input_dim']\n",
    "SAE_LATENT_DIM = sae_config['latent_dim']\n",
    "\n",
    "print(f\"Ablating features: {FEATURES_TO_ABLATE}\")\n",
    "print(f\"Layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"SAE Size: {SAE_SIZE} ({SAE_LATENT_DIM} features)\")\n",
    "print(f\"Max Samples: {MAX_SAMPLES}\\n\")\n",
    "\n",
    "# Verify features are within valid range\n",
    "if any(fid < 0 or fid >= SAE_LATENT_DIM for fid in FEATURES_TO_ABLATE):\n",
    "    invalid = [fid for fid in FEATURES_TO_ABLATE if fid < 0 or fid >= SAE_LATENT_DIM]\n",
    "    raise ValueError(f\"Invalid feature IDs (must be 0-{SAE_LATENT_DIM-1}): {invalid}\")\n",
    "\n",
    "# Storage for results\n",
    "baseline_predictions = []\n",
    "ablated_predictions = []\n",
    "sample_data = []\n",
    "\n",
    "# Initialize trackers for SAE features (same as inference cell)\n",
    "feature_stats_ablated = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
    "top_token_tracker_ablated = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "headline_aggregator_ablated = HeadlineFeatureAggregator(top_k=10)\n",
    "all_prompt_metadata_ablated = []\n",
    "\n",
    "# Storage for capturing SAE features during ablation (for tracking)\n",
    "current_sample_data = {\"sae_features\": None, \"token_ids\": None, \"prompt_tokens\": None, \"text\": None, \"idx\": None}\n",
    "\n",
    "# Run baseline inference (no ablation)\n",
    "print(\"ðŸ”¬ Running baseline inference (no ablation)...\")\n",
    "baseline_results = []\n",
    "baseline_features_map = {}  # Store baseline SAE features for comparison\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Forward pass (normal, no intervention)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        pred_id = logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        confidence = probs[0, pred_id].item()\n",
    "        \n",
    "        baseline_results.append({\n",
    "            \"sample_idx\": idx,\n",
    "            \"text\": text,\n",
    "            \"true_label\": model.config.id2label[true_label],\n",
    "            \"predicted_label\": pred_label,\n",
    "            \"predicted_id\": pred_id,\n",
    "            \"confidence\": confidence,\n",
    "            \"logits\": logits.cpu().numpy(),\n",
    "            \"probs\": probs.cpu().numpy()\n",
    "        })\n",
    "        \n",
    "        # Extract baseline SAE features for ablation comparison\n",
    "        # Capture activations from target layer\n",
    "        captured_acts = []\n",
    "        def capture_hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                captured_acts.append(output[0].detach())\n",
    "            else:\n",
    "                captured_acts.append(output.detach())\n",
    "        \n",
    "        target_layer_baseline = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "        temp_hook = target_layer_baseline.register_forward_hook(capture_hook)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        temp_hook.remove()\n",
    "        \n",
    "        if captured_acts:\n",
    "            bert_activation = captured_acts[0].squeeze(0)\n",
    "            \n",
    "            # Filter special tokens\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            valid_mask = attention_mask & not_special\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            if bert_activation.shape[0] > 0:\n",
    "                # Get SAE features\n",
    "                sae_features = sae.encode(bert_activation)\n",
    "                sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "                \n",
    "                # Get max activation per feature across all tokens\n",
    "                max_activations_per_feature = sae_features_cpu.max(axis=0)\n",
    "                \n",
    "                # Get top 10 features\n",
    "                top_10_indices = np.argsort(max_activations_per_feature)[-10:][::-1]\n",
    "                top_features = [\n",
    "                    {\n",
    "                        \"feature_id\": int(fid), \n",
    "                        \"activation\": float(max_activations_per_feature[fid])\n",
    "                    }\n",
    "                    for fid in top_10_indices\n",
    "                ]\n",
    "                total_activation = sum(feat[\"activation\"] for feat in top_features)\n",
    "                \n",
    "                baseline_features_map[idx] = {\n",
    "                    \"top_features\": top_features,\n",
    "                    \"total_activation\": total_activation\n",
    "                }\n",
    "            else:\n",
    "                baseline_features_map[idx] = {\n",
    "                    \"top_features\": [],\n",
    "                    \"total_activation\": 0.0\n",
    "                }\n",
    "        else:\n",
    "            baseline_features_map[idx] = {\n",
    "                \"top_features\": [],\n",
    "                \"total_activation\": 0.0\n",
    "            }\n",
    "        \n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Baseline: {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "baseline_accuracy = sum(1 for r in baseline_results if r[\"predicted_id\"] == test_ds[r[\"sample_idx\"]][\"label\"]) / len(baseline_results)\n",
    "print(f\"âœ“ Baseline accuracy: {baseline_accuracy:.2%}\\n\")\n",
    "\n",
    "# Register intervention hook\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "intervention_hook = create_intervention_hook(sae, FEATURES_TO_ABLATE, device, current_sample_data)\n",
    "hook_handle = target_layer.register_forward_hook(intervention_hook)\n",
    "\n",
    "# Run ablation inference\n",
    "print(\"ðŸ”¬ Running ablation inference (features zeroed)...\")\n",
    "ablated_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Clear sample data\n",
    "        current_sample_data[\"sae_features\"] = None\n",
    "        current_sample_data[\"token_ids\"] = token_ids\n",
    "        current_sample_data[\"prompt_tokens\"] = prompt_tokens\n",
    "        current_sample_data[\"text\"] = text\n",
    "        current_sample_data[\"idx\"] = idx\n",
    "        \n",
    "        # Forward pass with intervention (features ablated)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        pred_id = logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        confidence = probs[0, pred_id].item()\n",
    "        \n",
    "        ablated_results.append({\n",
    "            \"sample_idx\": idx,\n",
    "            \"text\": text,\n",
    "            \"true_label\": model.config.id2label[true_label],\n",
    "            \"predicted_label\": pred_label,\n",
    "            \"predicted_id\": pred_id,\n",
    "            \"confidence\": confidence,\n",
    "            \"logits\": logits.cpu().numpy(),\n",
    "            \"probs\": probs.cpu().numpy()\n",
    "        })\n",
    "        \n",
    "        # Track SAE features (after ablation) if we captured them\n",
    "        if current_sample_data[\"sae_features\"] is not None:\n",
    "            sae_features_cpu = current_sample_data[\"sae_features\"].cpu().numpy()\n",
    "            \n",
    "            # Filter special tokens (same as inference cell)\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool().cpu().numpy()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0).cpu().numpy()\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = np.array([tid not in special_ids for tid in token_ids_tensor])\n",
    "            valid_mask = attention_mask & not_special\n",
    "            \n",
    "            # Filter features and tokens\n",
    "            sae_features_filtered = sae_features_cpu[valid_mask]\n",
    "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask) if valid]\n",
    "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask) if valid]\n",
    "            \n",
    "            if sae_features_filtered.shape[0] > 0:\n",
    "                seq_len = sae_features_filtered.shape[0]\n",
    "                \n",
    "                # Update feature statistics\n",
    "                feature_stats_ablated.update(sae_features_filtered)\n",
    "                \n",
    "                # Track top tokens per feature\n",
    "                top_token_tracker_ablated.update(\n",
    "                    sae_features_filtered,\n",
    "                    filtered_token_ids,\n",
    "                    prompt_idx=idx,\n",
    "                    prompt_text=text,\n",
    "                    prompt_tokens=filtered_prompt_tokens,\n",
    "                    predicted_label=pred_label,\n",
    "                    true_label=model.config.id2label[true_label]\n",
    "                )\n",
    "                \n",
    "                # Aggregate top features at headline level\n",
    "                headline_aggregator_ablated.add_headline_with_ablation_metrics(\n",
    "                    prompt_idx=idx,\n",
    "                    prompt_text=text,\n",
    "                    token_activations=sae_features_filtered,\n",
    "                    token_ids=filtered_token_ids,\n",
    "                    token_strings=filtered_prompt_tokens,\n",
    "                    predicted_label=pred_label,\n",
    "                    true_label=model.config.id2label[true_label],\n",
    "                    baseline_features=baseline_features_map[idx],\n",
    "                    features_to_ablate=FEATURES_TO_ABLATE\n",
    "                )\n",
    "                \n",
    "                # Save prompt metadata\n",
    "                all_prompt_metadata_ablated.append({\n",
    "                    \"row_id\": idx,\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"prompt\": text,\n",
    "                    \"predicted_label\": pred_label,\n",
    "                    \"true_label\": model.config.id2label[true_label],\n",
    "                    \"correct\": pred_id == true_label\n",
    "                })\n",
    "        \n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Ablated: {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "ablated_accuracy = sum(1 for r in ablated_results if r[\"predicted_id\"] == test_ds[r[\"sample_idx\"]][\"label\"]) / len(ablated_results)\n",
    "print(f\"âœ“ Ablated accuracy: {ablated_accuracy:.2%}\\n\")\n",
    "\n",
    "# Compare results and find flipped predictions\n",
    "flipped_samples = []\n",
    "for baseline, ablated in zip(baseline_results, ablated_results):\n",
    "    if baseline[\"predicted_id\"] != ablated[\"predicted_id\"]:\n",
    "        # Get top SAE features for this sample (from baseline run)\n",
    "        # We need to capture activations for this sample\n",
    "        # For now, we'll compute them on-the-fly\n",
    "        \n",
    "        # Tokenize and get activations\n",
    "        inputs = tokenizer(baseline[\"text\"], return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Capture activations\n",
    "        captured_acts = []\n",
    "        def capture_hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                captured_acts.append(output[0].detach())\n",
    "            else:\n",
    "                captured_acts.append(output.detach())\n",
    "        \n",
    "        temp_hook = target_layer.register_forward_hook(capture_hook)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        temp_hook.remove()\n",
    "        \n",
    "        if captured_acts:\n",
    "            bert_activation = captured_acts[0].squeeze(0)\n",
    "            \n",
    "            # Filter special tokens\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            valid_mask = attention_mask & not_special\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            if bert_activation.shape[0] > 0:\n",
    "                # Get SAE features\n",
    "                sae_features = sae.encode(bert_activation)\n",
    "                sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "                \n",
    "                # Get max activation per feature across all tokens\n",
    "                max_activations_per_feature = sae_features_cpu.max(axis=0)\n",
    "                \n",
    "                # Get top 10 features\n",
    "                top_10_indices = np.argsort(max_activations_per_feature)[-10:][::-1]\n",
    "                top_features = [\n",
    "                    {\"feature_id\": int(fid), \"activation\": float(max_activations_per_feature[fid]), \n",
    "                     \"ablated\": fid in FEATURES_TO_ABLATE}\n",
    "                    for fid in top_10_indices\n",
    "                ]\n",
    "            else:\n",
    "                top_features = []\n",
    "        else:\n",
    "            top_features = []\n",
    "        \n",
    "        flipped_samples.append({\n",
    "            \"sample_idx\": baseline[\"sample_idx\"],\n",
    "            \"text\": baseline[\"text\"],\n",
    "            \"true_label\": baseline[\"true_label\"],\n",
    "            \"baseline_pred\": baseline[\"predicted_label\"],\n",
    "            \"baseline_conf\": baseline[\"confidence\"],\n",
    "            \"ablated_pred\": ablated[\"predicted_label\"],\n",
    "            \"ablated_conf\": ablated[\"confidence\"],\n",
    "            \"top_features\": top_features\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ABLATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ablated Features: {FEATURES_TO_ABLATE}\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.2%}\")\n",
    "print(f\"Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
    "print(f\"Accuracy Change: {(ablated_accuracy - baseline_accuracy):.2%}\")\n",
    "print(f\"\\nFlipped Predictions: {len(flipped_samples)}/{len(baseline_results)} samples\")\n",
    "print(f\"Flip Rate: {len(flipped_samples)/len(baseline_results):.2%}\\n\")\n",
    "\n",
    "if flipped_samples:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"FLIPPED PREDICTIONS (showing first {min(10, len(flipped_samples))}):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, flip in enumerate(flipped_samples[:10], 1):\n",
    "        print(f\"\\n--- Sample #{flip['sample_idx']} ---\")\n",
    "        print(f\"Text: {flip['text'][:120]}{'...' if len(flip['text']) > 120 else ''}\")\n",
    "        print(f\"True Label: {flip['true_label']}\")\n",
    "        print(f\"Original: {flip['baseline_pred']} (conf: {flip['baseline_conf']:.3f}) â†’ \"\n",
    "              f\"Ablated: {flip['ablated_pred']} (conf: {flip['ablated_conf']:.3f})\")\n",
    "        \n",
    "        if flip['top_features']:\n",
    "            print(\"Top 10 SAE Features:\")\n",
    "            for feat in flip['top_features']:\n",
    "                ablated_marker = \" [ABLATED]\" if feat['ablated'] else \"\"\n",
    "                print(f\"  Feature {feat['feature_id']}: {feat['activation']:.4f}{ablated_marker}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No predictions were flipped by ablating these features.\")\n",
    "\n",
    "# Save ablated results in the same format as inference cell\n",
    "print(\"\\nðŸ’¾ Saving ablated results for visualization...\")\n",
    "\n",
    "# Create run directory for ablated results (use directly, don't create separate folder)\n",
    "ablated_run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"ðŸ’¾ Saving ablated results to: {ablated_run_dir}\")\n",
    "\n",
    "# Compute final statistics for ablated run\n",
    "stats_ablated = feature_stats_ablated.get_stats()\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric_ablated = {}\n",
    "for metric_name, values in stats_ablated.items():\n",
    "    if metric_name == \"mean_act_squared\":\n",
    "        continue\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric_ablated[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {\n",
    "                \"mean_activation\": float(stats_ablated[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats_ablated[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats_ablated[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# 1. Save prompts metadata\n",
    "prompts_file = ablated_run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata_ablated:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics\n",
    "feature_stats_file = ablated_run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": SAE_LATENT_DIM,\n",
    "    \"total_tokens\": feature_stats_ablated.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"accuracy\": ablated_accuracy,\n",
    "    \"num_samples\": len(all_prompt_metadata_ablated),\n",
    "    \"mean_act_squared\": stats_ablated[\"mean_act_squared\"].tolist(),\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric_ablated[metric_name]\n",
    "        }\n",
    "        for metric_name in stats_ablated.keys() if metric_name != \"mean_act_squared\"\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature\n",
    "feature_tokens_file = ablated_run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker_ablated.export()\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save headline-level features\n",
    "headline_features_file = ablated_run_dir / \"headline_features.json\"\n",
    "with open(headline_features_file, \"w\") as f:\n",
    "    json.dump(headline_aggregator_ablated.export(), f, indent=2)\n",
    "\n",
    "# 5. Save metadata\n",
    "metadata_file = ablated_run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata_ablated),\n",
    "        \"total_tokens\": feature_stats_ablated.total_tokens,\n",
    "        \"accuracy\": ablated_accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": SAE_INPUT_DIM,\n",
    "        \"latent_dim\": SAE_LATENT_DIM,\n",
    "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"ablated_features\": FEATURES_TO_ABLATE,\n",
    "        \"note\": f\"SAE sparse features with predictions (features {FEATURES_TO_ABLATE} ablated)\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Ablation experiment complete!\")\n",
    "print(f\"   ðŸ“ Ablated results saved to: {ablated_run_dir.name}\")\n",
    "print(f\"   ðŸŽ¯ Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
    "print(f\"   ðŸ”¢ Total tokens: {feature_stats_ablated.total_tokens}\")\n",
    "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
    "print(f\"\\nðŸŒ Start the viewer to see ablated results:\")\n",
    "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
    "print(f\"   cd sae-viewer && npm start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefe6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING SAE FEATURES FROM FINBERT\n",
      "============================================================\n",
      "âœ“ Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
      "  Layer: 8\n",
      "  Input dim: 768\n",
      "  Latent dim: 32768\n",
      "âœ“ SAE loaded: 768 dims â†’ 32768 sparse features\n",
      "\n",
      "ðŸ’¾ Saving results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-01-27T21-41-46_run-045\n",
      "\n",
      "ðŸ”¬ Processing 100 samples...\n",
      "   Layer: 8\n",
      "   Using SAE: 32768 sparse features\n",
      "   Filtering: ALL special tokens excluded (content only)\n",
      "\n",
      "Processed 10/100 samples\n",
      "Processed 20/100 samples\n",
      "Processed 30/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 50/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 70/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 90/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "ðŸ“Š Computing feature statistics...\n",
      "ðŸŽ¯ Model Accuracy: 87.00%\n",
      "\n",
      "ðŸ’¾ Saving results...\n",
      "\n",
      "âœ… COMPLETE!\n",
      "   ðŸ“ Results saved to: 2026-01-27T21-41-46_run-045\n",
      "   ðŸŽ¯ Accuracy: 87.00%\n",
      "   ðŸ”¢ Total tokens: 1542\n",
      "   âœ¨ SAE features: 32768\n",
      "\n",
      "ðŸ“Š Top 5 features by mean activation:\n",
      "   1. Feature 21110: mean=2.3480, max=8.7495, frac=96.95%\n",
      "   2. Feature 24583: mean=2.0934, max=8.8531, frac=96.95%\n",
      "   3. Feature 4456: mean=2.0080, max=10.7064, frac=95.46%\n",
      "   4. Feature 21969: mean=1.9741, max=5.8842, frac=97.80%\n",
      "   5. Feature 4247: mean=1.8205, max=6.2565, frac=98.90%\n",
      "\n",
      "ðŸŒ Start the viewer to see results:\n",
      "   python viz_analysis/feature_probe_server.py\n",
      "   cd sae-viewer && npm start\n"
     ]
    }
   ],
   "source": [
    "# Inference (non refactored)\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Start of Inference\n",
    "# Add project root to path to import utilities\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
    "\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the SAE using the helper function\n",
    "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
    "\n",
    "# Extract dimensions from the loaded config\n",
    "SAE_INPUT_DIM = sae_config['input_dim']\n",
    "SAE_LATENT_DIM = sae_config['latent_dim']\n",
    "\n",
    "print(f\"âœ“ SAE loaded: {SAE_INPUT_DIM} dims â†’ {SAE_LATENT_DIM} sparse features\")\n",
    "\n",
    "# Create run directory using the same utility as main.py\n",
    "# This ensures the server can find it automatically in analysis_data/\n",
    "run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"\\nðŸ’¾ Saving results to: {run_dir}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sae.to(device)\n",
    "model.eval()\n",
    "sae.eval()\n",
    "\n",
    "# Load dataset\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "# Feature statistics tracker (per-token aggregation)\n",
    "class FeatureStatsAggregator:\n",
    "    def __init__(self, feature_dim: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.total_tokens = 0\n",
    "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray):\n",
    "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
    "        self.total_tokens += token_activations.shape[0]\n",
    "        self.sum_activations += token_activations.sum(axis=0)\n",
    "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
    "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
    "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
    "    \n",
    "    def get_stats(self):\n",
    "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
    "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
    "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
    "        return {\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": self.max_activations,\n",
    "            \"fraction_active\": frac_active,\n",
    "            \"mean_act_squared\": mean_act_squared\n",
    "        }\n",
    "\n",
    "# Top token tracker per feature\n",
    "class FeatureTopTokenTracker:\n",
    "    def __init__(self, feature_dim: int, top_k: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.top_k = top_k\n",
    "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
    "        self.heaps = [[] for _ in range(feature_dim)]\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
    "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
    "               predicted_label: str = None, true_label: str = None):\n",
    "        \"\"\"Update with tokens from one prompt\"\"\"\n",
    "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
    "            # For each token, find top features\n",
    "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
    "            \n",
    "            for feat_id in top_features:\n",
    "                activation = float(act_vec[feat_id])\n",
    "                if activation <= 0:\n",
    "                    continue\n",
    "                \n",
    "                heap = self.heaps[feat_id]\n",
    "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"activation\": activation,\n",
    "                    \"token_str\": token_str,\n",
    "                    \"token_id\": int(token_id),\n",
    "                    \"token_position\": int(token_pos),\n",
    "                    \"prompt_index\": int(prompt_idx),\n",
    "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
    "                    \"prompt_snippet\": prompt_text[:160],\n",
    "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
    "                    \"true_label\": true_label,\n",
    "                }\n",
    "                \n",
    "                if len(heap) < self.top_k:\n",
    "                    heapq.heappush(heap, (activation, metadata))\n",
    "                elif activation > heap[0][0]:\n",
    "                    heapq.heapreplace(heap, (activation, metadata))\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export top tokens for each feature\"\"\"\n",
    "        result = {}\n",
    "        for feat_id in range(self.feature_dim):\n",
    "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
    "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
    "        return result\n",
    "\n",
    "# Aggregate top features per headline (sample-level view)\n",
    "class HeadlineFeatureAggregator:\n",
    "    def __init__(self, top_k: int = 10):\n",
    "        self.top_k = top_k\n",
    "        self.headlines = []  # List of headline metadata with top features\n",
    "    \n",
    "    def add_headline(self, prompt_idx: int, prompt_text: str,\n",
    "                     token_activations: np.ndarray,\n",
    "                     token_ids: List[int],\n",
    "                     token_strings: List[str],\n",
    "                     predicted_label: str, true_label: str):\n",
    "        \"\"\"Aggregate features across all tokens in a headline\"\"\"\n",
    "        if token_activations.size == 0:\n",
    "            return\n",
    "        # Max activation per feature and which token triggered it\n",
    "        max_token_idx_per_feature = token_activations.argmax(axis=0)  # [feature_dim]\n",
    "        max_activation_per_feature = token_activations.max(axis=0)     # [feature_dim]\n",
    "        \n",
    "        # Get top K features by their max activation in this headline\n",
    "        top_feature_ids = np.argsort(max_activation_per_feature)[-self.top_k:][::-1]\n",
    "        \n",
    "        features = [\n",
    "            {\n",
    "                \"feature_id\": int(fid),\n",
    "                \"max_activation\": float(max_activation_per_feature[fid]),\n",
    "                \"token_position\": int(max_token_idx_per_feature[fid]),\n",
    "                \"token_id\": int(token_ids[max_token_idx_per_feature[fid]]),\n",
    "                \"token_str\": token_strings[max_token_idx_per_feature[fid]],\n",
    "            }\n",
    "            for fid in top_feature_ids if max_activation_per_feature[fid] > 0\n",
    "        ]\n",
    "        \n",
    "        self.headlines.append({\n",
    "            \"row_id\": int(prompt_idx),\n",
    "            \"prompt\": prompt_text,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"true_label\": true_label,\n",
    "            \"correct\": predicted_label == true_label,\n",
    "            \"num_tokens\": int(token_activations.shape[0]),\n",
    "            \"features\": features\n",
    "        })\n",
    "    \n",
    "    def export(self):\n",
    "        return self.headlines\n",
    "\n",
    "# Initialize trackers for SAE features\n",
    "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
    "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "headline_aggregator = HeadlineFeatureAggregator(top_k=10)\n",
    "\n",
    "# Storage for per-sample metadata\n",
    "all_prompt_metadata = []\n",
    "all_prediction_metadata = []\n",
    "\n",
    "# Hook to capture activations\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook on target layer\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "print(f\"\\nðŸ”¬ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
    "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
    "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        captured_activations.clear()\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        \n",
    "        # Get captured activation and pass through SAE\n",
    "        if captured_activations:\n",
    "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
    "            bert_activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (same as training) - do on GPU\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "            \n",
    "            # Filter activations on GPU\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            # Skip if no valid tokens\n",
    "            if bert_activation.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
    "            sae_features = sae.encode(bert_activation)\n",
    "            \n",
    "            # Only now move to CPU for numpy conversion and token filtering\n",
    "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
    "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
    "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
    "            \n",
    "            seq_len = sae_features_cpu.shape[0]\n",
    "            \n",
    "            # Update feature statistics with SAE features\n",
    "            feature_stats.update(sae_features_cpu)\n",
    "            \n",
    "            # Track top tokens per feature\n",
    "            top_token_tracker.update(\n",
    "                sae_features_cpu, \n",
    "                filtered_token_ids, \n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                prompt_tokens=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,  # Pass prediction info\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Aggregate top features at headline level\n",
    "            headline_aggregator.add_headline(\n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                token_activations=sae_features_cpu,\n",
    "                token_ids=filtered_token_ids,\n",
    "                token_strings=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Save prompt metadata\n",
    "            all_prompt_metadata.append({\n",
    "                \"row_id\": idx,\n",
    "                \"seq_len\": seq_len,\n",
    "                \"prompt\": text,\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"true_label\": model.config.id2label[true_label],\n",
    "                \"correct\": pred_id == true_label\n",
    "            })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Compute final statistics\n",
    "print(\"\\nðŸ“Š Computing feature statistics...\")\n",
    "stats = feature_stats.get_stats()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
    "print(f\"ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric = {}\n",
    "for metric_name, values in stats.items():\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
    "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# Save results\n",
    "print(\"\\nðŸ’¾ Saving results...\")\n",
    "\n",
    "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
    "prompts_file = run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
    "feature_stats_file = run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": SAE_LATENT_DIM,\n",
    "    \"total_tokens\": feature_stats.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
    "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
    "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric[metric_name]\n",
    "        }\n",
    "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
    "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save headline-level features\n",
    "headline_features_file = run_dir / \"headline_features.json\"\n",
    "with open(headline_features_file, \"w\") as f:\n",
    "    json.dump(headline_aggregator.export(), f, indent=2)\n",
    "\n",
    "# 5. Save metadata\n",
    "metadata_file = run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata),\n",
    "        \"total_tokens\": feature_stats.total_tokens,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": SAE_INPUT_DIM,\n",
    "        \"latent_dim\": SAE_LATENT_DIM,\n",
    "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"note\": \"SAE sparse features with predictions\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… COMPLETE!\")\n",
    "print(f\"   ðŸ“ Results saved to: {run_dir.name}\")\n",
    "print(f\"   ðŸŽ¯ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"   ðŸ”¢ Total tokens: {feature_stats.total_tokens}\")\n",
    "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
    "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
    "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
    "    metrics = feat['metrics']\n",
    "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
    "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
    "          f\"max={metrics['max_activation']:.4f}, \"\n",
    "          f\"frac={metrics['fraction_active']:.2%}\")\n",
    "\n",
    "print(f\"\\nðŸŒ Start the viewer to see results:\")\n",
    "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
    "print(f\"   cd sae-viewer && npm start\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def14c72",
   "metadata": {},
   "source": [
    "Testing Inference based on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de85884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BULLISH  | TSLA beats earnings expectations and raises full-year guidance.\n",
      "BEARISH  | Apple shares fall after reporting weaker-than-expected iPhone sales.\n",
      "NEUTRAL  | The company reported results largely in line with analyst expectations.\n",
      "BEARISH  | Amazon warns of margin pressure due to rising logistics costs.\n",
      "BULLISH  | NVIDIA stock surges as demand for AI chips remains strong.\n",
      "BEARISH  | The firm announced a restructuring plan, sending shares lower.\n",
      "NEUTRAL  | Revenue growth slowed quarter-over-quarter, but profitability improved.\n",
      "BEARISH  | Investors remain cautious ahead of the Federal Reserve meeting.\n",
      "BULLISH  | Strong cash flow and reduced debt boosted investor confidence.\n",
      "BEARISH  | The outlook remains uncertain amid macroeconomic headwinds.\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis on simple headlines\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "\n",
    "example_sentences = [\n",
    "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
    "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
    "    \"The company reported results largely in line with analyst expectations.\",\n",
    "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
    "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
    "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
    "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
    "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
    "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
    "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "# optional: move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    pred_id = out.logits.argmax(dim=-1).item()\n",
    "    return model.config.id2label[pred_id]\n",
    "\n",
    "for text in example_sentences:\n",
    "    label = predict_sentiment(text)\n",
    "    print(f\"{label.upper():8} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0b474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$ALLY - Ally Financial pulls outlook',\n",
       " '$DELL $HPE - Dell, HPE targets trimmed on compute headwinds',\n",
       " \"$PRTY - Moody's turns negative on Party City\",\n",
       " '$SAN: Deutsche Bank cuts to Hold',\n",
       " '$SITC: Compass Point cuts to Sell',\n",
       " '$TAP - Barclays cools on Molson Coors',\n",
       " '$TAP: Barclays cuts to Equal Weight',\n",
       " 'Analysts Eviscerate Musk\\'s Cybertruck: \"0% Of Responses Felt It Will Be A Success\"',\n",
       " 'Barclays assigns only a 20% chance that studies on a Gilead antiviral drug being done in China will succeed againstâ€¦',\n",
       " \"BTIG points to breakfast pressure for Dunkin' Brands\",\n",
       " \"Children's Place downgraded to neutral from outperform at Wedbush, price target slashed to $60 from $130\",\n",
       " 'Clovis Oncology downgraded to in line from outperform at Evercore ISI',\n",
       " 'Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦',\n",
       " \"Goldman pulls Progressive from Goldman's conviction list; shares -2.7%\",\n",
       " 'Hanesbrands downgraded to underperform vs. neutral at BofA Merrill Lynch',\n",
       " 'Intelsat cut to Market Perform at Raymond James',\n",
       " 'LendingTree price target cut to $350 from $400 at SunTrust Robinson Humphrey',\n",
       " 'Mizuho cuts XLNX target on near-term headwinds',\n",
       " 'MPLX cut at Credit Suisse on potential dilution from Marathon strategic review',\n",
       " 'Netflix downgraded to underperform at Wells Fargo',\n",
       " 'NortonLifeLock stock price target cut to $18 from $25 at Deutsche Bank',\n",
       " 'Norwegian Cruise stock price target cut to $48 from $55 at CFRA',\n",
       " 'Novus Therapeutics stock price target cut to $2.50 from $3.75 at Ascendiant Capital',\n",
       " 'Nutrien stock price target cut to $54 vs. $55 at BofA Merrill Lynch',\n",
       " 'Okta -2% on valuation downgrade',\n",
       " 'Solarworlds stock price target cut to $23 vs. $24 at Instinet',\n",
       " 'Trendforce cuts iPhone estimate after Foxconn delay',\n",
       " 'Wayfair price target lowered to $110 from $120 at Stifel, buy rating maintained',\n",
       " 'WWE stock price target cut to $58 from $92 at MKM Partners',\n",
       " '$ABEO as expected, keeps going higher. Cantor doubled its price target this morning to $4',\n",
       " '$CACI - CACI gains a bull on growth acceleration',\n",
       " '$CNHI - CNH Industrial upped to Buy at Deutsche Bank on valuation',\n",
       " '$DVN $HES $VET - Devon Energy, Hess upgraded at J.P. Morgan on improved E&P outlook',\n",
       " \"$ENR - Energizer shakes off JPMorgan's bear call\",\n",
       " '$GOOS - Baird bullish on beat-up Canada Goose',\n",
       " '$NKE - BMO Capital joins Nike bull camp',\n",
       " '$OIH $BKR $NOV - Buy oil service firms, Bernstein says after seven-year bearish view',\n",
       " '$SMPQY $HSIC $XRAY - Goldman ups view on Henry Schein in premarket analyst action',\n",
       " '$STML: Alliance Global Partners starts at Buy',\n",
       " '$URBN - Urban Outfitters stands out in mall sector - BofA',\n",
       " '$ZYME: H.C. Wainwright starts at Buy',\n",
       " 'Adobe price target raised to $350 vs. $320 at Canaccord',\n",
       " 'AM Best Revises Outlooks to Positive for PacÃ­fico CompaÃ±Ã­a de Seguros y Reaseguros S.A.',\n",
       " \"AMD +1.3% after Cowen's target lift\",\n",
       " 'Applied Materials, Inc. Reported Earnings Last Week And Analysts Are Already Upgrading Their Estimates',\n",
       " 'Arch Coal rated Buy at Benchmark, seeing opportunity after selloff',\n",
       " 'Autodesk stock price target raised to $162 from $149 at Wedbush',\n",
       " 'Avnet stock price target raised to $37 vs. $35 at SunTrust Robinson Humphrey',\n",
       " 'Boeing started at buy with $375 stock price target at Benchmark',\n",
       " \"BofA sees 'solid demand backdrop' for D.R. Horton in 2020\",\n",
       " 'Broadcom stock price target raised to $361 vs. $322 at SunTrust Robinson Humphrey',\n",
       " 'Cantor sees 19% upside in J&J in premarket analyst action',\n",
       " 'Champions Oncology started at speculative buy with $11 stock price target at Benchmark',\n",
       " 'Citi raises Alphabet on margin potential',\n",
       " 'Citigroup stock price target raised to $127 from $124 at Oppenheimer',\n",
       " 'ETF assets to surge tenfold in 10 years to $50 trillion, Bank of America predicts',\n",
       " 'GE upgraded from Sell by longtime bear',\n",
       " 'Goldman Sachs stock price target raised to $367 from $358 at Oppenheimer',\n",
       " 'GrubHub stock price target raised to $50 from $40 at Stifel Nicolaus',\n",
       " 'Halliburton started at buy with $26 stock price target at Deutsche Bank',\n",
       " 'Helen of Troy started at outperform with $210 stock price target at Oppenheimer',\n",
       " 'Highlight: Edward Jones Sr. Equity Analyst Jennifer Roland on ExxonMobil reducing 2020 Capex by 30%: â€œDefinitely aâ€¦',\n",
       " 'Humana stock price target raised to $370 from $350 at SunTrust RH',\n",
       " 'JMP Securities upgrades snap to outperform with $20 pt',\n",
       " 'Liberty Global PLC maintained as buy with $32 price target at Benchmark',\n",
       " 'Lululemon Athletica price target raised to $255 from $220 at CFRA',\n",
       " 'Lumentum initiated as positive at Susquehanna',\n",
       " 'Markel Corporation Beat Analyst Estimates: See What The Consensus Is Forecasting For Next Year',\n",
       " 'Mednax upgraded to buy from hold at Stifel, price target raised to $33 from $25',\n",
       " 'Model N upgraded to overweight from neutral at JPMorgan, price target raised to $36 from $30',\n",
       " 'Morgan Stanley sees GM doubling in bull case scenario',\n",
       " 'Motorola Solutions stock fair value estimate raised to $192 from $167 at MKM Partners',\n",
       " 'New for subscribers: Analysts continue to upgrade stocks like Tesla and eBay on hopes the rebound is for realâ€¦',\n",
       " 'Omnicell stock price target raised to $96 vs. $90 at Benchmark',\n",
       " 'Peloton bulls defend the upside case',\n",
       " 'Piper Jaffray analyst Erinn Murphy reiterated an Overweight rating and $44.00 price target on Crocs (NASDAQ: $CROX)â€¦',\n",
       " 'Progyny started at overweight with $26 stock price target at J.P. Morgan',\n",
       " 'Snap Analyst Projects 37% Revenue Growth In 2020',\n",
       " 'Spirit Airlines stock price target raised to $55 from $49 at Deutsche Bank',\n",
       " 'State Street stock price target raised to $84 from $73 at Buckingham',\n",
       " \"Stock Market Update: AMD's price target raised to $47 at Cowen\",\n",
       " 'Synaptics stock price target raised to $76 from $60 at Susquehanna',\n",
       " 'TechnipFMC started at buy with $33 stock price target at Deutsche Bank',\n",
       " 'Tesla stock price target raised to $290 from $260 at Deutsche Bank',\n",
       " 'TJX stock price target raised to $70 from $62 at MKM Partners',\n",
       " 'Twitter upgraded to positive from neutral at Susquehanna',\n",
       " 'Uber stock price target raised to $42 from $31 at Susquehanna',\n",
       " 'Uber stock price target raised to $48 from $46 at HSBC Global',\n",
       " 'UBS Upgrades Hasbro As Stock Price Bakes In Tariffs, Slow Holiday',\n",
       " 'UnitedHealth stock price target raised to $335 from $310 at SunTrust RH',\n",
       " 'Vertex Pharmaceuticals stock price target raised to $245 from $235 at BofA Merrill Lynch',\n",
       " 'Xilinx stock price target raised to $97 vs. $94 at SunTrust Robinson Humphrey',\n",
       " 'AM Best Revises Outlooks to Stable for Sublimity Insurance Company',\n",
       " 'Analyst: Amazon Blocking FedEx Ground Good News For UPS',\n",
       " 'Analysts Expect Breakeven For China Online Education Group (NYSE:COE)',\n",
       " \"Crown Holdings, Inc. Full-Year Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Here are the best analyst calls of the week on Wall Street including Disney and a satellite play',\n",
       " 'Little impact on Netflix from Disney Plus launch - Credit Suisse',\n",
       " \"Mitek bull calls pullback 'an overreaction'\",\n",
       " 'Peabody Energy started at hold at Benchmark',\n",
       " \"Skechers U.S.A., Inc. Annual Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " \"T. Boone Pickens' BP Capital Adds 3 Energy Stocks to Portfolio\",\n",
       " 'Tenable Holdings, Inc. Just Released Its Third-Quarter And Analysts Have Been Updating Their Estimates',\n",
       " 'Top Analyst Upgrades and Downgrades: Cardinal Health, Dominoâ€™s, FedEx, Ford, GE, Grubhub, HSBC, Marvell, Twitter, Uber and More',\n",
       " \"Tyson Foods, Inc. First-Quarter Results Just Came Out: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Wayfair initiated as hold with $95 price target at SunTrust Robinson Humphrey',\n",
       " \"Central bank 'collateral damage' is skewing financial markets, one economist says\",\n",
       " 'China central bank warns high financial risks amid rising economic headwinds - Reuters',\n",
       " 'ECB data show eurozone banks had weak profits before coronavirus',\n",
       " \"Fed's Williams says U.S. economy is clearly facing several challenges\",\n",
       " \"Former Federal Reserve Chairman Ben Bernanke says he doesn't see a quick, sharp rebound in the economy after its prâ€¦\",\n",
       " 'Jerome Powell comes close to acknowledging that the Federal Reserve may not have the firepower to fight the next reâ€¦',\n",
       " 'No, The Fed Won\\'t \"Save The Market\" - Here\\'s Why',\n",
       " '\"There is every reason to believe that the economic rebound, when it comes, can be robust,\" Fed Chair Jerome Powellâ€¦',\n",
       " '$ECONX: Federal Reserve announces extensive new measures to support the economy',\n",
       " 'Commercial and industrial loans at all commercial banks climb by $105.8 billion to $2.845 trillion in the week endiâ€¦',\n",
       " 'Fed surprises market with program to support corporate bonds amid coronavirus pandemic',\n",
       " 'Highlight: The Fed has launched an unprecedented round of asset purchases. \"The idea here is the Federal Reserve isâ€¦',\n",
       " \"The World Bank plans to lend Uganda $1.9 billion to help finance the East African nation's budget deficit\",\n",
       " '\"We encourage families to explore virtual #financialliteracy resources. The Federal Reserve Bank of Richmond createâ€¦',\n",
       " 'Africa-Europe Alliance: Denmark provides \\x8010 million for sustainable development under the EU External Investment Pâ€¦',\n",
       " \"Bank agencies announced they'll temporarily lower the community bank leverage ratio to 8%. The rule change -\\x9d requirâ€¦\",\n",
       " 'Bank of Ireland : Mortgage Bank - 7 KB #BankofIreland #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 14-Day Repo Auction Announcement -24 February 2020 #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 30-day CD Auction Press Release #economy #MarketScreener',\n",
       " 'Brazil economists forecast more rate cuts ahead',\n",
       " 'Buyback Backlash Begins: Fed Will Limit Buybacks & Dividends For Companies Using Its Credit Facility',\n",
       " 'Central Bank of Nigeria Communique No. 128 of the Monetary Policy Committee Meeting of Jan... #Stockâ€¦',\n",
       " 'Central banks donâ€™t have as much monetary policy power as they used to, former BOE policy maker Ian McCafferty says',\n",
       " \"Cleveland Fed President Loretta Mester said the coronavirus outbreak is a threat to the economy but doesn't yet jusâ€¦\",\n",
       " 'Efforts to support the faltering economy could extend to some form of direct yield curve control. Newâ€¦',\n",
       " \"Fed Chair Jay Powell grilled on China's cryptocurrency plans, US response by @readDanwrite\",\n",
       " 'Fed Chair Jerome Powell was chided by a Democratic lawmaker for his attendance at a party thrown by Amazon CEO Jeffâ€¦',\n",
       " 'Fed Chairman Jerome Powell puts lawmakers on notice that fiscal policy may need to play a bigger role countering doâ€¦',\n",
       " 'Fed minutes to tell a story',\n",
       " 'Fed Officials Weigh Risks Of Covid-19 -- Update #economy #MarketScreener',\n",
       " 'Fed President Loretta Mester due to speak in 5 minutes.',\n",
       " 'Fed removes reference to \"global developments\", has yet to blame global warming for repo crisis',\n",
       " 'Fedâ€™s Rosengren Says Pursuing 2% Inflation Could Distort Markets',\n",
       " 'Federal Reserve Keeps Rates Steady and Sees Long Pause #currency #MarketScreener',\n",
       " 'Federal Reserve officials will feel comfortable maintaining their wait-and-see posture on interest rates after Fridâ€¦',\n",
       " \"Fed's 42-Day Repo 2x Oversubscribed In Scramble For Year End Liquidity\",\n",
       " \"Fed's Bullard said actions taken by the Fed and other authorities during this time period shouldn't be seen as stimâ€¦\",\n",
       " \"Fed's Mester says central bank can watch and wait on interest rates\",\n",
       " \"Fed's Rosengren backs 'patient' approach to any interest-rate moves\",\n",
       " \"Fed's Williams says rates are 'in the right place' but policy is not set in stone\",\n",
       " 'Global Risks to Extend Czech Rate Lockdown: Decision Day Guide',\n",
       " 'Heard on the Street: The Fed may need banks to lever up to absorb more activity, but it needs to do so in a way thaâ€¦',\n",
       " 'In the battle of central-bank stimulus bazookas, the Fed is winning out',\n",
       " 'In undertaking what will undoubtedly be its largest rescue effort ever, the Federal Reserve announced programs thatâ€¦',\n",
       " \"Indonesia's central bank says the New York Federal Reserve will provide it with a $60 billion repurchase facility tâ€¦\",\n",
       " 'Italy may be willing to compromise on its holdout against a German plan for further integration of the EUâ€™s bankingâ€¦',\n",
       " 'LISTEN NOW: The Fed continues to make moves to combat the coronavirus crisis, including assuring limitless asset puâ€¦',\n",
       " 'LIVE: Fed Chair Powell testifies before the Senate Banking Committee',\n",
       " 'Markets bet Fed is pushed to cut rates in coronavirus response',\n",
       " 'Minneapolis Fed chief Neel Kashkari says monetary policy can play the kind of redistributing role once thought to bâ€¦',\n",
       " 'Morocco is considering tapping at least part of a $2.97 billion liquidity line with the IMF as the coronavirus outbâ€¦',\n",
       " \"New Zealand central bank governor Adrian Orr may signal he's prepared to cut interest rates to cushion the economic...\",\n",
       " \"Norway's central bank governor says it's too early to reach any conclusions on the long term outlook for inflation\",\n",
       " 'Praet: ECB Needs Governments to Act',\n",
       " 'Rabobank: \"Fed\\'s Kashkari Has Just Come Out With A Jaw-Dropping Policy Shift\"',\n",
       " 'RBI May Cut Rates Later This Year, TS Lombard Says',\n",
       " 'RBIâ€™s Long-Term Repo Operations Seen As Stealth Move To Bring Down Rates',\n",
       " 'Spain to guarantee up to 80% of SME bank loans to ease virus impact #economy #MarketScreener',\n",
       " \"The Bank of England's Financial Policy Committee says it's prepared to take further action as needed to help the fiâ€¦\",\n",
       " \"The Bank of England's Term Funding Scheme... #Stock #MarketScreener\",\n",
       " 'The Committee lowers the CBR to 7.25PC #Stock #MarketScreener',\n",
       " 'The consensus case for following the Fed is right, Ashok Bhatia of Neuberger Bergman tells @FerroTVâ€¦',\n",
       " \"The Fed mounted an extraordinary new array of programs to offset the 'severe disruptions' to the economy caused byâ€¦\",\n",
       " 'The Federal Reserve has launched an array of programs aimed at helping the markets and economy through the coronaviâ€¦',\n",
       " 'The Federal Reserve System includes 12 districts. Using those numbers, 1-12, is a handy way to learn about the centâ€¦',\n",
       " 'The FRED Blog: Since the end of the Great Recession, market-based measures of long-run inflation expectations haveâ€¦',\n",
       " \"The IMF may launch a new program that could back up the Federal Reserve's campaign to keep dollars flowing in the gâ€¦\",\n",
       " 'The Most Momentous Rate Decision This Month Isnâ€™t at Fed or ECB',\n",
       " '-The U.K. will go into full lockdown to stymie the spread of coronavirus -The Fed offers to directly finance U.S. câ€¦',\n",
       " 'Top economists say that the spread of the new coronavirus outbreak makes it more likely that the Federal Reserve wiâ€¦',\n",
       " 'Two crisis-veteran Federal Reserve officials said the central bank has plenty of room for more moves following a flâ€¦',\n",
       " \"Two weeks later, no change, Fed ain't stopping the selling\",\n",
       " 'U.S. Fed preparing to buy new small business payroll loans: Wall Street Journal',\n",
       " 'UK starts first stage of 330 billion pound loan guarantee scheme',\n",
       " 'Zambiaâ€™s kwacha staged a comeback on Tuesday after the central bank raised the amount of funds that commercial lendâ€¦',\n",
       " 'Bank of America sees demand surge for paycheck protection loans',\n",
       " '$HON - Honeywell says MAX production freeze to hurt 2020 sales growth',\n",
       " 'Africa retailer Jumia suspends e-commerce in Cameroon',\n",
       " 'Amazon blames holiday delivery delays on winter storms and high demand',\n",
       " 'Beckhamsâ€™ business empire sees revenues drop 18%',\n",
       " 'BREAKING: FDA issues ban on some fruit and mint flavored vaping products',\n",
       " 'Bristol-Myers says trial of Opdivo plus Yervoy in melanoma treatment failed to meet main goal',\n",
       " 'Casino Operator Wynn Resorts Is Losing Up To $2.6 Million Per Day From Coronavirus Shutdown',\n",
       " 'China Solar Group Flags Raw Material, Labor Shortages on Virus',\n",
       " 'Fiat Chrysler Relies on Ram Growth as Europe Losses Mount',\n",
       " 'Ford recalls 262,000 pickup trucks with defective tailgate latches',\n",
       " 'Fox says cable ad revenue was hurt by impeachment coverage last quarter',\n",
       " 'ICYMI: Transport for London said Uber $UBER was not \"fit and proper\" to hold an operating license in London after aâ€¦',\n",
       " 'Jetstar to cut capacity by 10% in January as pilots take industrial action',\n",
       " \"Lowe's to shut 34 Canadian stores as it reports revenue miss\",\n",
       " \"Macy's says temporary issue with e-commerce business also impacted Q3 performance\",\n",
       " 'Mahindra Says Coronavirus May Jeopardize India Emissions Rollout',\n",
       " 'Major blockchain developer ConsenSys announces job losses #economy #MarketScreener',\n",
       " 'NTSB cites Uber, driver in fatal crash']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Visualisation for Dataset\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "test_ds[\"text\"][0:200]\n",
    "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "#ds2[\"validation\"][\"text\"][34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL INFERENCE WITHOUT SAEs\n",
      "============================================================\n",
      "\n",
      "ðŸ”¬ Running inference on 100 test samples...\n",
      "   Device: cuda\n",
      "   Model: ./finbert_twitter_ft/best\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   4%|â–         | 100/2388 [00:01<00:33, 67.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… INFERENCE COMPLETE (WITHOUT SAEs)\n",
      "============================================================\n",
      "   ðŸ“Š Total Samples: 100\n",
      "   âœ“ Correct Predictions: 87\n",
      "   âœ— Incorrect Predictions: 13\n",
      "   ðŸŽ¯ Model Accuracy: 87.00%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference WITHOUT SAEs - Plain Model Accuracy on Test Data\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL INFERENCE WITHOUT SAEs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Use validation set for evaluation\n",
    "test_ds = ds[\"validation\"]\n",
    "# MAX_SAMPLES = len(test_ds)  # Process all samples, or set a limit if needed\n",
    "MAX_SAMPLES = 100\n",
    "MAX_SEQ_LENGTH = 64\n",
    "\n",
    "print(f\"\\nðŸ”¬ Running inference on {MAX_SAMPLES} test samples...\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model: {save_dir}\\n\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(test_ds, desc=\"Processing\")):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        if pred_id == true_label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"âœ… INFERENCE COMPLETE (WITHOUT SAEs)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"   ðŸ“Š Total Samples: {total_predictions}\")\n",
    "print(f\"   âœ“ Correct Predictions: {correct_predictions}\")\n",
    "print(f\"   âœ— Incorrect Predictions: {total_predictions - correct_predictions}\")\n",
    "print(f\"   ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
