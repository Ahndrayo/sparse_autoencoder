{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "deae2746",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "# import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import numpy as np\n",
        "import evaluate\n",
        "# import heapq\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "from typing import List\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "\n",
        "# Add project root to Python path\n",
        "repo_root = Path(\".\").resolve()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "# Import helper utilities from organized modules\n",
        "# import importlib\n",
        "\n",
        "# Remove cached modules to force a fresh import\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod.startswith(\"utils.\") or mod.startswith(\"sparse_autoencoder.\"):\n",
        "        del sys.modules[mod]\n",
        "\n",
        "import utils.finbert\n",
        "import utils.analysis\n",
        "import utils.ablation\n",
        "import utils.run_dirs\n",
        "import sparse_autoencoder.finbert_sae\n",
        "import utils.data_cleaning\n",
        "\n",
        "# Force reload to get latest code\n",
        "# importlib.reload(utils.finbert)\n",
        "# importlib.reload(utils.analysis)\n",
        "# importlib.reload(utils.ablation)\n",
        "# importlib.reload(utils.run_dirs)\n",
        "# importlib.reload(sparse_autoencoder.finbert_sae)\n",
        "\n",
        "from utils.finbert import compute_metrics\n",
        "from utils.analysis import (\n",
        "    FeatureStatsAggregator,\n",
        "    FeatureTopTokenTracker,\n",
        "    HeadlineFeatureAggregator\n",
        ")\n",
        "from utils.ablation import (\n",
        "    create_intervention_hook,\n",
        "    validate_feature_ids,\n",
        "    normalize_decoder_weights,\n",
        "    expand_features_with_similarity,\n",
        "    run_baseline_inference,\n",
        "    run_ablation_inference,\n",
        "    find_flipped_predictions,\n",
        ")\n",
        "from utils.run_dirs import make_analysis_run_dir\n",
        "from sparse_autoencoder.finbert_sae import SparseAutoencoder, load_sae\n",
        "from utils.data_cleaning import clean_text, remove_leading_tickers\n",
        "\n",
        "# --------- CUDA sanity check ----------\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Define device for SAE loading\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31a1b31e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9543/9543 [00:00<00:00, 14425.12 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2388/2388 [00:00<00:00, 18271.23 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9543/9543 [00:00<00:00, 25411.23 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2388/2388 [00:00<00:00, 18051.98 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# 1. Load dataset, comes with train and validation fold \n",
        "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "\n",
        "\n",
        "# Clean dataset\n",
        "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
        "ds = ds.map(lambda x: {\"text\": remove_leading_tickers(x[\"text\"])})\n",
        "\n",
        "# Load dataset\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8034bf6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants/Hyperparameters for training model and SAE\n",
        "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Configuration for Inference\n",
        "MAX_SAMPLES = 100  # Limit for testing\n",
        "TOP_FEATURES = 100  # Top features to track per metric\n",
        "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
        "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
        "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f233625a",
      "metadata": {},
      "source": [
        "Fine Tune Hyperparameters of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f40e9d6",
      "metadata": {},
      "source": [
        "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~4k to 32k interpretable sparse features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c951276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes SAEs based on BERT.\n",
        "# Configuration\n",
        "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
        "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
        "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Create SAE save directory\n",
        "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load dataset\n",
        "# train_ds = ds[\"train\"]\n",
        "\n",
        "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
        "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
        "\n",
        "# Collect training activations\n",
        "all_activations = []\n",
        "captured_activations = []\n",
        "\n",
        "def capture_hook(module, input, output):\n",
        "    if isinstance(output, tuple):\n",
        "        hidden_states = output[0]\n",
        "    else:\n",
        "        hidden_states = output\n",
        "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
        "\n",
        "# Register hook\n",
        "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
        "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
        "\n",
        "# Collect activations from all training data\n",
        "print(\"Extracting activations from training set...\")\n",
        "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(train_ds)):\n",
        "        text = sample[\"text\"]\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        captured_activations.clear()\n",
        "        _ = model(**inputs)\n",
        "        \n",
        "        if captured_activations:\n",
        "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
        "            activation = captured_activations[0].squeeze(0)\n",
        "            \n",
        "            # Get attention mask and token IDs (keep on GPU)\n",
        "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
        "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
        "            \n",
        "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
        "            special_ids = set(tokenizer.all_special_ids)\n",
        "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
        "                                       dtype=torch.bool, device=device)\n",
        "            \n",
        "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
        "\n",
        "            # Print the number of valid tokens\n",
        "            # kept = valid_mask.sum().item()\n",
        "            # total = attention_mask.sum().item()\n",
        "            # print(f\"Kept {kept}/{total} tokens\")\n",
        "\n",
        "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
        "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
        "\n",
        "            # print(\"TOKENS:\", tokens)\n",
        "            # print(\"DROPPED:\", dropped_tokens)\n",
        "            # print(\"KEPT:\", kept_tokens)\n",
        "            \n",
        "            # Only keep activations for real content tokens (still on GPU)\n",
        "            activation = activation[valid_mask]\n",
        "            \n",
        "            # Only add if there are real tokens\n",
        "            if activation.shape[0] > 0:\n",
        "                # Move to CPU only when storing for later processing\n",
        "                all_activations.append(activation.cpu())\n",
        "\n",
        "hook_handle.remove()\n",
        "\n",
        "# Flatten all activations into a single tensor [total_tokens, 768]\n",
        "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
        "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
        "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
        "\n",
        "# Train SAEs for each latent dimension\n",
        "for LATENT_DIM in LATENT_DIMS:\n",
        "    print(f\"\\\\n{'='*80}\")\n",
        "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create SAE\n",
        "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
        "    sae.to(device)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # Create DataLoader\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    dataset = TensorDataset(all_activations_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
        "    sae.train()\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        total_l1_loss = 0\n",
        "        \n",
        "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            reconstruction, latent = sae(batch_x)\n",
        "            \n",
        "            # Reconstruction loss (MSE)\n",
        "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
        "            \n",
        "            # L1 sparsity loss\n",
        "            l1_loss = latent.abs().mean()\n",
        "            \n",
        "            # Combined loss\n",
        "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Renormalize decoder weights (standard SAE practice)\n",
        "            with torch.no_grad():\n",
        "                sae.decoder.weight.data = nn.functional.normalize(\n",
        "                    sae.decoder.weight.data, dim=0\n",
        "                )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_l1_loss += l1_loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_recon = total_recon_loss / len(dataloader)\n",
        "        avg_l1 = total_l1_loss / len(dataloader)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
        "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
        "    \n",
        "    # Save the trained SAE\n",
        "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
        "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
        "    torch.save({\n",
        "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
        "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
        "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
        "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
        "        'config': {\n",
        "            'input_dim': 768,\n",
        "            'latent_dim': LATENT_DIM,\n",
        "            'layer': LAYER_TO_EXTRACT,\n",
        "            'model': save_dir,\n",
        "        }\n",
        "    }, SAE_SAVE_PATH)\n",
        "    \n",
        "    # Test sparsity\n",
        "    sae.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_acts = all_activations_tensor[:1000].to(device)\n",
        "        sample_latent = sae.encode(sample_acts)\n",
        "        sparsity = (sample_latent > 0).float().mean()\n",
        "        print(f\"\\\\n‚úì SAE trained successfully!\")\n",
        "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
        "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"All SAEs trained successfully!\")\n",
        "print(f\"Available SAE models:\")\n",
        "for dim in LATENT_DIMS:\n",
        "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
        "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d70e94b",
      "metadata": {},
      "source": [
        "Finetune FinBERT Model\n",
        "\n",
        "The FinBERT model is trained on the training fold of our dataset to improve its prediction accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0697b2e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell finetunes the FINBERT model.\n",
        "\n",
        "# 2) Load model/tokenizer\n",
        "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# 3) Tokenize\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
        "val_tok = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
        "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
        "\n",
        "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4) Metrics\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# 5) Training config\n",
        "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_twitter_ft\",\n",
        "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
        "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
        "    report_to=\"none\",              # avoids needing wandb, etc.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
        "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f30b519",
      "metadata": {},
      "source": [
        "Inference with Interpretability\n",
        "\n",
        "We use our FinBERT + SAE on test data. We extract a Layer Activations with Sentiment Predictions (SAE-style Analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64315a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ABLATION EXPERIMENT\n",
            "============================================================\n",
            "Ablation Mode: manual\n",
            "Similarity Expansion: True\n",
            "Similarity Top-M: 64\n",
            "‚úì Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
            "  Layer: 8\n",
            "  Input dim: 768\n",
            "  Latent dim: 32768\n",
            "Ablation mode: manual\n",
            "Layer: 8\n",
            "SAE Size: 32k (32768 features)\n",
            "Max Samples: 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Feature Ablation Experiment\n",
        "# This cell performs ablation by zeroing out specified SAE features and comparing predictions\n",
        "\n",
        "# Configuration\n",
        "# ========== ABLATION CONFIGURATION ==========\n",
        "ABLATION_CONFIG = {\n",
        "    \n",
        "    \"mode\": \"manual\",                 # Mode 1, Options: \"manual\" | \"per_sample_top_k\" | \"union_top_k\"\n",
        "    #\"mode\": \"per_sample_top_k\",       # Mode 2\n",
        "    #\"mode\": \"union_top_k\",             # Mode 3\n",
        "    \"k\": 10,                           # Only for per_sample_top_k and union_top_k modes\n",
        "    \"skip_sae_reconstruction\": False,  # If True, skip SAE hook entirely (true baseline)\n",
        "    \"similarity_expansion\": True,      # If True, include top m similar features to selected features\n",
        "    \"similarity_top_m\": 64            # Total features per seed feature (includes original)\n",
        "}\n",
        "\n",
        "# Mode 1: Manual features (only used if mode == \"manual\")\n",
        "#MANUAL_FEATURES = [4456, 21508, 21969, 27518, 21110, 24583, 32601, 15959, 27518, 29555, 3993, 13142, 22354, 21858]\n",
        "# MANUAL_FEATURES = [21110, 24583, 21508, 32601, 15959, 27518, 29555, 3993, 13142, 22354] # row 0 true top 10\n",
        "MANUAL_FEATURES = [21508, 27518]\n",
        "#MANUAL_FEATURES = []\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "if ABLATION_CONFIG['mode'] != 'manual':\n",
        "    print(f\"K value: {ABLATION_CONFIG['k']}\")\n",
        "print(f\"Similarity Expansion: {ABLATION_CONFIG['similarity_expansion']}\")\n",
        "if ABLATION_CONFIG['similarity_expansion']:\n",
        "    print(f\"Similarity Top-M: {ABLATION_CONFIG['similarity_top_m']}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# Define device and move model to it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu only, please install CUDA-compatible Torch\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the SAE using the helper function\n",
        "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
        "\n",
        "# Extract dimensions from the loaded config\n",
        "SAE_INPUT_DIM = sae_config['input_dim']\n",
        "SAE_LATENT_DIM = sae_config['latent_dim']\n",
        "\n",
        "print(f\"Ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "print(f\"Layer: {LAYER_TO_EXTRACT}\")\n",
        "print(f\"SAE Size: {SAE_SIZE} ({SAE_LATENT_DIM} features)\")\n",
        "print(f\"Max Samples: {MAX_SAMPLES}\\n\")\n",
        "\n",
        "# Storage for results\n",
        "baseline_predictions = []\n",
        "ablated_predictions = []\n",
        "sample_data = []\n",
        "\n",
        "# Initialize trackers for SAE features (same as inference cell)\n",
        "feature_stats_ablated = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
        "top_token_tracker_ablated = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
        "headline_aggregator_ablated = HeadlineFeatureAggregator(top_k=10)\n",
        "all_prompt_metadata_ablated = []\n",
        "\n",
        "# Storage for capturing SAE features during ablation (for tracking)\n",
        "current_sample_data = {\"sae_features\": None, \"token_ids\": None, \"prompt_tokens\": None, \"text\": None, \"idx\": None}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cac4bc40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Running baseline inference (no ablation)...\n",
            "  Baseline: 20/100 samples\n",
            "  Baseline: 40/100 samples\n",
            "  Baseline: 60/100 samples\n",
            "  Baseline: 80/100 samples\n",
            "  Baseline: 100/100 samples\n",
            "‚úì Baseline accuracy: 87.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Baseline Inference\n",
        "print(\"üî¨ Running baseline inference (no ablation)...\")\n",
        "baseline_results, baseline_features_map, baseline_accuracy = run_baseline_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_ds=test_ds,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_samples=MAX_SAMPLES,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "print(f\"‚úì Baseline accuracy: {baseline_accuracy:.2%}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "760bf70f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity expansion: enabled (top_m=64)\n",
            "\n",
            "Mode 1 (Manual): Ablating 64 manually specified features\n",
            "  Similarity expansion: 1 ‚Üí 64 (top_m=64)\n",
            "Features to ablate: [534, 575, 769, 825, 1050, 1677, 3171, 3245, 3304, 4248, 4635, 5247, 5342, 5514, 5975, 6241, 7973, 9071, 9341, 10152, 10577, 10898, 11563, 11581, 12349, 12651, 12695, 13055, 13721, 13724, 13889, 14779, 15040, 15562, 17753, 19060, 19788, 20026, 20247, 20361, 20927, 20997, 21187, 21508, 22080, 22178, 22695, 22861, 23155, 23206, 23208, 24120, 24190, 25080, 25792, 26172, 27420, 27616, 27664, 27865, 28037, 28876, 30928, 32228]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Feature Selection & Expansion\n",
        "skip_hooks = ABLATION_CONFIG.get(\"skip_sae_reconstruction\", False)\n",
        "\n",
        "similarity_enabled = ABLATION_CONFIG.get(\"similarity_expansion\", False)\n",
        "similarity_top_m = int(ABLATION_CONFIG.get(\"similarity_top_m\", 10))\n",
        "if similarity_top_m < 1:\n",
        "    raise ValueError(\"similarity_top_m must be >= 1\")\n",
        "\n",
        "normalized_decoder = None\n",
        "similarity_cache = {}\n",
        "if similarity_enabled and not skip_hooks:\n",
        "    normalized_decoder = normalize_decoder_weights(sae, device)\n",
        "    print(f\"Similarity expansion: enabled (top_m={similarity_top_m})\")\n",
        "elif similarity_enabled and skip_hooks:\n",
        "    print(\"Similarity expansion requested, but skip_sae_reconstruction=True; expansion disabled.\")\n",
        "    similarity_enabled = False\n",
        "\n",
        "if skip_hooks:\n",
        "    print(\"\\n‚ÑπÔ∏è Skipping SAE reconstruction (true baseline mode)\")\n",
        "    print(\"   Predictions will match baseline exactly\\n\")\n",
        "    FEATURES_TO_ABLATE = None\n",
        "    ORIGINAL_SEED_FEATURES = None\n",
        "else:\n",
        "    ORIGINAL_SEED_FEATURES = None\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\":\n",
        "        FEATURES_TO_ABLATE = MANUAL_FEATURES\n",
        "        ORIGINAL_SEED_FEATURES = list(MANUAL_FEATURES)\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"manual features\")\n",
        "        original_feature_count = len(FEATURES_TO_ABLATE)\n",
        "        if similarity_enabled:\n",
        "            FEATURES_TO_ABLATE = expand_features_with_similarity(\n",
        "                FEATURES_TO_ABLATE, normalized_decoder, similarity_top_m, similarity_cache\n",
        "            )\n",
        "        print(f\"\\nMode 1 (Manual): Ablating {len(FEATURES_TO_ABLATE)} manually specified features\")\n",
        "        if similarity_enabled:\n",
        "            print(f\"  Similarity expansion: {original_feature_count} ‚Üí {len(FEATURES_TO_ABLATE)} (top_m={similarity_top_m})\")\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"union_top_k\":\n",
        "        feature_set = set()\n",
        "        for idx in baseline_features_map:\n",
        "            top_k_ids = [f[\"feature_id\"] for f in baseline_features_map[idx][\"top_features\"][:ABLATION_CONFIG[\"k\"]]]\n",
        "            feature_set.update(top_k_ids)\n",
        "        FEATURES_TO_ABLATE = sorted(list(feature_set))\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"union_top_k features\")\n",
        "        original_feature_count = len(FEATURES_TO_ABLATE)\n",
        "        if similarity_enabled:\n",
        "            FEATURES_TO_ABLATE = expand_features_with_similarity(\n",
        "                FEATURES_TO_ABLATE, normalized_decoder, similarity_top_m, similarity_cache\n",
        "            )\n",
        "        print(f\"\\nMode 3 (Union Top-K): Collected {len(FEATURES_TO_ABLATE)} unique features from union of top-{ABLATION_CONFIG['k']} across {len(baseline_results)} samples\")\n",
        "        if similarity_enabled:\n",
        "            print(f\"  Similarity expansion: {original_feature_count} ‚Üí {len(FEATURES_TO_ABLATE)} (top_m={similarity_top_m})\")\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "        FEATURES_TO_ABLATE = None\n",
        "        print(f\"\\nMode 2 (Per-Sample Top-K): Will ablate top-{ABLATION_CONFIG['k']} features individually for each sample\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown ablation mode: {ABLATION_CONFIG['mode']}\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] != \"per_sample_top_k\":\n",
        "        validate_feature_ids(FEATURES_TO_ABLATE, SAE_LATENT_DIM, \"global ablation features\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\":\n",
        "        print(f\"Features to ablate: {FEATURES_TO_ABLATE if FEATURES_TO_ABLATE is not None else 'Per-sample dynamic'}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a1d3f4f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Running ablation inference (features zeroed)...\n",
            "  Ablated: 20/100 samples\n",
            "  Ablated: 40/100 samples\n",
            "  Ablated: 60/100 samples\n",
            "  Ablated: 80/100 samples\n",
            "  Ablated: 100/100 samples\n",
            "‚úì Ablated accuracy: 88.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ablation Inference\n",
        "print(\"üî¨ Running ablation inference (features zeroed)...\")\n",
        "(\n",
        "    ablated_results,\n",
        "    all_prompt_metadata_ablated,\n",
        "    similarity_stats,\n",
        "    all_ablated_features_set,\n",
        "    ablated_accuracy,\n",
        ") = run_ablation_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_ds=test_ds,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_samples=MAX_SAMPLES,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ablation_config=ABLATION_CONFIG,\n",
        "    features_to_ablate=FEATURES_TO_ABLATE,\n",
        "    baseline_results=baseline_results,\n",
        "    baseline_features_map=baseline_features_map,\n",
        "    feature_stats_ablated=feature_stats_ablated,\n",
        "    top_token_tracker_ablated=top_token_tracker_ablated,\n",
        "    headline_aggregator_ablated=headline_aggregator_ablated,\n",
        "    current_sample_data=current_sample_data,\n",
        "    similarity_enabled=similarity_enabled,\n",
        "    normalized_decoder=normalized_decoder,\n",
        "    similarity_top_m=similarity_top_m,\n",
        "    similarity_cache=similarity_cache,\n",
        ")\n",
        "print(f\"‚úì Ablated accuracy: {ablated_accuracy:.2%}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "57981356",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ABLATION RESULTS\n",
            "============================================================\n",
            "Ablation Mode: manual\n",
            "Seed Features: [21508]\n",
            "Total Features Ablated: 64\n",
            "  (Expanded from 1 seeds)\n",
            "Baseline Accuracy: 87.00%\n",
            "Ablated Accuracy: 88.00%\n",
            "Accuracy Change: 1.00%\n",
            "\n",
            "Flipped Predictions: 3/100 samples\n",
            "Flip Rate: 3.00%\n",
            "\n",
            "============================================================\n",
            "FLIPPED PREDICTIONS (showing first 3):\n",
            "============================================================\n",
            "\n",
            "--- Sample #5 ---\n",
            "Text: Barclays cools on Molson Coors\n",
            "True Label: Bearish\n",
            "Original: Bearish (conf: 0.648) ‚Üí Ablated: Neutral (conf: 0.450)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 8.8932\n",
            "  Feature 32601: 6.1672\n",
            "  Feature 15991: 4.6884\n",
            "  Feature 21508: 4.4652 [ABLATED]\n",
            "  Feature 29952: 4.4536\n",
            "  Feature 5111: 4.2631\n",
            "  Feature 28660: 4.2182\n",
            "  Feature 7927: 4.2108\n",
            "  Feature 687: 4.1628\n",
            "  Feature 27757: 4.0974\n",
            "\n",
            "\n",
            "--- Sample #38 ---\n",
            "Text: Alliance Global Partners starts at Buy\n",
            "True Label: Bullish\n",
            "Original: Neutral (conf: 0.528) ‚Üí Ablated: Bullish (conf: 0.507)\n",
            "Top 10 SAE Features:\n",
            "  Feature 4456: 9.0261\n",
            "  Feature 21110: 6.6836\n",
            "  Feature 25797: 6.2264\n",
            "  Feature 24583: 4.7995\n",
            "  Feature 18425: 4.3438\n",
            "  Feature 687: 4.2223\n",
            "  Feature 9847: 3.8847\n",
            "  Feature 32411: 3.8798\n",
            "  Feature 18317: 3.8705\n",
            "  Feature 13142: 3.8668\n",
            "\n",
            "\n",
            "--- Sample #77 ---\n",
            "Text: Snap Analyst Projects 37% Revenue Growth In 2020\n",
            "True Label: Bullish\n",
            "Original: Bearish (conf: 0.677) ‚Üí Ablated: Bullish (conf: 0.629)\n",
            "Top 10 SAE Features:\n",
            "  Feature 21110: 7.8783\n",
            "  Feature 4456: 7.1909\n",
            "  Feature 25797: 5.5498\n",
            "  Feature 18317: 5.2954\n",
            "  Feature 25518: 5.1933\n",
            "  Feature 32601: 5.1514\n",
            "  Feature 13142: 5.0637\n",
            "  Feature 24583: 5.0035\n",
            "  Feature 6026: 4.9927\n",
            "  Feature 26260: 4.9678\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Results Analysis\n",
        "flipped_samples = find_flipped_predictions(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    sae=sae,\n",
        "    layer_to_extract=LAYER_TO_EXTRACT,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    baseline_results=baseline_results,\n",
        "    ablated_results=ablated_results,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ABLATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if skip_hooks is False:\n",
        "    print(f\"Ablation Mode: {ABLATION_CONFIG['mode']}\")\n",
        "\n",
        "    if ABLATION_CONFIG[\"mode\"] == \"manual\" and ORIGINAL_SEED_FEATURES is not None:\n",
        "        print(f\"Seed Features: {ORIGINAL_SEED_FEATURES}\")\n",
        "        print(f\"Total Features Ablated: {len(FEATURES_TO_ABLATE)}\")\n",
        "        if similarity_enabled and len(FEATURES_TO_ABLATE) != len(ORIGINAL_SEED_FEATURES):\n",
        "            print(f\"  (Expanded from {len(ORIGINAL_SEED_FEATURES)} seeds)\")\n",
        "\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"union_top_k\" and FEATURES_TO_ABLATE is not None:\n",
        "        print(f\"Total Features Ablated: {len(FEATURES_TO_ABLATE)}\")\n",
        "        if similarity_enabled:\n",
        "            print(\"  (After similarity expansion)\")\n",
        "\n",
        "    elif ABLATION_CONFIG[\"mode\"] == \"per_sample_top_k\":\n",
        "        print(f\"Per-Sample: top-{ABLATION_CONFIG['k']} features\")\n",
        "        if len(all_ablated_features_set) > 0:\n",
        "            print(f\"Unique Features Ablated: {len(all_ablated_features_set)} (across {len(ablated_results)} samples)\")\n",
        "            if similarity_stats[\"expanded_counts\"]:\n",
        "                avg_per_sample = float(np.mean(similarity_stats[\"expanded_counts\"]))\n",
        "                print(f\"Per-Sample Average: {avg_per_sample:.1f} features\")\n",
        "            else:\n",
        "                print(f\"Per-Sample Average: {ABLATION_CONFIG['k']} features\")\n",
        "\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.2%}\")\n",
        "print(f\"Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"Accuracy Change: {(ablated_accuracy - baseline_accuracy):.2%}\")\n",
        "print(f\"\\nFlipped Predictions: {len(flipped_samples)}/{len(baseline_results)} samples\")\n",
        "print(f\"Flip Rate: {len(flipped_samples)/len(baseline_results):.2%}\\n\")\n",
        "\n",
        "if flipped_samples:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"FLIPPED PREDICTIONS (showing first {min(10, len(flipped_samples))}):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, flip in enumerate(flipped_samples[:10], 1):\n",
        "        print(f\"\\n--- Sample #{flip['sample_idx']} ---\")\n",
        "        print(f\"Text: {flip['text'][:120]}{'...' if len(flip['text']) > 120 else ''}\")\n",
        "        print(f\"True Label: {flip['true_label']}\")\n",
        "        print(f\"Original: {flip['baseline_pred']} (conf: {flip['baseline_conf']:.3f}) ‚Üí \"\n",
        "              f\"Ablated: {flip['ablated_pred']} (conf: {flip['ablated_conf']:.3f})\")\n",
        "\n",
        "        if flip['top_features']:\n",
        "            print(\"Top 10 SAE Features:\")\n",
        "            for feat in flip['top_features']:\n",
        "                ablated_marker = \" [ABLATED]\" if feat['ablated'] else \"\"\n",
        "                print(f\"  Feature {feat['feature_id']}: {feat['activation']:.4f}{ablated_marker}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No predictions were flipped by ablating these features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "06e4df4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Saving ablated results for visualization...\n",
            "üíæ Saving ablated results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-02-02T20-30-43_run-089\n",
            "\n",
            "‚úÖ Ablation experiment complete!\n",
            "   üìÅ Ablated results saved to: 2026-02-02T20-30-43_run-089\n",
            "   üéØ Ablated Accuracy: 88.00%\n",
            "   üî¢ Total tokens: 1542\n",
            "   ‚ú® SAE features: 32768\n",
            "\n",
            "üåê Start the viewer to see ablated results:\n",
            "   python viz_analysis/feature_probe_server.py\n",
            "   cd sae-viewer && npm start\n"
          ]
        }
      ],
      "source": [
        "# Save Results\n",
        "print(\"\\nüíæ Saving ablated results for visualization...\")\n",
        "\n",
        "ablated_run_dir = make_analysis_run_dir(str(repo_root))\n",
        "print(f\"üíæ Saving ablated results to: {ablated_run_dir}\")\n",
        "\n",
        "stats_ablated = feature_stats_ablated.get_stats()\n",
        "\n",
        "top_features_by_metric_ablated = {}\n",
        "for metric_name, values in stats_ablated.items():\n",
        "    if metric_name == \"mean_act_squared\":\n",
        "        continue\n",
        "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
        "    top_features_by_metric_ablated[metric_name] = [\n",
        "        {\n",
        "            \"feature_id\": int(idx),\n",
        "            \"value\": float(values[idx]),\n",
        "            \"metrics\": {\n",
        "                \"mean_activation\": float(stats_ablated[\"mean_activation\"][idx]),\n",
        "                \"max_activation\": float(stats_ablated[\"max_activation\"][idx]),\n",
        "                \"fraction_active\": float(stats_ablated[\"fraction_active\"][idx]),\n",
        "            },\n",
        "        }\n",
        "        for idx in top_indices\n",
        "    ]\n",
        "\n",
        "prompts_file = ablated_run_dir / \"prompts.jsonl\"\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in all_prompt_metadata_ablated:\n",
        "        json.dump(meta, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "feature_stats_file = ablated_run_dir / \"feature_stats.json\"\n",
        "feature_stats_data = {\n",
        "    \"num_features\": SAE_LATENT_DIM,\n",
        "    \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "    \"top_feature_count\": TOP_FEATURES,\n",
        "    \"accuracy\": ablated_accuracy,\n",
        "    \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "    \"mean_act_squared\": stats_ablated[\"mean_act_squared\"].tolist(),\n",
        "    \"metrics\": {\n",
        "        metric_name: {\n",
        "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
        "            \"top_features\": top_features_by_metric_ablated[metric_name],\n",
        "        }\n",
        "        for metric_name in stats_ablated.keys() if metric_name != \"mean_act_squared\"\n",
        "    },\n",
        "}\n",
        "with open(feature_stats_file, \"w\") as f:\n",
        "    json.dump(feature_stats_data, f, indent=2)\n",
        "\n",
        "feature_tokens_file = ablated_run_dir / \"feature_tokens.json\"\n",
        "feature_tokens_data = {\"features\": top_token_tracker_ablated.export()}\n",
        "with open(feature_tokens_file, \"w\") as f:\n",
        "    json.dump(feature_tokens_data, f, indent=2)\n",
        "\n",
        "headline_features_file = ablated_run_dir / \"headline_features.json\"\n",
        "with open(headline_features_file, \"w\") as f:\n",
        "    json.dump(headline_aggregator_ablated.export(), f, indent=2)\n",
        "\n",
        "metadata_file = ablated_run_dir / \"metadata.json\"\n",
        "with open(metadata_file, \"w\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"model\": save_dir,\n",
        "            \"layer_extracted\": LAYER_TO_EXTRACT,\n",
        "            \"num_samples\": len(all_prompt_metadata_ablated),\n",
        "            \"total_tokens\": feature_stats_ablated.total_tokens,\n",
        "            \"accuracy\": ablated_accuracy,\n",
        "            \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
        "            \"split\": \"validation\",\n",
        "            \"hidden_dim\": SAE_INPUT_DIM,\n",
        "            \"latent_dim\": SAE_LATENT_DIM,\n",
        "            \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
        "            \"top_features_per_metric\": TOP_FEATURES,\n",
        "            \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
        "            \"ablation_mode\": ABLATION_CONFIG[\"mode\"],\n",
        "            \"ablated_features\": FEATURES_TO_ABLATE if FEATURES_TO_ABLATE is not None else \"per_sample_dynamic\",\n",
        "            \"ablation_k\": ABLATION_CONFIG.get(\"k\"),\n",
        "            \"skip_sae_reconstruction\": ABLATION_CONFIG.get(\"skip_sae_reconstruction\", False),\n",
        "            \"similarity_expansion\": {\"enabled\": similarity_enabled, \"top_m\": similarity_top_m},\n",
        "            \"note\": f\"SAE sparse features with predictions (mode: {ABLATION_CONFIG['mode']})\",\n",
        "        },\n",
        "        f,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(\"\\n‚úÖ Ablation experiment complete!\")\n",
        "print(f\"   üìÅ Ablated results saved to: {ablated_run_dir.name}\")\n",
        "print(f\"   üéØ Ablated Accuracy: {ablated_accuracy:.2%}\")\n",
        "print(f\"   üî¢ Total tokens: {feature_stats_ablated.total_tokens}\")\n",
        "print(f\"   ‚ú® SAE features: {SAE_LATENT_DIM}\")\n",
        "print(\"\\nüåê Start the viewer to see ablated results:\")\n",
        "print(\"   python viz_analysis/feature_probe_server.py\")\n",
        "print(\"   cd sae-viewer && npm start\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def14c72",
      "metadata": {},
      "source": [
        "Testing Inference based on Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de85884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick analysis on simple headlines\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "\n",
        "example_sentences = [\n",
        "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
        "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
        "    \"The company reported results largely in line with analyst expectations.\",\n",
        "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
        "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
        "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
        "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
        "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
        "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
        "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
        "]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# optional: move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def predict_sentiment(text: str):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs)\n",
        "    pred_id = out.logits.argmax(dim=-1).item()\n",
        "    return model.config.id2label[pred_id]\n",
        "\n",
        "for text in example_sentences:\n",
        "    label = predict_sentiment(text)\n",
        "    print(f\"{label.upper():8} | {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f0b474",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Visualisation for Dataset\n",
        "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
        "\n",
        "test_ds[\"text\"][0:200]\n",
        "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "#ds2[\"validation\"][\"text\"][34]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce67a413",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference WITHOUT SAEs - Plain Model Accuracy on Test Data\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL INFERENCE WITHOUT SAEs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "save_dir = \"./finbert_twitter_ft/best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Use validation set for evaluation\n",
        "test_ds = ds[\"validation\"]\n",
        "# MAX_SAMPLES = len(test_ds)  # Process all samples, or set a limit if needed\n",
        "MAX_SAMPLES = 100\n",
        "MAX_SEQ_LENGTH = 64\n",
        "\n",
        "print(f\"\\nüî¨ Running inference on {MAX_SAMPLES} test samples...\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Model: {save_dir}\\n\")\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Process samples\n",
        "with torch.no_grad():\n",
        "    for idx, sample in enumerate(tqdm(test_ds, desc=\"Processing\")):\n",
        "        if idx >= MAX_SAMPLES:\n",
        "            break\n",
        "        \n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"label\"]\n",
        "        \n",
        "        # Tokenize with truncation\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
        "        \n",
        "        # Check if prediction is correct\n",
        "        if pred_id == true_label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"‚úÖ INFERENCE COMPLETE (WITHOUT SAEs)\")\n",
        "print(f\"{'=' * 60}\")\n",
        "print(f\"   üìä Total Samples: {total_predictions}\")\n",
        "print(f\"   ‚úì Correct Predictions: {correct_predictions}\")\n",
        "print(f\"   ‚úó Incorrect Predictions: {total_predictions - correct_predictions}\")\n",
        "print(f\"   üéØ Model Accuracy: {accuracy:.2%}\")\n",
        "print(f\"{'=' * 60}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
