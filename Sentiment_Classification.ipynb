{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a1b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import re\n",
    "\n",
    "# 1. Load dataset, comes with train and validation fold \n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_leading_tickers(text):\n",
    "    return re.sub(\n",
    "        r'^\\s*(?:\\$[A-Z]{1,6}\\s*)+(?:[-:]\\s*)?',\n",
    "        '',\n",
    "        text\n",
    "    )\n",
    "\n",
    "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "ds = ds.map(lambda x: {\"text\": remove_leading_tickers(x[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deae2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233625a",
   "metadata": {},
   "source": [
    "Fine Tune Hyperparamters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40e9d6",
   "metadata": {},
   "source": [
    "Train Sparse Autoencoder on FinBERT Activations\n",
    "\n",
    "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~32k interpretable sparse features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c951276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from 9543 training samples...\n",
      "Target layer: 8\n",
      "Will train SAEs with latent dimensions: [4096, 8192, 16384, 32768]\n",
      "Extracting activations from training set...\n",
      "Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9543/9543 [02:44<00:00, 57.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCollected 168708 token activations\n",
      "Activation shape: torch.Size([168708, 768])\n",
      "\\n================================================================================\n",
      "Training SAE with 4096 latent features (4k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0400, Recon=0.0397, L1=0.2554\n",
      "Epoch 2/3: Loss=0.0202, Recon=0.0199, L1=0.2603\n",
      "Epoch 3/3: Loss=0.0191, Recon=0.0189, L1=0.2549\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_4k.pt\n",
      "\\nâœ“ SAE trained successfully!\n",
      "  Average sparsity: 30.67% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_4k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 8192 latent features (8k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0447, Recon=0.0445, L1=0.1270\n",
      "Epoch 2/3: Loss=0.0213, Recon=0.0211, L1=0.1287\n",
      "Epoch 3/3: Loss=0.0199, Recon=0.0198, L1=0.1260\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_8k.pt\n",
      "\\nâœ“ SAE trained successfully!\n",
      "  Average sparsity: 15.38% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_8k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 16384 latent features (16k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0537, Recon=0.0536, L1=0.0646\n",
      "Epoch 2/3: Loss=0.0205, Recon=0.0204, L1=0.0653\n",
      "Epoch 3/3: Loss=0.0196, Recon=0.0195, L1=0.0635\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_16k.pt\n",
      "\\nâœ“ SAE trained successfully!\n",
      "  Average sparsity: 7.59% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_16k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 32768 latent features (32k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0816, Recon=0.0816, L1=0.0334\n",
      "Epoch 2/3: Loss=0.0221, Recon=0.0221, L1=0.0323\n",
      "Epoch 3/3: Loss=0.0208, Recon=0.0207, L1=0.0314\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_32k.pt\n",
      "\\nâœ“ SAE trained successfully!\n",
      "  Average sparsity: 3.81% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_32k.pt\n",
      "\\n================================================================================\n",
      "All SAEs trained successfully!\n",
      "Available SAE models:\n",
      "  - layer_8_4k.pt (4096 features)\n",
      "  - layer_8_8k.pt (8192 features)\n",
      "  - layer_8_16k.pt (16384 features)\n",
      "  - layer_8_32k.pt (32768 features)\n",
      "\\nThese SAEs can now be used in main.py for interpretability analysis!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# This cell finetunes SAEs based on BERT.\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
    "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
    "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Create SAE save directory\n",
    "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
    "\n",
    "# Define Sparse Autoencoder (compatible with OpenAI's architecture)\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, latent_dim=32768):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: input -> latent\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        \n",
    "        # Decoder: latent -> reconstruction\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim, bias=True)\n",
    "        \n",
    "        # Initialize decoder with unit norm columns (standard for SAEs)\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.data = nn.functional.normalize(\n",
    "                self.decoder.weight.data, dim=0\n",
    "            )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode to sparse latent representation\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        latent = nn.functional.relu(latent)  # ReLU for sparsity\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        \"\"\"Decode from latent representation\"\"\"\n",
    "        return self.decoder(latent)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        reconstruction = self.decode(latent)\n",
    "        return reconstruction, latent\n",
    "    \n",
    "    def get_feature_activations(self, x):\n",
    "        \"\"\"Get sparse feature activations (for analysis)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.encode(x)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "train_ds = ds[\"train\"]\n",
    "\n",
    "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
    "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
    "\n",
    "# Collect training activations\n",
    "all_activations = []\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "# Collect activations from all training data\n",
    "print(\"Extracting activations from training set...\")\n",
    "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(train_ds)):\n",
    "        text = sample[\"text\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        captured_activations.clear()\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "        if captured_activations:\n",
    "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
    "            activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Get attention mask and token IDs (keep on GPU)\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "\n",
    "            # Print the number of valid tokens\n",
    "            # kept = valid_mask.sum().item()\n",
    "            # total = attention_mask.sum().item()\n",
    "            # print(f\"Kept {kept}/{total} tokens\")\n",
    "\n",
    "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
    "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
    "\n",
    "            # print(\"TOKENS:\", tokens)\n",
    "            # print(\"DROPPED:\", dropped_tokens)\n",
    "            # print(\"KEPT:\", kept_tokens)\n",
    "            \n",
    "            # Only keep activations for real content tokens (still on GPU)\n",
    "            activation = activation[valid_mask]\n",
    "            \n",
    "            # Only add if there are real tokens\n",
    "            if activation.shape[0] > 0:\n",
    "                # Move to CPU only when storing for later processing\n",
    "                all_activations.append(activation.cpu())\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "# Flatten all activations into a single tensor [total_tokens, 768]\n",
    "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
    "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
    "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
    "\n",
    "# Train SAEs for each latent dimension\n",
    "for LATENT_DIM in LATENT_DIMS:\n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create SAE\n",
    "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
    "    sae.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    dataset = TensorDataset(all_activations_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
    "    sae.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_l1_loss = 0\n",
    "        \n",
    "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstruction, latent = sae(batch_x)\n",
    "            \n",
    "            # Reconstruction loss (MSE)\n",
    "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
    "            \n",
    "            # L1 sparsity loss\n",
    "            l1_loss = latent.abs().mean()\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Renormalize decoder weights (standard SAE practice)\n",
    "            with torch.no_grad():\n",
    "                sae.decoder.weight.data = nn.functional.normalize(\n",
    "                    sae.decoder.weight.data, dim=0\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_l1_loss += l1_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_recon = total_recon_loss / len(dataloader)\n",
    "        avg_l1 = total_l1_loss / len(dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
    "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
    "    \n",
    "    # Save the trained SAE\n",
    "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
    "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
    "    torch.save({\n",
    "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
    "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
    "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
    "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
    "        'config': {\n",
    "            'input_dim': 768,\n",
    "            'latent_dim': LATENT_DIM,\n",
    "            'layer': LAYER_TO_EXTRACT,\n",
    "            'model': save_dir,\n",
    "        }\n",
    "    }, SAE_SAVE_PATH)\n",
    "    \n",
    "    # Test sparsity\n",
    "    sae.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_acts = all_activations_tensor[:1000].to(device)\n",
    "        sample_latent = sae.encode(sample_acts)\n",
    "        sparsity = (sample_latent > 0).float().mean()\n",
    "        print(f\"\\\\nâœ“ SAE trained successfully!\")\n",
    "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
    "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"All SAEs trained successfully!\")\n",
    "print(f\"Available SAE models:\")\n",
    "for dim in LATENT_DIMS:\n",
    "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
    "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70e94b",
   "metadata": {},
   "source": [
    "Load Trained SAE for Inference\n",
    "\n",
    "Use this cell to load a specific SAE model based on the latent dimension you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3eb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sparse Autoencoder (compatible with OpenAI's architecture)\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, latent_dim=32768):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: input -> latent\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        \n",
    "        # Decoder: latent -> reconstruction\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim, bias=True)\n",
    "        \n",
    "        # Initialize decoder with unit norm columns (standard for SAEs)\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.data = nn.functional.normalize(\n",
    "                self.decoder.weight.data, dim=0\n",
    "            )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode to sparse latent representation\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        latent = nn.functional.relu(latent)  # ReLU for sparsity\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        \"\"\"Decode from latent representation\"\"\"\n",
    "        return self.decoder(latent)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        reconstruction = self.decode(latent)\n",
    "        return reconstruction, latent\n",
    "    \n",
    "    def get_feature_activations(self, x):\n",
    "        \"\"\"Get sparse feature activations (for analysis)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.encode(x)\n",
    "\n",
    "# Helper function to load a trained SAE\n",
    "def load_sae(layer=8, latent_size=\"32k\"):\n",
    "    \"\"\"\n",
    "    Load a trained SAE model.\n",
    "    \n",
    "    Args:\n",
    "        layer: The layer number (default: 8)\n",
    "        latent_size: Size of latent dimension as string: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "    \n",
    "    Returns:\n",
    "        sae: The loaded SAE model\n",
    "        config: Configuration dictionary\n",
    "    \"\"\"\n",
    "    sae_path = f\"./finbert_sae/layer_{layer}_{latent_size}.pt\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(sae_path, map_location=device)\n",
    "    \n",
    "    # Create SAE model\n",
    "    config = checkpoint['config']\n",
    "    sae = SparseAutoencoder(input_dim=config['input_dim'], latent_dim=config['latent_dim'])\n",
    "    \n",
    "    # Load weights\n",
    "    sae.encoder.weight.data = checkpoint['encoder_weight']\n",
    "    sae.encoder.bias.data = checkpoint['encoder_bias']\n",
    "    sae.decoder.weight.data = checkpoint['decoder_weight']\n",
    "    sae.decoder.bias.data = checkpoint['decoder_bias']\n",
    "    \n",
    "    sae.to(device)\n",
    "    sae.eval()\n",
    "    \n",
    "    print(f\"âœ“ Loaded SAE from {sae_path}\")\n",
    "    print(f\"  Layer: {config['layer']}\")\n",
    "    print(f\"  Input dim: {config['input_dim']}\")\n",
    "    print(f\"  Latent dim: {config['latent_dim']}\")\n",
    "    \n",
    "    return sae, config\n",
    "\n",
    "# Example usage:\n",
    "# Load the 32k latent dimension SAE\n",
    "# sae_32k, config = load_sae(layer=8, latent_size=\"32k\")\n",
    "\n",
    "# Load the 16k latent dimension SAE\n",
    "# sae_16k, config = load_sae(layer=8, latent_size=\"16k\")\n",
    "\n",
    "# Load the 8k latent dimension SAE\n",
    "# sae_8k, config = load_sae(layer=8, latent_size=\"8k\")\n",
    "\n",
    "# Load the 4k latent dimension SAE\n",
    "# sae_4k, config = load_sae(layer=8, latent_size=\"4k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0697b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_42420\\4216466311.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1791' max='1791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1791/1791 05:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.470456</td>\n",
       "      <td>0.822446</td>\n",
       "      <td>0.764702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.449918</td>\n",
       "      <td>0.840034</td>\n",
       "      <td>0.779471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0.532123</td>\n",
       "      <td>0.832077</td>\n",
       "      <td>0.771107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finbert_twitter_ft/best\\\\tokenizer_config.json',\n",
       " './finbert_twitter_ft/best\\\\special_tokens_map.json',\n",
       " './finbert_twitter_ft/best\\\\vocab.txt',\n",
       " './finbert_twitter_ft/best\\\\added_tokens.json',\n",
       " './finbert_twitter_ft/best\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell finetunes the FINBERT model.\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --------- CUDA sanity check ----------\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Load dataset\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "# 2) Load model/tokenizer\n",
    "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# 3) Tokenize\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4) Metrics\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# 5) Training config\n",
    "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finbert_twitter_ft\",\n",
    "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
    "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
    "    report_to=\"none\",              # avoids needing wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
    "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30b519",
   "metadata": {},
   "source": [
    "Extract Layer Activations with Sentiment Predictions (SAE-style Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eefe6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING SAE FEATURES FROM FINBERT\n",
      "============================================================\n",
      "âœ“ Loaded SAE from ./finbert_sae/layer_8_32k.pt\n",
      "  Layer: 8\n",
      "  Input dim: 768\n",
      "  Latent dim: 32768\n",
      "âœ“ SAE loaded: 768 dims â†’ 32768 sparse features\n",
      "\n",
      "ðŸ’¾ Saving results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-01-17T14-13-16_run-044\n",
      "\n",
      "ðŸ”¬ Processing 100 samples...\n",
      "   Layer: 8\n",
      "   Using SAE: 32768 sparse features\n",
      "   Filtering: ALL special tokens excluded (content only)\n",
      "\n",
      "Processed 10/100 samples\n",
      "Processed 20/100 samples\n",
      "Processed 30/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 50/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 70/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 90/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "ðŸ“Š Computing feature statistics...\n",
      "ðŸŽ¯ Model Accuracy: 87.00%\n",
      "\n",
      "ðŸ’¾ Saving results...\n",
      "\n",
      "âœ… COMPLETE!\n",
      "   ðŸ“ Results saved to: 2026-01-17T14-13-16_run-044\n",
      "   ðŸŽ¯ Accuracy: 87.00%\n",
      "   ðŸ”¢ Total tokens: 1542\n",
      "   âœ¨ SAE features: 32768\n",
      "\n",
      "ðŸ“Š Top 5 features by mean activation:\n",
      "   1. Feature 21110: mean=2.3480, max=8.7495, frac=96.95%\n",
      "   2. Feature 24583: mean=2.0934, max=8.8531, frac=96.95%\n",
      "   3. Feature 4456: mean=2.0080, max=10.7064, frac=95.46%\n",
      "   4. Feature 21969: mean=1.9741, max=5.8842, frac=97.80%\n",
      "   5. Feature 4247: mean=1.8205, max=6.2565, frac=98.90%\n",
      "\n",
      "ðŸŒ Start the viewer to see results:\n",
      "   python viz_analysis/feature_probe_server.py\n",
      "   cd sae-viewer && npm start\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import utilities\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
    "\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the SAE using the helper function\n",
    "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
    "\n",
    "# Extract dimensions from the loaded config\n",
    "SAE_INPUT_DIM = sae_config['input_dim']\n",
    "SAE_LATENT_DIM = sae_config['latent_dim']\n",
    "\n",
    "print(f\"âœ“ SAE loaded: {SAE_INPUT_DIM} dims â†’ {SAE_LATENT_DIM} sparse features\")\n",
    "\n",
    "# Create run directory using the same utility as main.py\n",
    "# This ensures the server can find it automatically in analysis_data/\n",
    "run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"\\nðŸ’¾ Saving results to: {run_dir}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sae.to(device)\n",
    "model.eval()\n",
    "sae.eval()\n",
    "\n",
    "# Load dataset\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "# Feature statistics tracker (per-token aggregation)\n",
    "class FeatureStatsAggregator:\n",
    "    def __init__(self, feature_dim: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.total_tokens = 0\n",
    "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray):\n",
    "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
    "        self.total_tokens += token_activations.shape[0]\n",
    "        self.sum_activations += token_activations.sum(axis=0)\n",
    "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
    "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
    "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
    "    \n",
    "    def get_stats(self):\n",
    "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
    "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
    "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
    "        return {\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": self.max_activations,\n",
    "            \"fraction_active\": frac_active,\n",
    "            \"mean_act_squared\": mean_act_squared\n",
    "        }\n",
    "\n",
    "# Top token tracker per feature\n",
    "class FeatureTopTokenTracker:\n",
    "    def __init__(self, feature_dim: int, top_k: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.top_k = top_k\n",
    "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
    "        self.heaps = [[] for _ in range(feature_dim)]\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
    "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
    "               predicted_label: str = None, true_label: str = None):\n",
    "        \"\"\"Update with tokens from one prompt\"\"\"\n",
    "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
    "            # For each token, find top features\n",
    "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
    "            \n",
    "            for feat_id in top_features:\n",
    "                activation = float(act_vec[feat_id])\n",
    "                if activation <= 0:\n",
    "                    continue\n",
    "                \n",
    "                heap = self.heaps[feat_id]\n",
    "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"activation\": activation,\n",
    "                    \"token_str\": token_str,\n",
    "                    \"token_id\": int(token_id),\n",
    "                    \"token_position\": int(token_pos),\n",
    "                    \"prompt_index\": int(prompt_idx),\n",
    "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
    "                    \"prompt_snippet\": prompt_text[:160],\n",
    "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
    "                    \"true_label\": true_label,\n",
    "                }\n",
    "                \n",
    "                if len(heap) < self.top_k:\n",
    "                    heapq.heappush(heap, (activation, metadata))\n",
    "                elif activation > heap[0][0]:\n",
    "                    heapq.heapreplace(heap, (activation, metadata))\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export top tokens for each feature\"\"\"\n",
    "        result = {}\n",
    "        for feat_id in range(self.feature_dim):\n",
    "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
    "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
    "        return result\n",
    "\n",
    "# Aggregate top features per headline (sample-level view)\n",
    "class HeadlineFeatureAggregator:\n",
    "    def __init__(self, top_k: int = 10):\n",
    "        self.top_k = top_k\n",
    "        self.headlines = []  # List of headline metadata with top features\n",
    "    \n",
    "    def add_headline(self, prompt_idx: int, prompt_text: str,\n",
    "                     token_activations: np.ndarray,\n",
    "                     token_ids: List[int],\n",
    "                     token_strings: List[str],\n",
    "                     predicted_label: str, true_label: str):\n",
    "        \"\"\"Aggregate features across all tokens in a headline\"\"\"\n",
    "        if token_activations.size == 0:\n",
    "            return\n",
    "        # Max activation per feature and which token triggered it\n",
    "        max_token_idx_per_feature = token_activations.argmax(axis=0)  # [feature_dim]\n",
    "        max_activation_per_feature = token_activations.max(axis=0)     # [feature_dim]\n",
    "        \n",
    "        # Get top K features by their max activation in this headline\n",
    "        top_feature_ids = np.argsort(max_activation_per_feature)[-self.top_k:][::-1]\n",
    "        \n",
    "        features = [\n",
    "            {\n",
    "                \"feature_id\": int(fid),\n",
    "                \"max_activation\": float(max_activation_per_feature[fid]),\n",
    "                \"token_position\": int(max_token_idx_per_feature[fid]),\n",
    "                \"token_id\": int(token_ids[max_token_idx_per_feature[fid]]),\n",
    "                \"token_str\": token_strings[max_token_idx_per_feature[fid]],\n",
    "            }\n",
    "            for fid in top_feature_ids if max_activation_per_feature[fid] > 0\n",
    "        ]\n",
    "        \n",
    "        self.headlines.append({\n",
    "            \"row_id\": int(prompt_idx),\n",
    "            \"prompt\": prompt_text,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"true_label\": true_label,\n",
    "            \"correct\": predicted_label == true_label,\n",
    "            \"num_tokens\": int(token_activations.shape[0]),\n",
    "            \"features\": features\n",
    "        })\n",
    "    \n",
    "    def export(self):\n",
    "        return self.headlines\n",
    "\n",
    "# Initialize trackers for SAE features\n",
    "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
    "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "headline_aggregator = HeadlineFeatureAggregator(top_k=10)\n",
    "\n",
    "# Storage for per-sample metadata\n",
    "all_prompt_metadata = []\n",
    "all_prediction_metadata = []\n",
    "\n",
    "# Hook to capture activations\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook on target layer\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "print(f\"\\nðŸ”¬ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
    "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
    "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        captured_activations.clear()\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        \n",
    "        # Get captured activation and pass through SAE\n",
    "        if captured_activations:\n",
    "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
    "            bert_activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (same as training) - do on GPU\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "            \n",
    "            # Filter activations on GPU\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            # Skip if no valid tokens\n",
    "            if bert_activation.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
    "            sae_features = sae.encode(bert_activation)\n",
    "            \n",
    "            # Only now move to CPU for numpy conversion and token filtering\n",
    "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
    "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
    "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
    "            \n",
    "            seq_len = sae_features_cpu.shape[0]\n",
    "            \n",
    "            # Update feature statistics with SAE features\n",
    "            feature_stats.update(sae_features_cpu)\n",
    "            \n",
    "            # Track top tokens per feature\n",
    "            top_token_tracker.update(\n",
    "                sae_features_cpu, \n",
    "                filtered_token_ids, \n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                prompt_tokens=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,  # Pass prediction info\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Aggregate top features at headline level\n",
    "            headline_aggregator.add_headline(\n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                token_activations=sae_features_cpu,\n",
    "                token_ids=filtered_token_ids,\n",
    "                token_strings=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Save prompt metadata\n",
    "            all_prompt_metadata.append({\n",
    "                \"row_id\": idx,\n",
    "                \"seq_len\": seq_len,\n",
    "                \"prompt\": text,\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"true_label\": model.config.id2label[true_label],\n",
    "                \"correct\": pred_id == true_label\n",
    "            })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Compute final statistics\n",
    "print(\"\\nðŸ“Š Computing feature statistics...\")\n",
    "stats = feature_stats.get_stats()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
    "print(f\"ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric = {}\n",
    "for metric_name, values in stats.items():\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
    "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# Save results\n",
    "print(\"\\nðŸ’¾ Saving results...\")\n",
    "\n",
    "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
    "prompts_file = run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
    "feature_stats_file = run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": SAE_LATENT_DIM,\n",
    "    \"total_tokens\": feature_stats.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
    "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
    "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric[metric_name]\n",
    "        }\n",
    "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
    "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save headline-level features\n",
    "headline_features_file = run_dir / \"headline_features.json\"\n",
    "with open(headline_features_file, \"w\") as f:\n",
    "    json.dump(headline_aggregator.export(), f, indent=2)\n",
    "\n",
    "# 5. Save metadata\n",
    "metadata_file = run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata),\n",
    "        \"total_tokens\": feature_stats.total_tokens,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": SAE_INPUT_DIM,\n",
    "        \"latent_dim\": SAE_LATENT_DIM,\n",
    "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"note\": \"SAE sparse features with predictions\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… COMPLETE!\")\n",
    "print(f\"   ðŸ“ Results saved to: {run_dir.name}\")\n",
    "print(f\"   ðŸŽ¯ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"   ðŸ”¢ Total tokens: {feature_stats.total_tokens}\")\n",
    "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
    "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
    "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
    "    metrics = feat['metrics']\n",
    "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
    "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
    "          f\"max={metrics['max_activation']:.4f}, \"\n",
    "          f\"frac={metrics['fraction_active']:.2%}\")\n",
    "\n",
    "print(f\"\\nðŸŒ Start the viewer to see results:\")\n",
    "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
    "print(f\"   cd sae-viewer && npm start\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def14c72",
   "metadata": {},
   "source": [
    "Testing Inference based on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de85884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BULLISH  | TSLA beats earnings expectations and raises full-year guidance.\n",
      "BEARISH  | Apple shares fall after reporting weaker-than-expected iPhone sales.\n",
      "NEUTRAL  | The company reported results largely in line with analyst expectations.\n",
      "BEARISH  | Amazon warns of margin pressure due to rising logistics costs.\n",
      "BULLISH  | NVIDIA stock surges as demand for AI chips remains strong.\n",
      "BEARISH  | The firm announced a restructuring plan, sending shares lower.\n",
      "NEUTRAL  | Revenue growth slowed quarter-over-quarter, but profitability improved.\n",
      "BEARISH  | Investors remain cautious ahead of the Federal Reserve meeting.\n",
      "BULLISH  | Strong cash flow and reduced debt boosted investor confidence.\n",
      "BEARISH  | The outlook remains uncertain amid macroeconomic headwinds.\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "\n",
    "example_sentences = [\n",
    "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
    "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
    "    \"The company reported results largely in line with analyst expectations.\",\n",
    "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
    "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
    "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
    "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
    "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
    "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
    "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "# optional: move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    pred_id = out.logits.argmax(dim=-1).item()\n",
    "    return model.config.id2label[pred_id]\n",
    "\n",
    "for text in example_sentences:\n",
    "    label = predict_sentiment(text)\n",
    "    print(f\"{label.upper():8} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f0b474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$ALLY - Ally Financial pulls outlook',\n",
       " '$DELL $HPE - Dell, HPE targets trimmed on compute headwinds',\n",
       " \"$PRTY - Moody's turns negative on Party City\",\n",
       " '$SAN: Deutsche Bank cuts to Hold',\n",
       " '$SITC: Compass Point cuts to Sell',\n",
       " '$TAP - Barclays cools on Molson Coors',\n",
       " '$TAP: Barclays cuts to Equal Weight',\n",
       " 'Analysts Eviscerate Musk\\'s Cybertruck: \"0% Of Responses Felt It Will Be A Success\"',\n",
       " 'Barclays assigns only a 20% chance that studies on a Gilead antiviral drug being done in China will succeed againstâ€¦',\n",
       " \"BTIG points to breakfast pressure for Dunkin' Brands\",\n",
       " \"Children's Place downgraded to neutral from outperform at Wedbush, price target slashed to $60 from $130\",\n",
       " 'Clovis Oncology downgraded to in line from outperform at Evercore ISI',\n",
       " 'Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦',\n",
       " \"Goldman pulls Progressive from Goldman's conviction list; shares -2.7%\",\n",
       " 'Hanesbrands downgraded to underperform vs. neutral at BofA Merrill Lynch',\n",
       " 'Intelsat cut to Market Perform at Raymond James',\n",
       " 'LendingTree price target cut to $350 from $400 at SunTrust Robinson Humphrey',\n",
       " 'Mizuho cuts XLNX target on near-term headwinds',\n",
       " 'MPLX cut at Credit Suisse on potential dilution from Marathon strategic review',\n",
       " 'Netflix downgraded to underperform at Wells Fargo',\n",
       " 'NortonLifeLock stock price target cut to $18 from $25 at Deutsche Bank',\n",
       " 'Norwegian Cruise stock price target cut to $48 from $55 at CFRA',\n",
       " 'Novus Therapeutics stock price target cut to $2.50 from $3.75 at Ascendiant Capital',\n",
       " 'Nutrien stock price target cut to $54 vs. $55 at BofA Merrill Lynch',\n",
       " 'Okta -2% on valuation downgrade',\n",
       " 'Solarworlds stock price target cut to $23 vs. $24 at Instinet',\n",
       " 'Trendforce cuts iPhone estimate after Foxconn delay',\n",
       " 'Wayfair price target lowered to $110 from $120 at Stifel, buy rating maintained',\n",
       " 'WWE stock price target cut to $58 from $92 at MKM Partners',\n",
       " '$ABEO as expected, keeps going higher. Cantor doubled its price target this morning to $4',\n",
       " '$CACI - CACI gains a bull on growth acceleration',\n",
       " '$CNHI - CNH Industrial upped to Buy at Deutsche Bank on valuation',\n",
       " '$DVN $HES $VET - Devon Energy, Hess upgraded at J.P. Morgan on improved E&P outlook',\n",
       " \"$ENR - Energizer shakes off JPMorgan's bear call\",\n",
       " '$GOOS - Baird bullish on beat-up Canada Goose',\n",
       " '$NKE - BMO Capital joins Nike bull camp',\n",
       " '$OIH $BKR $NOV - Buy oil service firms, Bernstein says after seven-year bearish view',\n",
       " '$SMPQY $HSIC $XRAY - Goldman ups view on Henry Schein in premarket analyst action',\n",
       " '$STML: Alliance Global Partners starts at Buy',\n",
       " '$URBN - Urban Outfitters stands out in mall sector - BofA',\n",
       " '$ZYME: H.C. Wainwright starts at Buy',\n",
       " 'Adobe price target raised to $350 vs. $320 at Canaccord',\n",
       " 'AM Best Revises Outlooks to Positive for PacÃ­fico CompaÃ±Ã­a de Seguros y Reaseguros S.A.',\n",
       " \"AMD +1.3% after Cowen's target lift\",\n",
       " 'Applied Materials, Inc. Reported Earnings Last Week And Analysts Are Already Upgrading Their Estimates',\n",
       " 'Arch Coal rated Buy at Benchmark, seeing opportunity after selloff',\n",
       " 'Autodesk stock price target raised to $162 from $149 at Wedbush',\n",
       " 'Avnet stock price target raised to $37 vs. $35 at SunTrust Robinson Humphrey',\n",
       " 'Boeing started at buy with $375 stock price target at Benchmark',\n",
       " \"BofA sees 'solid demand backdrop' for D.R. Horton in 2020\",\n",
       " 'Broadcom stock price target raised to $361 vs. $322 at SunTrust Robinson Humphrey',\n",
       " 'Cantor sees 19% upside in J&J in premarket analyst action',\n",
       " 'Champions Oncology started at speculative buy with $11 stock price target at Benchmark',\n",
       " 'Citi raises Alphabet on margin potential',\n",
       " 'Citigroup stock price target raised to $127 from $124 at Oppenheimer',\n",
       " 'ETF assets to surge tenfold in 10 years to $50 trillion, Bank of America predicts',\n",
       " 'GE upgraded from Sell by longtime bear',\n",
       " 'Goldman Sachs stock price target raised to $367 from $358 at Oppenheimer',\n",
       " 'GrubHub stock price target raised to $50 from $40 at Stifel Nicolaus',\n",
       " 'Halliburton started at buy with $26 stock price target at Deutsche Bank',\n",
       " 'Helen of Troy started at outperform with $210 stock price target at Oppenheimer',\n",
       " 'Highlight: Edward Jones Sr. Equity Analyst Jennifer Roland on ExxonMobil reducing 2020 Capex by 30%: â€œDefinitely aâ€¦',\n",
       " 'Humana stock price target raised to $370 from $350 at SunTrust RH',\n",
       " 'JMP Securities upgrades snap to outperform with $20 pt',\n",
       " 'Liberty Global PLC maintained as buy with $32 price target at Benchmark',\n",
       " 'Lululemon Athletica price target raised to $255 from $220 at CFRA',\n",
       " 'Lumentum initiated as positive at Susquehanna',\n",
       " 'Markel Corporation Beat Analyst Estimates: See What The Consensus Is Forecasting For Next Year',\n",
       " 'Mednax upgraded to buy from hold at Stifel, price target raised to $33 from $25',\n",
       " 'Model N upgraded to overweight from neutral at JPMorgan, price target raised to $36 from $30',\n",
       " 'Morgan Stanley sees GM doubling in bull case scenario',\n",
       " 'Motorola Solutions stock fair value estimate raised to $192 from $167 at MKM Partners',\n",
       " 'New for subscribers: Analysts continue to upgrade stocks like Tesla and eBay on hopes the rebound is for realâ€¦',\n",
       " 'Omnicell stock price target raised to $96 vs. $90 at Benchmark',\n",
       " 'Peloton bulls defend the upside case',\n",
       " 'Piper Jaffray analyst Erinn Murphy reiterated an Overweight rating and $44.00 price target on Crocs (NASDAQ: $CROX)â€¦',\n",
       " 'Progyny started at overweight with $26 stock price target at J.P. Morgan',\n",
       " 'Snap Analyst Projects 37% Revenue Growth In 2020',\n",
       " 'Spirit Airlines stock price target raised to $55 from $49 at Deutsche Bank',\n",
       " 'State Street stock price target raised to $84 from $73 at Buckingham',\n",
       " \"Stock Market Update: AMD's price target raised to $47 at Cowen\",\n",
       " 'Synaptics stock price target raised to $76 from $60 at Susquehanna',\n",
       " 'TechnipFMC started at buy with $33 stock price target at Deutsche Bank',\n",
       " 'Tesla stock price target raised to $290 from $260 at Deutsche Bank',\n",
       " 'TJX stock price target raised to $70 from $62 at MKM Partners',\n",
       " 'Twitter upgraded to positive from neutral at Susquehanna',\n",
       " 'Uber stock price target raised to $42 from $31 at Susquehanna',\n",
       " 'Uber stock price target raised to $48 from $46 at HSBC Global',\n",
       " 'UBS Upgrades Hasbro As Stock Price Bakes In Tariffs, Slow Holiday',\n",
       " 'UnitedHealth stock price target raised to $335 from $310 at SunTrust RH',\n",
       " 'Vertex Pharmaceuticals stock price target raised to $245 from $235 at BofA Merrill Lynch',\n",
       " 'Xilinx stock price target raised to $97 vs. $94 at SunTrust Robinson Humphrey',\n",
       " 'AM Best Revises Outlooks to Stable for Sublimity Insurance Company',\n",
       " 'Analyst: Amazon Blocking FedEx Ground Good News For UPS',\n",
       " 'Analysts Expect Breakeven For China Online Education Group (NYSE:COE)',\n",
       " \"Crown Holdings, Inc. Full-Year Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Here are the best analyst calls of the week on Wall Street including Disney and a satellite play',\n",
       " 'Little impact on Netflix from Disney Plus launch - Credit Suisse',\n",
       " \"Mitek bull calls pullback 'an overreaction'\",\n",
       " 'Peabody Energy started at hold at Benchmark',\n",
       " \"Skechers U.S.A., Inc. Annual Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " \"T. Boone Pickens' BP Capital Adds 3 Energy Stocks to Portfolio\",\n",
       " 'Tenable Holdings, Inc. Just Released Its Third-Quarter And Analysts Have Been Updating Their Estimates',\n",
       " 'Top Analyst Upgrades and Downgrades: Cardinal Health, Dominoâ€™s, FedEx, Ford, GE, Grubhub, HSBC, Marvell, Twitter, Uber and More',\n",
       " \"Tyson Foods, Inc. First-Quarter Results Just Came Out: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Wayfair initiated as hold with $95 price target at SunTrust Robinson Humphrey',\n",
       " \"Central bank 'collateral damage' is skewing financial markets, one economist says\",\n",
       " 'China central bank warns high financial risks amid rising economic headwinds - Reuters',\n",
       " 'ECB data show eurozone banks had weak profits before coronavirus',\n",
       " \"Fed's Williams says U.S. economy is clearly facing several challenges\",\n",
       " \"Former Federal Reserve Chairman Ben Bernanke says he doesn't see a quick, sharp rebound in the economy after its prâ€¦\",\n",
       " 'Jerome Powell comes close to acknowledging that the Federal Reserve may not have the firepower to fight the next reâ€¦',\n",
       " 'No, The Fed Won\\'t \"Save The Market\" - Here\\'s Why',\n",
       " '\"There is every reason to believe that the economic rebound, when it comes, can be robust,\" Fed Chair Jerome Powellâ€¦',\n",
       " '$ECONX: Federal Reserve announces extensive new measures to support the economy',\n",
       " 'Commercial and industrial loans at all commercial banks climb by $105.8 billion to $2.845 trillion in the week endiâ€¦',\n",
       " 'Fed surprises market with program to support corporate bonds amid coronavirus pandemic',\n",
       " 'Highlight: The Fed has launched an unprecedented round of asset purchases. \"The idea here is the Federal Reserve isâ€¦',\n",
       " \"The World Bank plans to lend Uganda $1.9 billion to help finance the East African nation's budget deficit\",\n",
       " '\"We encourage families to explore virtual #financialliteracy resources. The Federal Reserve Bank of Richmond createâ€¦',\n",
       " 'Africa-Europe Alliance: Denmark provides \\x8010 million for sustainable development under the EU External Investment Pâ€¦',\n",
       " \"Bank agencies announced they'll temporarily lower the community bank leverage ratio to 8%. The rule change -\\x9d requirâ€¦\",\n",
       " 'Bank of Ireland : Mortgage Bank - 7 KB #BankofIreland #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 14-Day Repo Auction Announcement -24 February 2020 #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 30-day CD Auction Press Release #economy #MarketScreener',\n",
       " 'Brazil economists forecast more rate cuts ahead',\n",
       " 'Buyback Backlash Begins: Fed Will Limit Buybacks & Dividends For Companies Using Its Credit Facility',\n",
       " 'Central Bank of Nigeria Communique No. 128 of the Monetary Policy Committee Meeting of Jan... #Stockâ€¦',\n",
       " 'Central banks donâ€™t have as much monetary policy power as they used to, former BOE policy maker Ian McCafferty says',\n",
       " \"Cleveland Fed President Loretta Mester said the coronavirus outbreak is a threat to the economy but doesn't yet jusâ€¦\",\n",
       " 'Efforts to support the faltering economy could extend to some form of direct yield curve control. Newâ€¦',\n",
       " \"Fed Chair Jay Powell grilled on China's cryptocurrency plans, US response by @readDanwrite\",\n",
       " 'Fed Chair Jerome Powell was chided by a Democratic lawmaker for his attendance at a party thrown by Amazon CEO Jeffâ€¦',\n",
       " 'Fed Chairman Jerome Powell puts lawmakers on notice that fiscal policy may need to play a bigger role countering doâ€¦',\n",
       " 'Fed minutes to tell a story',\n",
       " 'Fed Officials Weigh Risks Of Covid-19 -- Update #economy #MarketScreener',\n",
       " 'Fed President Loretta Mester due to speak in 5 minutes.',\n",
       " 'Fed removes reference to \"global developments\", has yet to blame global warming for repo crisis',\n",
       " 'Fedâ€™s Rosengren Says Pursuing 2% Inflation Could Distort Markets',\n",
       " 'Federal Reserve Keeps Rates Steady and Sees Long Pause #currency #MarketScreener',\n",
       " 'Federal Reserve officials will feel comfortable maintaining their wait-and-see posture on interest rates after Fridâ€¦',\n",
       " \"Fed's 42-Day Repo 2x Oversubscribed In Scramble For Year End Liquidity\",\n",
       " \"Fed's Bullard said actions taken by the Fed and other authorities during this time period shouldn't be seen as stimâ€¦\",\n",
       " \"Fed's Mester says central bank can watch and wait on interest rates\",\n",
       " \"Fed's Rosengren backs 'patient' approach to any interest-rate moves\",\n",
       " \"Fed's Williams says rates are 'in the right place' but policy is not set in stone\",\n",
       " 'Global Risks to Extend Czech Rate Lockdown: Decision Day Guide',\n",
       " 'Heard on the Street: The Fed may need banks to lever up to absorb more activity, but it needs to do so in a way thaâ€¦',\n",
       " 'In the battle of central-bank stimulus bazookas, the Fed is winning out',\n",
       " 'In undertaking what will undoubtedly be its largest rescue effort ever, the Federal Reserve announced programs thatâ€¦',\n",
       " \"Indonesia's central bank says the New York Federal Reserve will provide it with a $60 billion repurchase facility tâ€¦\",\n",
       " 'Italy may be willing to compromise on its holdout against a German plan for further integration of the EUâ€™s bankingâ€¦',\n",
       " 'LISTEN NOW: The Fed continues to make moves to combat the coronavirus crisis, including assuring limitless asset puâ€¦',\n",
       " 'LIVE: Fed Chair Powell testifies before the Senate Banking Committee',\n",
       " 'Markets bet Fed is pushed to cut rates in coronavirus response',\n",
       " 'Minneapolis Fed chief Neel Kashkari says monetary policy can play the kind of redistributing role once thought to bâ€¦',\n",
       " 'Morocco is considering tapping at least part of a $2.97 billion liquidity line with the IMF as the coronavirus outbâ€¦',\n",
       " \"New Zealand central bank governor Adrian Orr may signal he's prepared to cut interest rates to cushion the economic...\",\n",
       " \"Norway's central bank governor says it's too early to reach any conclusions on the long term outlook for inflation\",\n",
       " 'Praet: ECB Needs Governments to Act',\n",
       " 'Rabobank: \"Fed\\'s Kashkari Has Just Come Out With A Jaw-Dropping Policy Shift\"',\n",
       " 'RBI May Cut Rates Later This Year, TS Lombard Says',\n",
       " 'RBIâ€™s Long-Term Repo Operations Seen As Stealth Move To Bring Down Rates',\n",
       " 'Spain to guarantee up to 80% of SME bank loans to ease virus impact #economy #MarketScreener',\n",
       " \"The Bank of England's Financial Policy Committee says it's prepared to take further action as needed to help the fiâ€¦\",\n",
       " \"The Bank of England's Term Funding Scheme... #Stock #MarketScreener\",\n",
       " 'The Committee lowers the CBR to 7.25PC #Stock #MarketScreener',\n",
       " 'The consensus case for following the Fed is right, Ashok Bhatia of Neuberger Bergman tells @FerroTVâ€¦',\n",
       " \"The Fed mounted an extraordinary new array of programs to offset the 'severe disruptions' to the economy caused byâ€¦\",\n",
       " 'The Federal Reserve has launched an array of programs aimed at helping the markets and economy through the coronaviâ€¦',\n",
       " 'The Federal Reserve System includes 12 districts. Using those numbers, 1-12, is a handy way to learn about the centâ€¦',\n",
       " 'The FRED Blog: Since the end of the Great Recession, market-based measures of long-run inflation expectations haveâ€¦',\n",
       " \"The IMF may launch a new program that could back up the Federal Reserve's campaign to keep dollars flowing in the gâ€¦\",\n",
       " 'The Most Momentous Rate Decision This Month Isnâ€™t at Fed or ECB',\n",
       " '-The U.K. will go into full lockdown to stymie the spread of coronavirus -The Fed offers to directly finance U.S. câ€¦',\n",
       " 'Top economists say that the spread of the new coronavirus outbreak makes it more likely that the Federal Reserve wiâ€¦',\n",
       " 'Two crisis-veteran Federal Reserve officials said the central bank has plenty of room for more moves following a flâ€¦',\n",
       " \"Two weeks later, no change, Fed ain't stopping the selling\",\n",
       " 'U.S. Fed preparing to buy new small business payroll loans: Wall Street Journal',\n",
       " 'UK starts first stage of 330 billion pound loan guarantee scheme',\n",
       " 'Zambiaâ€™s kwacha staged a comeback on Tuesday after the central bank raised the amount of funds that commercial lendâ€¦',\n",
       " 'Bank of America sees demand surge for paycheck protection loans',\n",
       " '$HON - Honeywell says MAX production freeze to hurt 2020 sales growth',\n",
       " 'Africa retailer Jumia suspends e-commerce in Cameroon',\n",
       " 'Amazon blames holiday delivery delays on winter storms and high demand',\n",
       " 'Beckhamsâ€™ business empire sees revenues drop 18%',\n",
       " 'BREAKING: FDA issues ban on some fruit and mint flavored vaping products',\n",
       " 'Bristol-Myers says trial of Opdivo plus Yervoy in melanoma treatment failed to meet main goal',\n",
       " 'Casino Operator Wynn Resorts Is Losing Up To $2.6 Million Per Day From Coronavirus Shutdown',\n",
       " 'China Solar Group Flags Raw Material, Labor Shortages on Virus',\n",
       " 'Fiat Chrysler Relies on Ram Growth as Europe Losses Mount',\n",
       " 'Ford recalls 262,000 pickup trucks with defective tailgate latches',\n",
       " 'Fox says cable ad revenue was hurt by impeachment coverage last quarter',\n",
       " 'ICYMI: Transport for London said Uber $UBER was not \"fit and proper\" to hold an operating license in London after aâ€¦',\n",
       " 'Jetstar to cut capacity by 10% in January as pilots take industrial action',\n",
       " \"Lowe's to shut 34 Canadian stores as it reports revenue miss\",\n",
       " \"Macy's says temporary issue with e-commerce business also impacted Q3 performance\",\n",
       " 'Mahindra Says Coronavirus May Jeopardize India Emissions Rollout',\n",
       " 'Major blockchain developer ConsenSys announces job losses #economy #MarketScreener',\n",
       " 'NTSB cites Uber, driver in fatal crash']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "test_ds[\"text\"][0:200]\n",
    "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "#ds2[\"validation\"][\"text\"][34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e2a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2388/2388 [00:00<00:00, 19143.73 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ally Financial pulls outlook',\n",
       " 'Dell, HPE targets trimmed on compute headwinds',\n",
       " \"Moody's turns negative on Party City\",\n",
       " 'Deutsche Bank cuts to Hold',\n",
       " 'Compass Point cuts to Sell',\n",
       " 'Barclays cools on Molson Coors',\n",
       " 'Barclays cuts to Equal Weight',\n",
       " 'Analysts Eviscerate Musk\\'s Cybertruck: \"0% Of Responses Felt It Will Be A Success\"',\n",
       " 'Barclays assigns only a 20% chance that studies on a Gilead antiviral drug being done in China will succeed againstâ€¦',\n",
       " \"BTIG points to breakfast pressure for Dunkin' Brands\",\n",
       " \"Children's Place downgraded to neutral from outperform at Wedbush, price target slashed to $60 from $130\",\n",
       " 'Clovis Oncology downgraded to in line from outperform at Evercore ISI',\n",
       " 'Downgrades 4/7: $AAN $BDN $BECN $BTE $CDEV $CHK $COOP $CPE $CVA $DAN $DOC $DRH $EPR $ESRT $ETM $FAST $FBM $GM $GMSâ€¦',\n",
       " \"Goldman pulls Progressive from Goldman's conviction list; shares -2.7%\",\n",
       " 'Hanesbrands downgraded to underperform vs. neutral at BofA Merrill Lynch',\n",
       " 'Intelsat cut to Market Perform at Raymond James',\n",
       " 'LendingTree price target cut to $350 from $400 at SunTrust Robinson Humphrey',\n",
       " 'Mizuho cuts XLNX target on near-term headwinds',\n",
       " 'MPLX cut at Credit Suisse on potential dilution from Marathon strategic review',\n",
       " 'Netflix downgraded to underperform at Wells Fargo',\n",
       " 'NortonLifeLock stock price target cut to $18 from $25 at Deutsche Bank',\n",
       " 'Norwegian Cruise stock price target cut to $48 from $55 at CFRA',\n",
       " 'Novus Therapeutics stock price target cut to $2.50 from $3.75 at Ascendiant Capital',\n",
       " 'Nutrien stock price target cut to $54 vs. $55 at BofA Merrill Lynch',\n",
       " 'Okta -2% on valuation downgrade',\n",
       " 'Solarworlds stock price target cut to $23 vs. $24 at Instinet',\n",
       " 'Trendforce cuts iPhone estimate after Foxconn delay',\n",
       " 'Wayfair price target lowered to $110 from $120 at Stifel, buy rating maintained',\n",
       " 'WWE stock price target cut to $58 from $92 at MKM Partners',\n",
       " 'as expected, keeps going higher. Cantor doubled its price target this morning to $4',\n",
       " 'CACI gains a bull on growth acceleration',\n",
       " 'CNH Industrial upped to Buy at Deutsche Bank on valuation',\n",
       " 'Devon Energy, Hess upgraded at J.P. Morgan on improved E&P outlook',\n",
       " \"Energizer shakes off JPMorgan's bear call\",\n",
       " 'Baird bullish on beat-up Canada Goose',\n",
       " 'BMO Capital joins Nike bull camp',\n",
       " 'Buy oil service firms, Bernstein says after seven-year bearish view',\n",
       " 'Goldman ups view on Henry Schein in premarket analyst action',\n",
       " 'Alliance Global Partners starts at Buy',\n",
       " 'Urban Outfitters stands out in mall sector - BofA',\n",
       " 'H.C. Wainwright starts at Buy',\n",
       " 'Adobe price target raised to $350 vs. $320 at Canaccord',\n",
       " 'AM Best Revises Outlooks to Positive for PacÃ­fico CompaÃ±Ã­a de Seguros y Reaseguros S.A.',\n",
       " \"AMD +1.3% after Cowen's target lift\",\n",
       " 'Applied Materials, Inc. Reported Earnings Last Week And Analysts Are Already Upgrading Their Estimates',\n",
       " 'Arch Coal rated Buy at Benchmark, seeing opportunity after selloff',\n",
       " 'Autodesk stock price target raised to $162 from $149 at Wedbush',\n",
       " 'Avnet stock price target raised to $37 vs. $35 at SunTrust Robinson Humphrey',\n",
       " 'Boeing started at buy with $375 stock price target at Benchmark',\n",
       " \"BofA sees 'solid demand backdrop' for D.R. Horton in 2020\",\n",
       " 'Broadcom stock price target raised to $361 vs. $322 at SunTrust Robinson Humphrey',\n",
       " 'Cantor sees 19% upside in J&J in premarket analyst action',\n",
       " 'Champions Oncology started at speculative buy with $11 stock price target at Benchmark',\n",
       " 'Citi raises Alphabet on margin potential',\n",
       " 'Citigroup stock price target raised to $127 from $124 at Oppenheimer',\n",
       " 'ETF assets to surge tenfold in 10 years to $50 trillion, Bank of America predicts',\n",
       " 'GE upgraded from Sell by longtime bear',\n",
       " 'Goldman Sachs stock price target raised to $367 from $358 at Oppenheimer',\n",
       " 'GrubHub stock price target raised to $50 from $40 at Stifel Nicolaus',\n",
       " 'Halliburton started at buy with $26 stock price target at Deutsche Bank',\n",
       " 'Helen of Troy started at outperform with $210 stock price target at Oppenheimer',\n",
       " 'Highlight: Edward Jones Sr. Equity Analyst Jennifer Roland on ExxonMobil reducing 2020 Capex by 30%: â€œDefinitely aâ€¦',\n",
       " 'Humana stock price target raised to $370 from $350 at SunTrust RH',\n",
       " 'JMP Securities upgrades snap to outperform with $20 pt',\n",
       " 'Liberty Global PLC maintained as buy with $32 price target at Benchmark',\n",
       " 'Lululemon Athletica price target raised to $255 from $220 at CFRA',\n",
       " 'Lumentum initiated as positive at Susquehanna',\n",
       " 'Markel Corporation Beat Analyst Estimates: See What The Consensus Is Forecasting For Next Year',\n",
       " 'Mednax upgraded to buy from hold at Stifel, price target raised to $33 from $25',\n",
       " 'Model N upgraded to overweight from neutral at JPMorgan, price target raised to $36 from $30',\n",
       " 'Morgan Stanley sees GM doubling in bull case scenario',\n",
       " 'Motorola Solutions stock fair value estimate raised to $192 from $167 at MKM Partners',\n",
       " 'New for subscribers: Analysts continue to upgrade stocks like Tesla and eBay on hopes the rebound is for realâ€¦',\n",
       " 'Omnicell stock price target raised to $96 vs. $90 at Benchmark',\n",
       " 'Peloton bulls defend the upside case',\n",
       " 'Piper Jaffray analyst Erinn Murphy reiterated an Overweight rating and $44.00 price target on Crocs (NASDAQ: $CROX)â€¦',\n",
       " 'Progyny started at overweight with $26 stock price target at J.P. Morgan',\n",
       " 'Snap Analyst Projects 37% Revenue Growth In 2020',\n",
       " 'Spirit Airlines stock price target raised to $55 from $49 at Deutsche Bank',\n",
       " 'State Street stock price target raised to $84 from $73 at Buckingham',\n",
       " \"Stock Market Update: AMD's price target raised to $47 at Cowen\",\n",
       " 'Synaptics stock price target raised to $76 from $60 at Susquehanna',\n",
       " 'TechnipFMC started at buy with $33 stock price target at Deutsche Bank',\n",
       " 'Tesla stock price target raised to $290 from $260 at Deutsche Bank',\n",
       " 'TJX stock price target raised to $70 from $62 at MKM Partners',\n",
       " 'Twitter upgraded to positive from neutral at Susquehanna',\n",
       " 'Uber stock price target raised to $42 from $31 at Susquehanna',\n",
       " 'Uber stock price target raised to $48 from $46 at HSBC Global',\n",
       " 'UBS Upgrades Hasbro As Stock Price Bakes In Tariffs, Slow Holiday',\n",
       " 'UnitedHealth stock price target raised to $335 from $310 at SunTrust RH',\n",
       " 'Vertex Pharmaceuticals stock price target raised to $245 from $235 at BofA Merrill Lynch',\n",
       " 'Xilinx stock price target raised to $97 vs. $94 at SunTrust Robinson Humphrey',\n",
       " 'AM Best Revises Outlooks to Stable for Sublimity Insurance Company',\n",
       " 'Analyst: Amazon Blocking FedEx Ground Good News For UPS',\n",
       " 'Analysts Expect Breakeven For China Online Education Group (NYSE:COE)',\n",
       " \"Crown Holdings, Inc. Full-Year Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Here are the best analyst calls of the week on Wall Street including Disney and a satellite play',\n",
       " 'Little impact on Netflix from Disney Plus launch - Credit Suisse',\n",
       " \"Mitek bull calls pullback 'an overreaction'\",\n",
       " 'Peabody Energy started at hold at Benchmark',\n",
       " \"Skechers U.S.A., Inc. Annual Results: Here's What Analysts Are Forecasting For Next Year\",\n",
       " \"T. Boone Pickens' BP Capital Adds 3 Energy Stocks to Portfolio\",\n",
       " 'Tenable Holdings, Inc. Just Released Its Third-Quarter And Analysts Have Been Updating Their Estimates',\n",
       " 'Top Analyst Upgrades and Downgrades: Cardinal Health, Dominoâ€™s, FedEx, Ford, GE, Grubhub, HSBC, Marvell, Twitter, Uber and More',\n",
       " \"Tyson Foods, Inc. First-Quarter Results Just Came Out: Here's What Analysts Are Forecasting For Next Year\",\n",
       " 'Wayfair initiated as hold with $95 price target at SunTrust Robinson Humphrey',\n",
       " \"Central bank 'collateral damage' is skewing financial markets, one economist says\",\n",
       " 'China central bank warns high financial risks amid rising economic headwinds - Reuters',\n",
       " 'ECB data show eurozone banks had weak profits before coronavirus',\n",
       " \"Fed's Williams says U.S. economy is clearly facing several challenges\",\n",
       " \"Former Federal Reserve Chairman Ben Bernanke says he doesn't see a quick, sharp rebound in the economy after its prâ€¦\",\n",
       " 'Jerome Powell comes close to acknowledging that the Federal Reserve may not have the firepower to fight the next reâ€¦',\n",
       " 'No, The Fed Won\\'t \"Save The Market\" - Here\\'s Why',\n",
       " '\"There is every reason to believe that the economic rebound, when it comes, can be robust,\" Fed Chair Jerome Powellâ€¦',\n",
       " 'Federal Reserve announces extensive new measures to support the economy',\n",
       " 'Commercial and industrial loans at all commercial banks climb by $105.8 billion to $2.845 trillion in the week endiâ€¦',\n",
       " 'Fed surprises market with program to support corporate bonds amid coronavirus pandemic',\n",
       " 'Highlight: The Fed has launched an unprecedented round of asset purchases. \"The idea here is the Federal Reserve isâ€¦',\n",
       " \"The World Bank plans to lend Uganda $1.9 billion to help finance the East African nation's budget deficit\",\n",
       " '\"We encourage families to explore virtual #financialliteracy resources. The Federal Reserve Bank of Richmond createâ€¦',\n",
       " 'Africa-Europe Alliance: Denmark provides \\x8010 million for sustainable development under the EU External Investment Pâ€¦',\n",
       " \"Bank agencies announced they'll temporarily lower the community bank leverage ratio to 8%. The rule change -\\x9d requirâ€¦\",\n",
       " 'Bank of Ireland : Mortgage Bank - 7 KB #BankofIreland #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 14-Day Repo Auction Announcement -24 February 2020 #Stock #MarketScreener',\n",
       " 'Bank of Jamaica 30-day CD Auction Press Release #economy #MarketScreener',\n",
       " 'Brazil economists forecast more rate cuts ahead',\n",
       " 'Buyback Backlash Begins: Fed Will Limit Buybacks & Dividends For Companies Using Its Credit Facility',\n",
       " 'Central Bank of Nigeria Communique No. 128 of the Monetary Policy Committee Meeting of Jan... #Stockâ€¦',\n",
       " 'Central banks donâ€™t have as much monetary policy power as they used to, former BOE policy maker Ian McCafferty says',\n",
       " \"Cleveland Fed President Loretta Mester said the coronavirus outbreak is a threat to the economy but doesn't yet jusâ€¦\",\n",
       " 'Efforts to support the faltering economy could extend to some form of direct yield curve control. Newâ€¦',\n",
       " \"Fed Chair Jay Powell grilled on China's cryptocurrency plans, US response by @readDanwrite\",\n",
       " 'Fed Chair Jerome Powell was chided by a Democratic lawmaker for his attendance at a party thrown by Amazon CEO Jeffâ€¦',\n",
       " 'Fed Chairman Jerome Powell puts lawmakers on notice that fiscal policy may need to play a bigger role countering doâ€¦',\n",
       " 'Fed minutes to tell a story',\n",
       " 'Fed Officials Weigh Risks Of Covid-19 -- Update #economy #MarketScreener',\n",
       " 'Fed President Loretta Mester due to speak in 5 minutes.',\n",
       " 'Fed removes reference to \"global developments\", has yet to blame global warming for repo crisis',\n",
       " 'Fedâ€™s Rosengren Says Pursuing 2% Inflation Could Distort Markets',\n",
       " 'Federal Reserve Keeps Rates Steady and Sees Long Pause #currency #MarketScreener',\n",
       " 'Federal Reserve officials will feel comfortable maintaining their wait-and-see posture on interest rates after Fridâ€¦',\n",
       " \"Fed's 42-Day Repo 2x Oversubscribed In Scramble For Year End Liquidity\",\n",
       " \"Fed's Bullard said actions taken by the Fed and other authorities during this time period shouldn't be seen as stimâ€¦\",\n",
       " \"Fed's Mester says central bank can watch and wait on interest rates\",\n",
       " \"Fed's Rosengren backs 'patient' approach to any interest-rate moves\",\n",
       " \"Fed's Williams says rates are 'in the right place' but policy is not set in stone\",\n",
       " 'Global Risks to Extend Czech Rate Lockdown: Decision Day Guide',\n",
       " 'Heard on the Street: The Fed may need banks to lever up to absorb more activity, but it needs to do so in a way thaâ€¦',\n",
       " 'In the battle of central-bank stimulus bazookas, the Fed is winning out',\n",
       " 'In undertaking what will undoubtedly be its largest rescue effort ever, the Federal Reserve announced programs thatâ€¦',\n",
       " \"Indonesia's central bank says the New York Federal Reserve will provide it with a $60 billion repurchase facility tâ€¦\",\n",
       " 'Italy may be willing to compromise on its holdout against a German plan for further integration of the EUâ€™s bankingâ€¦',\n",
       " 'LISTEN NOW: The Fed continues to make moves to combat the coronavirus crisis, including assuring limitless asset puâ€¦',\n",
       " 'LIVE: Fed Chair Powell testifies before the Senate Banking Committee',\n",
       " 'Markets bet Fed is pushed to cut rates in coronavirus response',\n",
       " 'Minneapolis Fed chief Neel Kashkari says monetary policy can play the kind of redistributing role once thought to bâ€¦',\n",
       " 'Morocco is considering tapping at least part of a $2.97 billion liquidity line with the IMF as the coronavirus outbâ€¦',\n",
       " \"New Zealand central bank governor Adrian Orr may signal he's prepared to cut interest rates to cushion the economic...\",\n",
       " \"Norway's central bank governor says it's too early to reach any conclusions on the long term outlook for inflation\",\n",
       " 'Praet: ECB Needs Governments to Act',\n",
       " 'Rabobank: \"Fed\\'s Kashkari Has Just Come Out With A Jaw-Dropping Policy Shift\"',\n",
       " 'RBI May Cut Rates Later This Year, TS Lombard Says',\n",
       " 'RBIâ€™s Long-Term Repo Operations Seen As Stealth Move To Bring Down Rates',\n",
       " 'Spain to guarantee up to 80% of SME bank loans to ease virus impact #economy #MarketScreener',\n",
       " \"The Bank of England's Financial Policy Committee says it's prepared to take further action as needed to help the fiâ€¦\",\n",
       " \"The Bank of England's Term Funding Scheme... #Stock #MarketScreener\",\n",
       " 'The Committee lowers the CBR to 7.25PC #Stock #MarketScreener',\n",
       " 'The consensus case for following the Fed is right, Ashok Bhatia of Neuberger Bergman tells @FerroTVâ€¦',\n",
       " \"The Fed mounted an extraordinary new array of programs to offset the 'severe disruptions' to the economy caused byâ€¦\",\n",
       " 'The Federal Reserve has launched an array of programs aimed at helping the markets and economy through the coronaviâ€¦',\n",
       " 'The Federal Reserve System includes 12 districts. Using those numbers, 1-12, is a handy way to learn about the centâ€¦',\n",
       " 'The FRED Blog: Since the end of the Great Recession, market-based measures of long-run inflation expectations haveâ€¦',\n",
       " \"The IMF may launch a new program that could back up the Federal Reserve's campaign to keep dollars flowing in the gâ€¦\",\n",
       " 'The Most Momentous Rate Decision This Month Isnâ€™t at Fed or ECB',\n",
       " '-The U.K. will go into full lockdown to stymie the spread of coronavirus -The Fed offers to directly finance U.S. câ€¦',\n",
       " 'Top economists say that the spread of the new coronavirus outbreak makes it more likely that the Federal Reserve wiâ€¦',\n",
       " 'Two crisis-veteran Federal Reserve officials said the central bank has plenty of room for more moves following a flâ€¦',\n",
       " \"Two weeks later, no change, Fed ain't stopping the selling\",\n",
       " 'U.S. Fed preparing to buy new small business payroll loans: Wall Street Journal',\n",
       " 'UK starts first stage of 330 billion pound loan guarantee scheme',\n",
       " 'Zambiaâ€™s kwacha staged a comeback on Tuesday after the central bank raised the amount of funds that commercial lendâ€¦',\n",
       " 'Bank of America sees demand surge for paycheck protection loans',\n",
       " 'Honeywell says MAX production freeze to hurt 2020 sales growth',\n",
       " 'Africa retailer Jumia suspends e-commerce in Cameroon',\n",
       " 'Amazon blames holiday delivery delays on winter storms and high demand',\n",
       " 'Beckhamsâ€™ business empire sees revenues drop 18%',\n",
       " 'BREAKING: FDA issues ban on some fruit and mint flavored vaping products',\n",
       " 'Bristol-Myers says trial of Opdivo plus Yervoy in melanoma treatment failed to meet main goal',\n",
       " 'Casino Operator Wynn Resorts Is Losing Up To $2.6 Million Per Day From Coronavirus Shutdown',\n",
       " 'China Solar Group Flags Raw Material, Labor Shortages on Virus',\n",
       " 'Fiat Chrysler Relies on Ram Growth as Europe Losses Mount',\n",
       " 'Ford recalls 262,000 pickup trucks with defective tailgate latches',\n",
       " 'Fox says cable ad revenue was hurt by impeachment coverage last quarter',\n",
       " 'ICYMI: Transport for London said Uber $UBER was not \"fit and proper\" to hold an operating license in London after aâ€¦',\n",
       " 'Jetstar to cut capacity by 10% in January as pilots take industrial action',\n",
       " \"Lowe's to shut 34 Canadian stores as it reports revenue miss\",\n",
       " \"Macy's says temporary issue with e-commerce business also impacted Q3 performance\",\n",
       " 'Mahindra Says Coronavirus May Jeopardize India Emissions Rollout',\n",
       " 'Major blockchain developer ConsenSys announces job losses #economy #MarketScreener',\n",
       " 'NTSB cites Uber, driver in fatal crash']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_leading_tickers(text):\n",
    "    return re.sub(\n",
    "        r'^\\s*(?:\\$[A-Z]{1,6}\\s*)+(?:[-:]\\s*)?',\n",
    "        '',\n",
    "        text\n",
    "    )\n",
    "\n",
    "test_ds = test_ds.map(lambda x: {\"clean_text\": remove_leading_tickers(x[\"text\"])})\n",
    "test_ds[\"clean_text\"][0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd689be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Inference\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import utilities\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
    "\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "SAE_SIZE = \"32k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the SAE using the helper function\n",
    "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
    "\n",
    "# Extract dimensions from the loaded config\n",
    "SAE_INPUT_DIM = sae_config['input_dim']\n",
    "SAE_LATENT_DIM = sae_config['latent_dim']\n",
    "\n",
    "print(f\"âœ“ SAE loaded: {SAE_INPUT_DIM} dims â†’ {SAE_LATENT_DIM} sparse features\")\n",
    "\n",
    "# Create run directory using the same utility as main.py\n",
    "# This ensures the server can find it automatically in analysis_data/\n",
    "run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"\\nðŸ’¾ Saving results to: {run_dir}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sae.to(device)\n",
    "model.eval()\n",
    "sae.eval()\n",
    "\n",
    "# Load dataset\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "# Feature statistics tracker (per-token aggregation)\n",
    "class FeatureStatsAggregator:\n",
    "    def __init__(self, feature_dim: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.total_tokens = 0\n",
    "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray):\n",
    "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
    "        self.total_tokens += token_activations.shape[0]\n",
    "        self.sum_activations += token_activations.sum(axis=0)\n",
    "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
    "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
    "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
    "    \n",
    "    def get_stats(self):\n",
    "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
    "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
    "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
    "        return {\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": self.max_activations,\n",
    "            \"fraction_active\": frac_active,\n",
    "            \"mean_act_squared\": mean_act_squared\n",
    "        }\n",
    "\n",
    "# Top token tracker per feature\n",
    "class FeatureTopTokenTracker:\n",
    "    def __init__(self, feature_dim: int, top_k: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.top_k = top_k\n",
    "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
    "        self.heaps = [[] for _ in range(feature_dim)]\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
    "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
    "               predicted_label: str = None, true_label: str = None):\n",
    "        \"\"\"Update with tokens from one prompt\"\"\"\n",
    "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
    "            # For each token, find top features\n",
    "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
    "            \n",
    "            for feat_id in top_features:\n",
    "                activation = float(act_vec[feat_id])\n",
    "                if activation <= 0:\n",
    "                    continue\n",
    "                \n",
    "                heap = self.heaps[feat_id]\n",
    "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"activation\": activation,\n",
    "                    \"token_str\": token_str,\n",
    "                    \"token_id\": int(token_id),\n",
    "                    \"token_position\": int(token_pos),\n",
    "                    \"prompt_index\": int(prompt_idx),\n",
    "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
    "                    \"prompt_snippet\": prompt_text[:160],\n",
    "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
    "                    \"true_label\": true_label,\n",
    "                }\n",
    "                \n",
    "                if len(heap) < self.top_k:\n",
    "                    heapq.heappush(heap, (activation, metadata))\n",
    "                elif activation > heap[0][0]:\n",
    "                    heapq.heapreplace(heap, (activation, metadata))\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export top tokens for each feature\"\"\"\n",
    "        result = {}\n",
    "        for feat_id in range(self.feature_dim):\n",
    "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
    "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
    "        return result\n",
    "\n",
    "# Initialize trackers for SAE features\n",
    "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
    "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "\n",
    "# Storage for per-sample metadata\n",
    "all_prompt_metadata = []\n",
    "all_prediction_metadata = []\n",
    "\n",
    "# Hook to capture activations\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook on target layer\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "print(f\"\\nðŸ”¬ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
    "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
    "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        captured_activations.clear()\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        \n",
    "        # Get captured activation and pass through SAE\n",
    "        if captured_activations:\n",
    "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
    "            bert_activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (same as training) - do on GPU\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "            \n",
    "            # Filter activations on GPU\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            # Skip if no valid tokens\n",
    "            if bert_activation.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
    "            sae_features = sae.encode(bert_activation)\n",
    "            \n",
    "            # Only now move to CPU for numpy conversion and token filtering\n",
    "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
    "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
    "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
    "            \n",
    "            seq_len = sae_features_cpu.shape[0]\n",
    "            \n",
    "            # Update feature statistics with SAE features\n",
    "            feature_stats.update(sae_features_cpu)\n",
    "            \n",
    "            # Track top tokens per feature\n",
    "            top_token_tracker.update(\n",
    "                sae_features_cpu, \n",
    "                filtered_token_ids, \n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                prompt_tokens=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,  # Pass prediction info\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Save prompt metadata\n",
    "            all_prompt_metadata.append({\n",
    "                \"row_id\": idx,\n",
    "                \"seq_len\": seq_len,\n",
    "                \"prompt\": text,\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"true_label\": model.config.id2label[true_label],\n",
    "                \"correct\": pred_id == true_label\n",
    "            })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Compute final statistics\n",
    "print(\"\\nðŸ“Š Computing feature statistics...\")\n",
    "stats = feature_stats.get_stats()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
    "print(f\"ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric = {}\n",
    "for metric_name, values in stats.items():\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
    "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# Save results\n",
    "print(\"\\nðŸ’¾ Saving results...\")\n",
    "\n",
    "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
    "prompts_file = run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
    "feature_stats_file = run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": SAE_LATENT_DIM,\n",
    "    \"total_tokens\": feature_stats.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
    "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
    "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric[metric_name]\n",
    "        }\n",
    "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
    "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save metadata\n",
    "metadata_file = run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata),\n",
    "        \"total_tokens\": feature_stats.total_tokens,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": SAE_INPUT_DIM,\n",
    "        \"latent_dim\": SAE_LATENT_DIM,\n",
    "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"note\": \"SAE sparse features with predictions\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… COMPLETE!\")\n",
    "print(f\"   ðŸ“ Results saved to: {run_dir.name}\")\n",
    "print(f\"   ðŸŽ¯ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"   ðŸ”¢ Total tokens: {feature_stats.total_tokens}\")\n",
    "print(f\"   âœ¨ SAE features: {SAE_LATENT_DIM}\")\n",
    "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
    "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
    "    metrics = feat['metrics']\n",
    "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
    "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
    "          f\"max={metrics['max_activation']:.4f}, \"\n",
    "          f\"frac={metrics['fraction_active']:.2%}\")\n",
    "\n",
    "print(f\"\\nðŸŒ Start the viewer to see results:\")\n",
    "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
    "print(f\"   cd sae-viewer && npm start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL INFERENCE WITHOUT SAEs\n",
      "============================================================\n",
      "\n",
      "ðŸ”¬ Running inference on 100 test samples...\n",
      "   Device: cuda\n",
      "   Model: ./finbert_twitter_ft/best\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|â–         | 100/2388 [00:01<00:33, 67.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… INFERENCE COMPLETE (WITHOUT SAEs)\n",
      "============================================================\n",
      "   ðŸ“Š Total Samples: 100\n",
      "   âœ“ Correct Predictions: 87\n",
      "   âœ— Incorrect Predictions: 13\n",
      "   ðŸŽ¯ Model Accuracy: 87.00%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference WITHOUT SAEs - Plain Model Accuracy on Test Data\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL INFERENCE WITHOUT SAEs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Use validation set for evaluation\n",
    "test_ds = ds[\"validation\"]\n",
    "# MAX_SAMPLES = len(test_ds)  # Process all samples, or set a limit if needed\n",
    "MAX_SAMPLES = 100\n",
    "MAX_SEQ_LENGTH = 64\n",
    "\n",
    "print(f\"\\nðŸ”¬ Running inference on {MAX_SAMPLES} test samples...\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model: {save_dir}\\n\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(test_ds, desc=\"Processing\")):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        if pred_id == true_label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"âœ… INFERENCE COMPLETE (WITHOUT SAEs)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"   ðŸ“Š Total Samples: {total_predictions}\")\n",
    "print(f\"   âœ“ Correct Predictions: {correct_predictions}\")\n",
    "print(f\"   âœ— Incorrect Predictions: {total_predictions - correct_predictions}\")\n",
    "print(f\"   ðŸŽ¯ Model Accuracy: {accuracy:.2%}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
