{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import re\n",
    "\n",
    "# 1. Load dataset\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deae2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233625a",
   "metadata": {},
   "source": [
    "Fine Tune Hyperparamters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40e9d6",
   "metadata": {},
   "source": [
    "Train Sparse Autoencoder on FinBERT Activations\n",
    "\n",
    "This trains an SAE to decompose FinBERT's 768-dimensional activations into ~32k interpretable sparse features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c951276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from 9543 training samples...\n",
      "Target layer: 8\n",
      "Will train SAEs with latent dimensions: [4096, 8192, 16384, 32768]\n",
      "Extracting activations from training set...\n",
      "Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9543/9543 [02:07<00:00, 74.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCollected 172027 token activations\n",
      "Activation shape: torch.Size([172027, 768])\n",
      "\\n================================================================================\n",
      "Training SAE with 4096 latent features (4k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0399, Recon=0.0397, L1=0.2533\n",
      "Epoch 2/3: Loss=0.0205, Recon=0.0203, L1=0.2581\n",
      "Epoch 3/3: Loss=0.0196, Recon=0.0194, L1=0.2516\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_4k.pt\n",
      "\\n‚úì SAE trained successfully!\n",
      "  Average sparsity: 30.57% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_4k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 8192 latent features (8k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0442, Recon=0.0441, L1=0.1266\n",
      "Epoch 2/3: Loss=0.0209, Recon=0.0208, L1=0.1288\n",
      "Epoch 3/3: Loss=0.0198, Recon=0.0197, L1=0.1257\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_8k.pt\n",
      "\\n‚úì SAE trained successfully!\n",
      "  Average sparsity: 15.30% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_8k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 16384 latent features (16k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0533, Recon=0.0533, L1=0.0641\n",
      "Epoch 2/3: Loss=0.0215, Recon=0.0215, L1=0.0641\n",
      "Epoch 3/3: Loss=0.0198, Recon=0.0197, L1=0.0630\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_16k.pt\n",
      "\\n‚úì SAE trained successfully!\n",
      "  Average sparsity: 7.67% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_16k.pt\n",
      "\\n================================================================================\n",
      "Training SAE with 32768 latent features (32k)\n",
      "================================================================================\n",
      "\\nTraining SAE for 3 epochs...\n",
      "Epoch 1/3: Loss=0.0796, Recon=0.0796, L1=0.0333\n",
      "Epoch 2/3: Loss=0.0215, Recon=0.0215, L1=0.0324\n",
      "Epoch 3/3: Loss=0.0202, Recon=0.0202, L1=0.0316\n",
      "\\nSaving trained SAE to ./finbert_sae/layer_8_32k.pt\n",
      "\\n‚úì SAE trained successfully!\n",
      "  Average sparsity: 3.82% of features active\n",
      "  Saved to: ./finbert_sae/layer_8_32k.pt\n",
      "\\n================================================================================\n",
      "All SAEs trained successfully!\n",
      "Available SAE models:\n",
      "  - layer_8_4k.pt (4096 features)\n",
      "  - layer_8_8k.pt (8192 features)\n",
      "  - layer_8_16k.pt (16384 features)\n",
      "  - layer_8_32k.pt (32768 features)\n",
      "\\nThese SAEs can now be used in main.py for interpretability analysis!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# This cell finetunes SAEs based on BERT.\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # Middle layer of BERT\n",
    "LATENT_DIMS = [4096, 8192, 16384, 32768]  # Train SAEs with 4k, 8k, 16k, 32k features\n",
    "L1_COEFFICIENT = 1e-3  # Sparsity penalty\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Create SAE save directory\n",
    "Path(\"./finbert_sae\").mkdir(exist_ok=True)\n",
    "\n",
    "# Define Sparse Autoencoder (compatible with OpenAI's architecture)\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, latent_dim=32768):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: input -> latent\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        \n",
    "        # Decoder: latent -> reconstruction\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim, bias=True)\n",
    "        \n",
    "        # Initialize decoder with unit norm columns (standard for SAEs)\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.data = nn.functional.normalize(\n",
    "                self.decoder.weight.data, dim=0\n",
    "            )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode to sparse latent representation\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        latent = nn.functional.relu(latent)  # ReLU for sparsity\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        \"\"\"Decode from latent representation\"\"\"\n",
    "        return self.decoder(latent)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        reconstruction = self.decode(latent)\n",
    "        return reconstruction, latent\n",
    "    \n",
    "    def get_feature_activations(self, x):\n",
    "        \"\"\"Get sparse feature activations (for analysis)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.encode(x)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "train_ds = ds[\"train\"]\n",
    "\n",
    "print(f\"Collecting activations from {len(train_ds)} training samples...\")\n",
    "print(f\"Target layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"Will train SAEs with latent dimensions: {LATENT_DIMS}\")\n",
    "\n",
    "# Collect training activations\n",
    "all_activations = []\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "# Collect activations from all training data\n",
    "print(\"Extracting activations from training set...\")\n",
    "print(\"Filtering out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.) - keeping only content tokens...\")\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(train_ds)):\n",
    "        text = sample[\"text\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        captured_activations.clear()\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "        if captured_activations:\n",
    "            # Get all token activations: [seq_len, 768] - stays on GPU\n",
    "            activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Get attention mask and token IDs (keep on GPU)\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "\n",
    "            # Print the number of valid tokens\n",
    "            # kept = valid_mask.sum().item()\n",
    "            # total = attention_mask.sum().item()\n",
    "            # print(f\"Kept {kept}/{total} tokens\")\n",
    "\n",
    "            # tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            # kept_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if m]\n",
    "            # dropped_tokens = [t for t, m in zip(tokens, valid_mask.tolist()) if not m]\n",
    "\n",
    "            # print(\"TOKENS:\", tokens)\n",
    "            # print(\"DROPPED:\", dropped_tokens)\n",
    "            # print(\"KEPT:\", kept_tokens)\n",
    "            \n",
    "            # Only keep activations for real content tokens (still on GPU)\n",
    "            activation = activation[valid_mask]\n",
    "            \n",
    "            # Only add if there are real tokens\n",
    "            if activation.shape[0] > 0:\n",
    "                # Move to CPU only when storing for later processing\n",
    "                all_activations.append(activation.cpu())\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "# Flatten all activations into a single tensor [total_tokens, 768]\n",
    "all_activations_tensor = torch.cat(all_activations, dim=0)\n",
    "print(f\"\\\\nCollected {all_activations_tensor.shape[0]} token activations\")\n",
    "print(f\"Activation shape: {all_activations_tensor.shape}\")\n",
    "\n",
    "# Train SAEs for each latent dimension\n",
    "for LATENT_DIM in LATENT_DIMS:\n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    print(f\"Training SAE with {LATENT_DIM} latent features ({LATENT_DIM//1024}k)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create SAE\n",
    "    sae = SparseAutoencoder(input_dim=768, latent_dim=LATENT_DIM)\n",
    "    sae.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(sae.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    dataset = TensorDataset(all_activations_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\\\nTraining SAE for {NUM_EPOCHS} epochs...\")\n",
    "    sae.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_l1_loss = 0\n",
    "        \n",
    "        for batch_idx, (batch_x,) in enumerate(dataloader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstruction, latent = sae(batch_x)\n",
    "            \n",
    "            # Reconstruction loss (MSE)\n",
    "            recon_loss = nn.functional.mse_loss(reconstruction, batch_x)\n",
    "            \n",
    "            # L1 sparsity loss\n",
    "            l1_loss = latent.abs().mean()\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = recon_loss + L1_COEFFICIENT * l1_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Renormalize decoder weights (standard SAE practice)\n",
    "            with torch.no_grad():\n",
    "                sae.decoder.weight.data = nn.functional.normalize(\n",
    "                    sae.decoder.weight.data, dim=0\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_l1_loss += l1_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_recon = total_recon_loss / len(dataloader)\n",
    "        avg_l1 = total_l1_loss / len(dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss={avg_loss:.4f}, \"\n",
    "              f\"Recon={avg_recon:.4f}, L1={avg_l1:.4f}\")\n",
    "    \n",
    "    # Save the trained SAE\n",
    "    SAE_SAVE_PATH = f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{LATENT_DIM//1024}k.pt\"\n",
    "    print(f\"\\\\nSaving trained SAE to {SAE_SAVE_PATH}\")\n",
    "    torch.save({\n",
    "        'encoder_weight': sae.encoder.weight.data.cpu(),\n",
    "        'encoder_bias': sae.encoder.bias.data.cpu(),\n",
    "        'decoder_weight': sae.decoder.weight.data.cpu(),\n",
    "        'decoder_bias': sae.decoder.bias.data.cpu(),\n",
    "        'config': {\n",
    "            'input_dim': 768,\n",
    "            'latent_dim': LATENT_DIM,\n",
    "            'layer': LAYER_TO_EXTRACT,\n",
    "            'model': save_dir,\n",
    "        }\n",
    "    }, SAE_SAVE_PATH)\n",
    "    \n",
    "    # Test sparsity\n",
    "    sae.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_acts = all_activations_tensor[:1000].to(device)\n",
    "        sample_latent = sae.encode(sample_acts)\n",
    "        sparsity = (sample_latent > 0).float().mean()\n",
    "        print(f\"\\\\n‚úì SAE trained successfully!\")\n",
    "        print(f\"  Average sparsity: {sparsity:.2%} of features active\")\n",
    "        print(f\"  Saved to: {SAE_SAVE_PATH}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"All SAEs trained successfully!\")\n",
    "print(f\"Available SAE models:\")\n",
    "for dim in LATENT_DIMS:\n",
    "    print(f\"  - layer_{LAYER_TO_EXTRACT}_{dim//1024}k.pt ({dim} features)\")\n",
    "print(f\"\\\\nThese SAEs can now be used in main.py for interpretability analysis!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70e94b",
   "metadata": {},
   "source": [
    "Load Trained SAE for Inference\n",
    "\n",
    "Use this cell to load a specific SAE model based on the latent dimension you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3eb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load a trained SAE\n",
    "def load_sae(layer=8, latent_size=\"32k\"):\n",
    "    \"\"\"\n",
    "    Load a trained SAE model.\n",
    "    \n",
    "    Args:\n",
    "        layer: The layer number (default: 8)\n",
    "        latent_size: Size of latent dimension as string: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "    \n",
    "    Returns:\n",
    "        sae: The loaded SAE model\n",
    "        config: Configuration dictionary\n",
    "    \"\"\"\n",
    "    sae_path = f\"./finbert_sae/layer_{layer}_{latent_size}.pt\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(sae_path, map_location=device)\n",
    "    \n",
    "    # Create SAE model\n",
    "    config = checkpoint['config']\n",
    "    sae = SparseAutoencoder(input_dim=config['input_dim'], latent_dim=config['latent_dim'])\n",
    "    \n",
    "    # Load weights\n",
    "    sae.encoder.weight.data = checkpoint['encoder_weight']\n",
    "    sae.encoder.bias.data = checkpoint['encoder_bias']\n",
    "    sae.decoder.weight.data = checkpoint['decoder_weight']\n",
    "    sae.decoder.bias.data = checkpoint['decoder_bias']\n",
    "    \n",
    "    sae.to(device)\n",
    "    sae.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded SAE from {sae_path}\")\n",
    "    print(f\"  Layer: {config['layer']}\")\n",
    "    print(f\"  Input dim: {config['input_dim']}\")\n",
    "    print(f\"  Latent dim: {config['latent_dim']}\")\n",
    "    \n",
    "    return sae, config\n",
    "\n",
    "# Example usage:\n",
    "# Load the 32k latent dimension SAE\n",
    "# sae_32k, config = load_sae(layer=8, latent_size=\"32k\")\n",
    "\n",
    "# Load the 16k latent dimension SAE\n",
    "# sae_16k, config = load_sae(layer=8, latent_size=\"16k\")\n",
    "\n",
    "# Load the 8k latent dimension SAE\n",
    "# sae_8k, config = load_sae(layer=8, latent_size=\"8k\")\n",
    "\n",
    "# Load the 4k latent dimension SAE\n",
    "# sae_4k, config = load_sae(layer=8, latent_size=\"4k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0697b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2388/2388 [00:00<00:00, 5957.53 examples/s]\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22928\\4216466311.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1791' max='1791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1791/1791 06:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>0.447270</td>\n",
       "      <td>0.832077</td>\n",
       "      <td>0.768876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.428374</td>\n",
       "      <td>0.843802</td>\n",
       "      <td>0.787618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.514164</td>\n",
       "      <td>0.839196</td>\n",
       "      <td>0.780188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finbert_twitter_ft/best\\\\tokenizer_config.json',\n",
       " './finbert_twitter_ft/best\\\\special_tokens_map.json',\n",
       " './finbert_twitter_ft/best\\\\vocab.txt',\n",
       " './finbert_twitter_ft/best\\\\added_tokens.json',\n",
       " './finbert_twitter_ft/best\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell finetunes the FINBERT model.\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --------- CUDA sanity check ----------\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Load dataset\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "# 2) Load model/tokenizer\n",
    "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# 3) Tokenize\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4) Metrics\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# 5) Training config\n",
    "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finbert_twitter_ft\",\n",
    "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
    "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
    "    report_to=\"none\",              # avoids needing wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
    "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30b519",
   "metadata": {},
   "source": [
    "Extract Layer Activations with Sentiment Predictions (SAE-style Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefe6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING SAE FEATURES FROM FINBERT\n",
      "============================================================\n",
      "\n",
      "üì¶ Loading trained SAE from: ./finbert_sae/layer_6_32k.pt\n",
      "‚úì SAE loaded: 768 dims ‚Üí 32768 sparse features\n",
      "\n",
      "üíæ Saving results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2026-01-10T21-59-42_run-034\n",
      "\n",
      "üî¨ Processing 100 samples...\n",
      "   Layer: 6\n",
      "   Using SAE: 32768 sparse features\n",
      "   Filtering: ALL special tokens excluded (content only)\n",
      "\n",
      "Processed 10/100 samples\n",
      "Processed 20/100 samples\n",
      "Processed 30/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 50/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 70/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 90/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "üìä Computing feature statistics...\n",
      "üéØ Model Accuracy: 88.00%\n",
      "\n",
      "üíæ Saving results...\n",
      "\n",
      "‚úÖ COMPLETE!\n",
      "   üìÅ Results saved to: 2026-01-10T21-59-42_run-034\n",
      "   üéØ Accuracy: 88.00%\n",
      "   üî¢ Total tokens: 1596\n",
      "   ‚ú® SAE features: 32768\n",
      "\n",
      "üìä Top 5 features by mean activation:\n",
      "   1. Feature 2902: mean=1.8567, max=6.4087, frac=98.62%\n",
      "   2. Feature 9585: mean=1.7436, max=9.3563, frac=83.46%\n",
      "   3. Feature 14459: mean=1.7057, max=6.7215, frac=95.93%\n",
      "   4. Feature 30013: mean=1.6196, max=4.9867, frac=98.75%\n",
      "   5. Feature 20803: mean=1.5987, max=5.1715, frac=97.87%\n",
      "\n",
      "üåê Start the viewer to see results:\n",
      "   python viz_analysis/feature_probe_server.py\n",
      "   cd sae-viewer && npm start\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import utilities\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
    "\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 8  # 3/4 layer of BERT (0-11 for base BERT)\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "SAE_SIZE = \"4k\"  # <-- Change this to switch between SAE models, Choose which SAE to use: \"4k\", \"8k\", \"16k\", or \"32k\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING SAE FEATURES FROM FINBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the SAE using the helper function\n",
    "sae, sae_config = load_sae(layer=LAYER_TO_EXTRACT, latent_size=SAE_SIZE)\n",
    "\n",
    "# Extract dimensions from the loaded config\n",
    "SAE_INPUT_DIM = sae_config['input_dim']\n",
    "SAE_LATENT_DIM = sae_config['latent_dim']\n",
    "\n",
    "print(f\"‚úì SAE loaded: {SAE_INPUT_DIM} dims ‚Üí {SAE_LATENT_DIM} sparse features\")\n",
    "\n",
    "# Create run directory using the same utility as main.py\n",
    "# This ensures the server can find it automatically in analysis_data/\n",
    "run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"\\nüíæ Saving results to: {run_dir}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sae.to(device)\n",
    "model.eval()\n",
    "sae.eval()\n",
    "\n",
    "# Load dataset\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "# Feature statistics tracker (per-token aggregation)\n",
    "class FeatureStatsAggregator:\n",
    "    def __init__(self, feature_dim: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.total_tokens = 0\n",
    "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray):\n",
    "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
    "        self.total_tokens += token_activations.shape[0]\n",
    "        self.sum_activations += token_activations.sum(axis=0)\n",
    "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
    "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
    "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
    "    \n",
    "    def get_stats(self):\n",
    "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
    "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
    "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
    "        return {\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": self.max_activations,\n",
    "            \"fraction_active\": frac_active,\n",
    "            \"mean_act_squared\": mean_act_squared\n",
    "        }\n",
    "\n",
    "# Top token tracker per feature\n",
    "class FeatureTopTokenTracker:\n",
    "    def __init__(self, feature_dim: int, top_k: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.top_k = top_k\n",
    "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
    "        self.heaps = [[] for _ in range(feature_dim)]\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
    "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
    "               predicted_label: str = None, true_label: str = None):\n",
    "        \"\"\"Update with tokens from one prompt\"\"\"\n",
    "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
    "            # For each token, find top features\n",
    "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
    "            \n",
    "            for feat_id in top_features:\n",
    "                activation = float(act_vec[feat_id])\n",
    "                if activation <= 0:\n",
    "                    continue\n",
    "                \n",
    "                heap = self.heaps[feat_id]\n",
    "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"activation\": activation,\n",
    "                    \"token_str\": token_str,\n",
    "                    \"token_id\": int(token_id),\n",
    "                    \"token_position\": int(token_pos),\n",
    "                    \"prompt_index\": int(prompt_idx),\n",
    "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
    "                    \"prompt_snippet\": prompt_text[:160],\n",
    "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
    "                    \"true_label\": true_label,\n",
    "                }\n",
    "                \n",
    "                if len(heap) < self.top_k:\n",
    "                    heapq.heappush(heap, (activation, metadata))\n",
    "                elif activation > heap[0][0]:\n",
    "                    heapq.heapreplace(heap, (activation, metadata))\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export top tokens for each feature\"\"\"\n",
    "        result = {}\n",
    "        for feat_id in range(self.feature_dim):\n",
    "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
    "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
    "        return result\n",
    "\n",
    "# Initialize trackers for SAE features\n",
    "feature_stats = FeatureStatsAggregator(SAE_LATENT_DIM)\n",
    "top_token_tracker = FeatureTopTokenTracker(SAE_LATENT_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "\n",
    "# Storage for per-sample metadata\n",
    "all_prompt_metadata = []\n",
    "all_prediction_metadata = []\n",
    "\n",
    "# Hook to capture activations\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach())  # Keep on GPU\n",
    "\n",
    "# Register hook on target layer\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "print(f\"\\nüî¨ Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
    "print(f\"   Layer: {LAYER_TO_EXTRACT}\")\n",
    "print(f\"   Using SAE: {SAE_LATENT_DIM} sparse features\")\n",
    "print(f\"   Filtering: ALL special tokens excluded (content only)\\n\")\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        captured_activations.clear()\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        \n",
    "        # Get captured activation and pass through SAE\n",
    "        if captured_activations:\n",
    "            # Get BERT activations: [seq_len, 768] - stays on GPU\n",
    "            bert_activation = captured_activations[0].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (same as training) - do on GPU\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(0).bool()\n",
    "            token_ids_tensor = inputs[\"input_ids\"].squeeze(0)\n",
    "            \n",
    "            # Filter out ALL special tokens (CLS, SEP, PAD, UNK, MASK, etc.)\n",
    "            special_ids = set(tokenizer.all_special_ids)\n",
    "            not_special = torch.tensor([tid.item() not in special_ids for tid in token_ids_tensor], \n",
    "                                       dtype=torch.bool, device=device)\n",
    "            \n",
    "            valid_mask = attention_mask & not_special  # GPU boolean mask\n",
    "            \n",
    "            # Filter activations on GPU\n",
    "            bert_activation = bert_activation[valid_mask]\n",
    "            \n",
    "            # Skip if no valid tokens\n",
    "            if bert_activation.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pass through SAE (all on GPU): [actual_len, 32768]\n",
    "            sae_features = sae.encode(bert_activation)\n",
    "            \n",
    "            # Only now move to CPU for numpy conversion and token filtering\n",
    "            sae_features_cpu = sae_features.detach().cpu().numpy()\n",
    "            valid_mask_cpu = valid_mask.cpu().numpy()\n",
    "            filtered_token_ids = [tid for tid, valid in zip(token_ids, valid_mask_cpu) if valid]\n",
    "            filtered_prompt_tokens = [tok for tok, valid in zip(prompt_tokens, valid_mask_cpu) if valid]\n",
    "            \n",
    "            seq_len = sae_features_cpu.shape[0]\n",
    "            \n",
    "            # Update feature statistics with SAE features\n",
    "            feature_stats.update(sae_features_cpu)\n",
    "            \n",
    "            # Track top tokens per feature\n",
    "            top_token_tracker.update(\n",
    "                sae_features_cpu, \n",
    "                filtered_token_ids, \n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                prompt_tokens=filtered_prompt_tokens,\n",
    "                predicted_label=pred_label,  # Pass prediction info\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Save prompt metadata\n",
    "            all_prompt_metadata.append({\n",
    "                \"row_id\": idx,\n",
    "                \"seq_len\": seq_len,\n",
    "                \"prompt\": text,\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"true_label\": model.config.id2label[true_label],\n",
    "                \"correct\": pred_id == true_label\n",
    "            })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Compute final statistics\n",
    "print(\"\\nüìä Computing feature statistics...\")\n",
    "stats = feature_stats.get_stats()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / max(len(all_prompt_metadata), 1)\n",
    "print(f\"üéØ Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric = {}\n",
    "for metric_name, values in stats.items():\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
    "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# Save results\n",
    "print(\"\\nüíæ Saving results...\")\n",
    "\n",
    "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
    "prompts_file = run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
    "feature_stats_file = run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": SAE_LATENT_DIM,\n",
    "    \"total_tokens\": feature_stats.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"accuracy\": accuracy,  # Add accuracy for viewer\n",
    "    \"num_samples\": len(all_prompt_metadata),  # Add sample count\n",
    "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric[metric_name]\n",
    "        }\n",
    "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
    "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save metadata\n",
    "metadata_file = run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata),\n",
    "        \"total_tokens\": feature_stats.total_tokens,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": SAE_INPUT_DIM,\n",
    "        \"latent_dim\": SAE_LATENT_DIM,\n",
    "        \"sae_path\": f\"./finbert_sae/layer_{LAYER_TO_EXTRACT}_{SAE_SIZE}.pt\",\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"note\": \"SAE sparse features with predictions\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ COMPLETE!\")\n",
    "print(f\"   üìÅ Results saved to: {run_dir.name}\")\n",
    "print(f\"   üéØ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"   üî¢ Total tokens: {feature_stats.total_tokens}\")\n",
    "print(f\"   ‚ú® SAE features: {SAE_LATENT_DIM}\")\n",
    "print(f\"\\nüìä Top 5 features by mean activation:\")\n",
    "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
    "    metrics = feat['metrics']\n",
    "    print(f\"   {i}. Feature {feat['feature_id']}: \"\n",
    "          f\"mean={metrics['mean_activation']:.4f}, \"\n",
    "          f\"max={metrics['max_activation']:.4f}, \"\n",
    "          f\"frac={metrics['fraction_active']:.2%}\")\n",
    "\n",
    "print(f\"\\nüåê Start the viewer to see results:\")\n",
    "print(f\"   python viz_analysis/feature_probe_server.py\")\n",
    "print(f\"   cd sae-viewer && npm start\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def14c72",
   "metadata": {},
   "source": [
    "Testing Inference based on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de85884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BULLISH  | TSLA beats earnings expectations and raises full-year guidance.\n",
      "BEARISH  | Apple shares fall after reporting weaker-than-expected iPhone sales.\n",
      "NEUTRAL  | The company reported results largely in line with analyst expectations.\n",
      "BEARISH  | Amazon warns of margin pressure due to rising logistics costs.\n",
      "BULLISH  | NVIDIA stock surges as demand for AI chips remains strong.\n",
      "BEARISH  | The firm announced a restructuring plan, sending shares lower.\n",
      "NEUTRAL  | Revenue growth slowed quarter-over-quarter, but profitability improved.\n",
      "BEARISH  | Investors remain cautious ahead of the Federal Reserve meeting.\n",
      "BULLISH  | Strong cash flow and reduced debt boosted investor confidence.\n",
      "BEARISH  | The outlook remains uncertain amid macroeconomic headwinds.\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "\n",
    "example_sentences = [\n",
    "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
    "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
    "    \"The company reported results largely in line with analyst expectations.\",\n",
    "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
    "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
    "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
    "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
    "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
    "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
    "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "# optional: move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    pred_id = out.logits.argmax(dim=-1).item()\n",
    "    return model.config.id2label[pred_id]\n",
    "\n",
    "for text in example_sentences:\n",
    "    label = predict_sentiment(text)\n",
    "    print(f\"{label.upper():8} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc24a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.6.0+cu124\n",
      "Torch file: c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\Lib\\site-packages\\torch\\__init__.py\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torch file:\", torch.__file__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f0b474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Crown Holdings, Inc. Full-Year Results: Here's What Analysts Are Forecasting For Next Year\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "test_ds[\"text\"][95]\n",
    "#ds2 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "#ds2[\"validation\"][\"text\"][34]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
