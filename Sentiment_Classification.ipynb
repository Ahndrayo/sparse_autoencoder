{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# 1. Load dataset\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233625a",
   "metadata": {},
   "source": [
    "Fine Tune Hyperparamters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0697b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_117228\\2709082109.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1791' max='1791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1791/1791 1:20:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.457106</td>\n",
       "      <td>0.821189</td>\n",
       "      <td>0.759460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339700</td>\n",
       "      <td>0.419859</td>\n",
       "      <td>0.848409</td>\n",
       "      <td>0.788654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.518351</td>\n",
       "      <td>0.838358</td>\n",
       "      <td>0.778926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finbert_twitter_ft/best\\\\tokenizer_config.json',\n",
       " './finbert_twitter_ft/best\\\\special_tokens_map.json',\n",
       " './finbert_twitter_ft/best\\\\vocab.txt',\n",
       " './finbert_twitter_ft/best\\\\added_tokens.json',\n",
       " './finbert_twitter_ft/best\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --------- CUDA sanity check ----------\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Load dataset\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "# 2) Load model/tokenizer\n",
    "model_name = \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "id2label = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Move model to GPU (Trainer will also handle this, but this is explicit and harmless)\n",
    "model.to(device)\n",
    "\n",
    "# 3) Tokenize\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok = val_tok.rename_column(\"label\", \"labels\")\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "val_tok.set_format(type=\"torch\", columns=cols_to_keep)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4) Metrics\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# 5) Training config\n",
    "use_fp16 = torch.cuda.is_available()  # fp16 only makes sense on GPU\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finbert_twitter_ft\",\n",
    "    eval_strategy=\"epoch\",   # <-- use this name; some versions don't accept eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    fp16=use_fp16,                 # <-- enables mixed precision on NVIDIA GPU\n",
    "    dataloader_num_workers=0,      # safer on Windows; avoids hanging\n",
    "    report_to=\"none\",              # avoids needing wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"./finbert_twitter_ft/best\")\n",
    "tokenizer.save_pretrained(\"./finbert_twitter_ft/best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30b519",
   "metadata": {},
   "source": [
    "Extract Layer Activations with Sentiment Predictions (SAE-style Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefe6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2025-12-19T14-47-41_run-018\n",
      "Processing 100 samples...\n",
      "Extracting PER-TOKEN activations from layer 6\n",
      "Hidden dimension: 768 (treating each dimension as a 'feature')\n",
      "Processed 10/100 samples\n",
      "Processed 20/100 samples\n",
      "Processed 30/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 50/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 70/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 90/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "Computing feature statistics...\n",
      "Saving results...\n",
      "\n",
      "âœ“ Saved 100 prompts to C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2025-12-19T14-47-41_run-018\\prompts.jsonl\n",
      "âœ“ Saved feature statistics to C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2025-12-19T14-47-41_run-018\\feature_stats.json\n",
      "âœ“ Saved top tokens per feature to C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2025-12-19T14-47-41_run-018\\feature_tokens.json\n",
      "âœ“ Saved metadata to C:\\Users\\andre\\OneDrive - National University of Singapore\\Desktop\\FYP\\sparse_autoencoder_openai\\analysis_data\\2025-12-19T14-47-41_run-018\\metadata.json\n",
      "\n",
      "Total tokens processed: 2290\n",
      "Accuracy: 87.00%\n",
      "\n",
      "ðŸ“Š Top 5 features by mean activation:\n",
      "  1. Feature 55: mean=0.8976, max=3.6313, frac=78.03%\n",
      "  2. Feature 724: mean=0.8530, max=3.5096, frac=83.28%\n",
      "  3. Feature 182: mean=0.7445, max=3.6348, frac=76.55%\n",
      "  4. Feature 65: mean=0.7041, max=3.6503, frac=82.71%\n",
      "  5. Feature 143: mean=0.6830, max=2.9800, frac=73.45%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import utilities\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root / \"sparse_autoencoder\") not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root / \"sparse_autoencoder\"))\n",
    "\n",
    "from utils.run_dirs import make_analysis_run_dir\n",
    "\n",
    "# Configuration\n",
    "LAYER_TO_EXTRACT = 6  # Middle layer of BERT (0-11 for base BERT)\n",
    "MAX_SAMPLES = 100  # Limit for testing\n",
    "TOP_FEATURES = 100  # Top features to track per metric\n",
    "TOP_TOKENS_PER_FEATURE = 20  # Top activating tokens per feature\n",
    "MAX_SEQ_LENGTH = 64  # Maximum sequence length to process\n",
    "\n",
    "# Create run directory using the same utility as main.py\n",
    "# This ensures the server can find it automatically in analysis_data/\n",
    "run_dir = make_analysis_run_dir(str(repo_root))\n",
    "print(f\"Saving results to: {run_dir}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "test_ds = ds[\"validation\"]  # Use validation set for analysis\n",
    "\n",
    "# Feature statistics tracker (per-token aggregation)\n",
    "class FeatureStatsAggregator:\n",
    "    def __init__(self, feature_dim: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.total_tokens = 0\n",
    "        self.sum_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.max_activations = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.nonzero_counts = np.zeros(feature_dim, dtype=np.float64)\n",
    "        self.sum_of_squares = np.zeros(feature_dim, dtype=np.float64)  # Track squared activations\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray):\n",
    "        \"\"\"Update with activations from tokens [num_tokens, feature_dim]\"\"\"\n",
    "        self.total_tokens += token_activations.shape[0]\n",
    "        self.sum_activations += token_activations.sum(axis=0)\n",
    "        self.max_activations = np.maximum(self.max_activations, token_activations.max(axis=0))\n",
    "        self.nonzero_counts += (token_activations > 0).sum(axis=0)\n",
    "        self.sum_of_squares += (token_activations ** 2).sum(axis=0)  # Accumulate squared values\n",
    "    \n",
    "    def get_stats(self):\n",
    "        mean_act = self.sum_activations / max(self.total_tokens, 1)\n",
    "        frac_active = self.nonzero_counts / max(self.total_tokens, 1)\n",
    "        mean_act_squared = self.sum_of_squares / max(self.total_tokens, 1)\n",
    "        return {\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": self.max_activations,\n",
    "            \"fraction_active\": frac_active,\n",
    "            \"mean_act_squared\": mean_act_squared\n",
    "        }\n",
    "\n",
    "# Top token tracker per feature\n",
    "class FeatureTopTokenTracker:\n",
    "    def __init__(self, feature_dim: int, top_k: int):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.top_k = top_k\n",
    "        # Store min-heaps: [(activation, token_str, token_id, prompt_idx, token_pos), ...]\n",
    "        self.heaps = [[] for _ in range(feature_dim)]\n",
    "    \n",
    "    def update(self, token_activations: np.ndarray, token_ids: List[int], \n",
    "               prompt_idx: int, prompt_text: str, prompt_tokens: List[str],\n",
    "               predicted_label: str = None, true_label: str = None):\n",
    "        \"\"\"Update with tokens from one prompt\"\"\"\n",
    "        for token_pos, (act_vec, token_id) in enumerate(zip(token_activations, token_ids)):\n",
    "            # For each token, find top features\n",
    "            top_features = np.argsort(act_vec)[-5:]  # Track top 5 features per token\n",
    "            \n",
    "            for feat_id in top_features:\n",
    "                activation = float(act_vec[feat_id])\n",
    "                if activation <= 0:\n",
    "                    continue\n",
    "                \n",
    "                heap = self.heaps[feat_id]\n",
    "                token_str = prompt_tokens[token_pos] if token_pos < len(prompt_tokens) else f\"[{token_id}]\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"activation\": activation,\n",
    "                    \"token_str\": token_str,\n",
    "                    \"token_id\": int(token_id),\n",
    "                    \"token_position\": int(token_pos),\n",
    "                    \"prompt_index\": int(prompt_idx),\n",
    "                    \"row_id\": int(prompt_idx),  # Add row_id for server compatibility\n",
    "                    \"prompt_snippet\": prompt_text[:160],\n",
    "                    \"prompt\": prompt_text,  # Changed from \"full_prompt\" to \"prompt\"\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"predicted_label\": predicted_label,  # Add prediction info\n",
    "                    \"true_label\": true_label,\n",
    "                }\n",
    "                \n",
    "                if len(heap) < self.top_k:\n",
    "                    heapq.heappush(heap, (activation, metadata))\n",
    "                elif activation > heap[0][0]:\n",
    "                    heapq.heapreplace(heap, (activation, metadata))\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export top tokens for each feature\"\"\"\n",
    "        result = {}\n",
    "        for feat_id in range(self.feature_dim):\n",
    "            sorted_tokens = sorted(self.heaps[feat_id], key=lambda x: -x[0])\n",
    "            result[str(feat_id)] = [meta for _, meta in sorted_tokens]\n",
    "        return result\n",
    "\n",
    "# Initialize trackers\n",
    "HIDDEN_DIM = 768  # BERT base\n",
    "feature_stats = FeatureStatsAggregator(HIDDEN_DIM)\n",
    "top_token_tracker = FeatureTopTokenTracker(HIDDEN_DIM, TOP_TOKENS_PER_FEATURE)\n",
    "\n",
    "# Storage for per-sample metadata\n",
    "all_prompt_metadata = []\n",
    "all_prediction_metadata = []\n",
    "\n",
    "# Hook to capture activations\n",
    "captured_activations = []\n",
    "\n",
    "def capture_hook(module, input, output):\n",
    "    \"\"\"Hook function to capture layer outputs\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0]\n",
    "    else:\n",
    "        hidden_states = output\n",
    "    captured_activations.append(hidden_states.detach().cpu())\n",
    "\n",
    "# Register hook on target layer\n",
    "target_layer = model.bert.encoder.layer[LAYER_TO_EXTRACT]\n",
    "hook_handle = target_layer.register_forward_hook(capture_hook)\n",
    "\n",
    "print(f\"Processing {min(MAX_SAMPLES, len(test_ds))} samples...\")\n",
    "print(f\"Extracting PER-TOKEN activations from layer {LAYER_TO_EXTRACT}\")\n",
    "print(f\"Hidden dimension: {HIDDEN_DIM} (treating each dimension as a 'feature')\")\n",
    "\n",
    "# Process samples\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_ds):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        true_label = sample[\"label\"]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "        \n",
    "        # Get string tokens for display (properly cleaned)\n",
    "        # Use tokenizer.convert_ids_to_tokens to get raw tokens, then clean them\n",
    "        raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        prompt_tokens = []\n",
    "        for tok in raw_tokens:\n",
    "            # Remove ## prefix for subword tokens, keep special tokens as-is\n",
    "            if tok.startswith(\"##\"):\n",
    "                prompt_tokens.append(tok[2:])  # Remove ##\n",
    "            else:\n",
    "                prompt_tokens.append(tok)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        captured_activations.clear()\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        \n",
    "        # Get captured activation: [1, seq_len, hidden_dim]\n",
    "        if captured_activations:\n",
    "            activation = captured_activations[0].squeeze(0).numpy()  # [seq_len, 768]\n",
    "            seq_len = activation.shape[0]\n",
    "            \n",
    "            # Update feature statistics (all tokens)\n",
    "            feature_stats.update(activation)\n",
    "            \n",
    "            # Track top tokens per feature\n",
    "            top_token_tracker.update(\n",
    "                activation, \n",
    "                token_ids, \n",
    "                prompt_idx=idx,\n",
    "                prompt_text=text,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                predicted_label=pred_label,  # Pass prediction info\n",
    "                true_label=model.config.id2label[true_label]\n",
    "            )\n",
    "            \n",
    "            # Save prompt metadata\n",
    "            all_prompt_metadata.append({\n",
    "                \"row_id\": idx,\n",
    "                \"seq_len\": seq_len,\n",
    "                \"prompt\": text,\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"true_label\": model.config.id2label[true_label],\n",
    "                \"correct\": pred_id == true_label\n",
    "            })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{min(MAX_SAMPLES, len(test_ds))} samples\")\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Compute final statistics\n",
    "print(\"\\nComputing feature statistics...\")\n",
    "stats = feature_stats.get_stats()\n",
    "\n",
    "# Get top features for each metric\n",
    "top_features_by_metric = {}\n",
    "for metric_name, values in stats.items():\n",
    "    top_indices = np.argsort(values)[-TOP_FEATURES:][::-1]\n",
    "    top_features_by_metric[metric_name] = [\n",
    "        {\n",
    "            \"feature_id\": int(idx),\n",
    "            \"value\": float(values[idx]),\n",
    "            \"metrics\": {  # Nest metrics in a sub-dict for server compatibility\n",
    "                \"mean_activation\": float(stats[\"mean_activation\"][idx]),\n",
    "                \"max_activation\": float(stats[\"max_activation\"][idx]),\n",
    "                \"fraction_active\": float(stats[\"fraction_active\"][idx])\n",
    "            }\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# 1. Save prompts metadata (replaces prompts.jsonl from main.py)\n",
    "prompts_file = run_dir / \"prompts.jsonl\"\n",
    "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for meta in all_prompt_metadata:\n",
    "        json.dump(meta, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save feature statistics (replaces feature_stats.json from main.py)\n",
    "feature_stats_file = run_dir / \"feature_stats.json\"\n",
    "feature_stats_data = {\n",
    "    \"num_features\": HIDDEN_DIM,\n",
    "    \"total_tokens\": feature_stats.total_tokens,\n",
    "    \"top_feature_count\": TOP_FEATURES,\n",
    "    \"mean_act_squared\": stats[\"mean_act_squared\"].tolist(),  # Add mean_act_squared for server\n",
    "    \"metrics\": {\n",
    "        metric_name: {\n",
    "            \"description\": f\"{metric_name.replace('_', ' ').title()} for each feature\",\n",
    "            \"top_features\": top_features_by_metric[metric_name]\n",
    "        }\n",
    "        for metric_name in stats.keys() if metric_name != \"mean_act_squared\"  # Exclude from metrics iteration\n",
    "    }\n",
    "}\n",
    "with open(feature_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats_data, f, indent=2)\n",
    "\n",
    "# 3. Save top tokens per feature (replaces feature_tokens.json from main.py)\n",
    "feature_tokens_file = run_dir / \"feature_tokens.json\"\n",
    "feature_tokens_data = {\n",
    "    \"features\": top_token_tracker.export()  # Wrap in \"features\" key for server compatibility\n",
    "}\n",
    "with open(feature_tokens_file, \"w\") as f:\n",
    "    json.dump(feature_tokens_data, f, indent=2)\n",
    "\n",
    "# 4. Save metadata\n",
    "accuracy = sum(1 for p in all_prompt_metadata if p[\"correct\"]) / len(all_prompt_metadata)\n",
    "metadata_file = run_dir / \"metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": save_dir,\n",
    "        \"layer_extracted\": LAYER_TO_EXTRACT,\n",
    "        \"num_samples\": len(all_prompt_metadata),\n",
    "        \"total_tokens\": feature_stats.total_tokens,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"dataset\": \"zeroshot/twitter-financial-news-sentiment\",\n",
    "        \"split\": \"validation\",\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"top_features_per_metric\": TOP_FEATURES,\n",
    "        \"top_tokens_per_feature\": TOP_TOKENS_PER_FEATURE,\n",
    "        \"note\": \"PER-TOKEN activations, compatible with sae-viewer\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved {len(all_prompt_metadata)} prompts to {prompts_file}\")\n",
    "print(f\"âœ“ Saved feature statistics to {feature_stats_file}\")\n",
    "print(f\"âœ“ Saved top tokens per feature to {feature_tokens_file}\")\n",
    "print(f\"âœ“ Saved metadata to {metadata_file}\")\n",
    "print(f\"\\nTotal tokens processed: {feature_stats.total_tokens}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\nðŸ“Š Top 5 features by mean activation:\")\n",
    "for i, feat in enumerate(top_features_by_metric[\"mean_activation\"][:5], 1):\n",
    "    metrics = feat['metrics']\n",
    "    print(f\"  {i}. Feature {feat['feature_id']}: mean={metrics['mean_activation']:.4f}, \"\n",
    "          f\"max={metrics['max_activation']:.4f}, frac={metrics['fraction_active']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cb0582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Analysts Eviscerate Musk\\'s Cybertruck: \"0% Of Responses Felt It Will Be A Success\" https://t.co/2NTzeZea4G',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"validation\"][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02717a98",
   "metadata": {},
   "source": [
    "Extract Sparse Autoencoder Features from BERT Activations\n",
    "\n",
    "This cell demonstrates how to pass the BERT activations through a sparse autoencoder to get interpretable features. Note: This requires a trained SAE compatible with BERT's hidden dimension (768)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dde893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is OPTIONAL and requires sparse_autoencoder package\n",
    "# Uncomment and run if you have a trained SAE for BERT or want to use GPT-2's SAE\n",
    "\n",
    "\"\"\"\n",
    "import sparse_autoencoder\n",
    "import blobfile as bf\n",
    "\n",
    "# Load a sparse autoencoder\n",
    "# Option 1: Use OpenAI's GPT-2 SAE (may not be perfectly calibrated for BERT)\n",
    "AUTOENCODER_PATH = \"s3://openaipublic/sparse-autoencoder/gpt2-small/layer_3/autoencoders/p_annealing_0_lr_resample_0_0001/1/ae.pt\"\n",
    "\n",
    "# Load autoencoder\n",
    "sae_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with bf.BlobFile(AUTOENCODER_PATH, \"rb\") as f:\n",
    "    state_dict = torch.load(f, map_location=sae_device)\n",
    "    \n",
    "# Initialize autoencoder (hidden_dim should match BERT's 768)\n",
    "sae = sparse_autoencoder.Autoencoder.from_state_dict(state_dict)\n",
    "sae.to(sae_device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded: input_dim={sae.n_inputs}, latent_dim={sae.n_latents}\")\n",
    "\n",
    "# Process the saved activations through SAE\n",
    "sae_features_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for act_dict in all_activations:\n",
    "        # Use mean pooled activation as input\n",
    "        mean_act = torch.tensor(act_dict[\"mean_activation\"], dtype=torch.float32).to(sae_device)\n",
    "        \n",
    "        # Forward through SAE\n",
    "        latent_acts, reconstructed = sae(mean_act.unsqueeze(0))\n",
    "        \n",
    "        # Store sparse features\n",
    "        sae_features_all.append({\n",
    "            \"latent_activations\": latent_acts.squeeze(0).cpu().numpy().tolist(),\n",
    "            \"num_active_features\": int((latent_acts > 0).sum().item()),\n",
    "            \"reconstruction_error\": float(torch.nn.functional.mse_loss(mean_act, reconstructed.squeeze(0)).item())\n",
    "        })\n",
    "\n",
    "# Save SAE features\n",
    "sae_features_file = run_dir / \"sae_features.json\"\n",
    "with open(sae_features_file, \"w\") as f:\n",
    "    json.dump(sae_features_all, f, indent=2)\n",
    "\n",
    "# Compute top active features across all samples\n",
    "all_latents = np.array([f[\"latent_activations\"] for f in sae_features_all])\n",
    "mean_latents = all_latents.mean(axis=0)\n",
    "max_latents = all_latents.max(axis=0)\n",
    "frac_active = (all_latents > 0).mean(axis=0)\n",
    "\n",
    "# Find top features by mean activation\n",
    "top_k = 20\n",
    "top_features_idx = np.argsort(mean_latents)[-top_k:][::-1]\n",
    "\n",
    "print(f\"\\nTop {top_k} SAE features by mean activation:\")\n",
    "for rank, feat_id in enumerate(top_features_idx, 1):\n",
    "    print(f\"  {rank}. Feature {feat_id}: mean={mean_latents[feat_id]:.4f}, \"\n",
    "          f\"max={max_latents[feat_id]:.4f}, frac_active={frac_active[feat_id]:.2%}\")\n",
    "\n",
    "# Save feature stats\n",
    "feature_stats = {\n",
    "    \"num_latents\": len(mean_latents),\n",
    "    \"top_features\": [\n",
    "        {\n",
    "            \"feature_id\": int(feat_id),\n",
    "            \"mean_activation\": float(mean_latents[feat_id]),\n",
    "            \"max_activation\": float(max_latents[feat_id]),\n",
    "            \"fraction_active\": float(frac_active[feat_id])\n",
    "        }\n",
    "        for feat_id in top_features_idx\n",
    "    ]\n",
    "}\n",
    "\n",
    "sae_stats_file = run_dir / \"sae_feature_stats.json\"\n",
    "with open(sae_stats_file, \"w\") as f:\n",
    "    json.dump(feature_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved SAE features to {sae_features_file}\")\n",
    "print(f\"âœ“ Saved feature stats to {sae_stats_file}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"This cell is commented out by default.\")\n",
    "print(\"To use SAE features, uncomment the code and ensure you have:\")\n",
    "print(\"  1. The sparse_autoencoder package installed\")\n",
    "print(\"  2. A trained SAE compatible with BERT's hidden dimension (768)\")\n",
    "print(\"  3. Access to the SAE weights (local or S3)\")\n",
    "print(\"\\nFor now, the previous cell has saved raw BERT layer activations with predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def14c72",
   "metadata": {},
   "source": [
    "Testing Inference based on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de85884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BULLISH  | TSLA beats earnings expectations and raises full-year guidance.\n",
      "BEARISH  | Apple shares fall after reporting weaker-than-expected iPhone sales.\n",
      "NEUTRAL  | The company reported results largely in line with analyst expectations.\n",
      "BEARISH  | Amazon warns of margin pressure due to rising logistics costs.\n",
      "BULLISH  | NVIDIA stock surges as demand for AI chips remains strong.\n",
      "BEARISH  | The firm announced a restructuring plan, sending shares lower.\n",
      "NEUTRAL  | Revenue growth slowed quarter-over-quarter, but profitability improved.\n",
      "BEARISH  | Investors remain cautious ahead of the Federal Reserve meeting.\n",
      "BULLISH  | Strong cash flow and reduced debt boosted investor confidence.\n",
      "BEARISH  | The outlook remains uncertain amid macroeconomic headwinds.\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./finbert_twitter_ft/best\"\n",
    "\n",
    "example_sentences = [\n",
    "    \"TSLA beats earnings expectations and raises full-year guidance.\",\n",
    "    \"Apple shares fall after reporting weaker-than-expected iPhone sales.\",\n",
    "    \"The company reported results largely in line with analyst expectations.\",\n",
    "    \"Amazon warns of margin pressure due to rising logistics costs.\",\n",
    "    \"NVIDIA stock surges as demand for AI chips remains strong.\",\n",
    "    \"The firm announced a restructuring plan, sending shares lower.\",\n",
    "    \"Revenue growth slowed quarter-over-quarter, but profitability improved.\",\n",
    "    \"Investors remain cautious ahead of the Federal Reserve meeting.\",\n",
    "    \"Strong cash flow and reduced debt boosted investor confidence.\",\n",
    "    \"The outlook remains uncertain amid macroeconomic headwinds.\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "# optional: move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    pred_id = out.logits.argmax(dim=-1).item()\n",
    "    return model.config.id2label[pred_id]\n",
    "\n",
    "for text in example_sentences:\n",
    "    label = predict_sentiment(text)\n",
    "    print(f\"{label.upper():8} | {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
